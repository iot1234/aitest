# advanced_training.py - FIXED FOR TRANSCRIPT FORMAT
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any, Optional
import logging
from datetime import datetime
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from imblearn.over_sampling import SMOTE
import warnings
warnings.filterwarnings('ignore')

# Setup logger
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class AdvancedFeatureEngineer:
    """
    Advanced Context-Aware Feature Engineering System
    ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Transcript Format: 1 ‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤ = ‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏ñ‡∏ß (1 ‡πÅ‡∏ñ‡∏ß = 1 ‡∏ß‡∏¥‡∏ä‡∏≤)
    """
    
    def __init__(self, grade_mapping: Dict[str, float]):
        self.grade_mapping = grade_mapping
        self.course_profiles = {}
        self.global_statistics = {}
        
    def prepare_training_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:
        """
        Main entry point - ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö Transcript Format
        """
        logger.info("="*70)
        logger.info("üöÄ Starting Advanced Context-Aware Training")
        logger.info("   Format: TRANSCRIPT (1 student = multiple rows)")
        logger.info("="*70)
        
        try:
            # Step 1: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            logger.info("üìã Step 1: Analyzing data structure...")
            student_col = self._validate_transcript_format(df)
            
            # Step 2: ‡∏™‡∏£‡πâ‡∏≤‡∏á Course DNA Profiles
            logger.info("üß¨ Step 2: Creating Course DNA Profiles...")
            self.course_profiles = self._create_course_profiles(df, student_col)
            
            # Step 3: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Global Statistics
            logger.info("üìä Step 3: Calculating global statistics...")
            self.global_statistics = self._calculate_global_statistics(df, student_col)
            
            # Step 4: ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô Student-Level Features
            logger.info("üë• Step 4: Transforming to student-level features...")
            student_features = self._transform_to_student_features(df, student_col)
            
            # Step 5: ‡∏™‡∏£‡πâ‡∏≤‡∏á Training Samples
            logger.info("üì∏ Step 5: Creating training samples...")
            X, y = self._create_training_samples(student_features)
            
            logger.info("="*70)
            logger.info(f"‚úÖ Training data prepared successfully!")
            logger.info(f"   Total students: {len(student_features)}")
            logger.info(f"   Training samples: {len(X)}")
            logger.info(f"   Features: {X.shape[1]}")
            logger.info(f"   Class distribution: {y.value_counts().to_dict()}")
            logger.info("="*70)
            
            return X, y
            
        except Exception as e:
            logger.error(f"‚ùå Error in prepare_training_data: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            raise
    
    def _validate_transcript_format(self, df: pd.DataFrame) -> str:
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô Transcript Format ‡πÅ‡∏•‡∏∞‡∏´‡∏≤ Student ID column
        """
        # ‡∏´‡∏≤ Student ID column
        student_col_candidates = [
            'Dummy StudentNO', 'DUMMY STUDENTNO', 'dummy studentno',
            'STUDENT_ID', 'StudentID', 'student_id', '‡∏£‡∏´‡∏±‡∏™‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤'
        ]
        
        student_col = None
        for col in df.columns:
            if col in student_col_candidates or 'student' in col.lower():
                student_col = col
                break
        
        if not student_col:
            raise ValueError("Cannot find student ID column!")
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        unique_students = df[student_col].nunique()
        total_rows = len(df)
        avg_rows_per_student = total_rows / unique_students if unique_students > 0 else 0
        
        logger.info(f"   ‚úÖ Found student column: '{student_col}'")
        logger.info(f"   üìä Data Statistics:")
        logger.info(f"      - Total rows: {total_rows:,}")
        logger.info(f"      - Unique students: {unique_students:,}")
        logger.info(f"      - Avg rows per student: {avg_rows_per_student:.1f}")
        logger.info(f"      - Min rows: {df[student_col].value_counts().min()}")
        logger.info(f"      - Max rows: {df[student_col].value_counts().max()}")
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        sample_student = df[student_col].iloc[0]
        sample_data = df[df[student_col] == sample_student].head(3)
        logger.info(f"   üìù Sample data for student {sample_student}:")
        for col in ['COURSE_CODE', 'COURSE_NAME', 'GRADE', 'CREDIT']:
            if col in df.columns:
                values = sample_data[col].tolist()
                logger.info(f"      {col}: {values}")
        
        return student_col
    
    def _create_course_profiles(self, df: pd.DataFrame, student_col: str) -> Dict:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á Course DNA Profile ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏∏‡∏Å‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤
        """
        profiles = {}
        
        # ‡∏´‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ
        course_col = self._find_column(df, ['COURSE_CODE', 'Course_Code', 'course_code'])
        grade_col = self._find_column(df, ['GRADE', 'Grade', 'grade'])
        
        if not course_col or not grade_col:
            logger.warning("Cannot find required columns for profiling")
            return profiles
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ß‡∏¥‡∏ä‡∏≤
        unique_courses = df[course_col].dropna().unique()
        logger.info(f"   Found {len(unique_courses)} unique courses")
        
        for course_code in unique_courses:
            course_data = df[df[course_col] == course_code]
            
            # ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏Å‡∏£‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Ç‡∏≠‡∏á‡∏ß‡∏¥‡∏ä‡∏≤‡∏ô‡∏µ‡πâ
            grades = []
            grade_dist = {}
            students_count = course_data[student_col].nunique()
            
            for grade in course_data[grade_col]:
                numeric = self._convert_grade_to_numeric(grade)
                if numeric is not None:
                    grades.append(numeric)
                    grade_str = str(grade).upper()
                    grade_dist[grade_str] = grade_dist.get(grade_str, 0) + 1
            
            if len(grades) < 5:  # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 5 ‡∏Ñ‡∏ô
                continue
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á Course DNA
            profile = {
                'course_code': course_code,
                'total_students': students_count,
                'total_grades': len(grades),
                
                # Statistics
                'avg_grade': np.mean(grades),
                'median_grade': np.median(grades),
                'std_grade': np.std(grades),
                'min_grade': np.min(grades),
                'max_grade': np.max(grades),
                
                # Percentiles
                'percentile_25': np.percentile(grades, 25),
                'percentile_50': np.percentile(grades, 50),
                'percentile_75': np.percentile(grades, 75),
                
                # Performance Rates
                'fail_rate': sum(1 for g in grades if g == 0) / len(grades),
                'pass_rate': sum(1 for g in grades if g > 0) / len(grades),
                'excellent_rate': sum(1 for g in grades if g >= 3.5) / len(grades),
                'withdraw_rate': grade_dist.get('W', 0) / len(grades) if grades else 0,
                
                # Difficulty Score (0-1)
                'difficulty_score': self._calculate_difficulty_score(grades),
                
                # Auto Classification
                'course_type': self._classify_course_type(grades, grade_dist)
            }
            
            profiles[str(course_code)] = profile
            
            # Log interesting courses
            if profile['fail_rate'] > 0.3:
                logger.info(f"      üî¥ Killer Course: {course_code} "
                          f"(fail={profile['fail_rate']:.1%}, avg={profile['avg_grade']:.2f})")
            elif profile['excellent_rate'] > 0.5:
                logger.info(f"      üü¢ Easy Course: {course_code} "
                          f"(A-rate={profile['excellent_rate']:.1%}, avg={profile['avg_grade']:.2f})")
        
        logger.info(f"   ‚úÖ Created {len(profiles)} course profiles")
        return profiles
    
    def _calculate_global_statistics(self, df: pd.DataFrame, student_col: str) -> Dict:
        """
        ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        """
        stats = {
            'total_students': df[student_col].nunique(),
            'total_records': len(df),
            'unique_courses': df['COURSE_CODE'].nunique() if 'COURSE_CODE' in df.columns else 0
        }
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Overall Performance
        grade_col = self._find_column(df, ['GRADE', 'Grade'])
        if grade_col:
            all_grades = []
            for grade in df[grade_col]:
                numeric = self._convert_grade_to_numeric(grade)
                if numeric is not None:
                    all_grades.append(numeric)
            
            if all_grades:
                stats['overall_avg_gpa'] = np.mean(all_grades)
                stats['overall_fail_rate'] = sum(1 for g in all_grades if g == 0) / len(all_grades)
                stats['overall_pass_rate'] = sum(1 for g in all_grades if g > 0) / len(all_grades)
        
        logger.info(f"   Global Stats: {stats}")
        return stats
    
    def _transform_to_student_features(self, df: pd.DataFrame, student_col: str) -> List[Dict]:
        """
        ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Transcript (‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏ñ‡∏ß‡∏ï‡πà‡∏≠‡∏Ñ‡∏ô) ‡πÄ‡∏õ‡πá‡∏ô Student Features (1 ‡πÅ‡∏ñ‡∏ß‡∏ï‡πà‡∏≠‡∏Ñ‡∏ô)
        """
        student_features = []
        unique_students = df[student_col].unique()
        
        logger.info(f"   Processing {len(unique_students)} students...")
        
        for i, student_id in enumerate(unique_students):
            if i % 100 == 0 and i > 0:
                logger.info(f"      Processed {i}/{len(unique_students)} students...")
            
            # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Ç‡∏≠‡∏á‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏Ñ‡∏ô‡∏ô‡∏µ‡πâ
            student_data = df[df[student_col] == student_id]
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á features ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏Ñ‡∏ô‡∏ô‡∏µ‡πâ
            features = self._create_student_features(student_data, student_id)
            if features:
                student_features.append(features)
        
        logger.info(f"   ‚úÖ Created features for {len(student_features)} students")
        return student_features
    
    def _create_student_features(self, student_data: pd.DataFrame, student_id: Any) -> Dict:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á Features ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤ 1 ‡∏Ñ‡∏ô ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏∏‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô
        """
        # ‡∏´‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ
        course_col = self._find_column(student_data, ['COURSE_CODE', 'Course_Code'])
        grade_col = self._find_column(student_data, ['GRADE', 'Grade'])
        credit_col = self._find_column(student_data, ['CREDIT', 'Credit'])
        
        if not grade_col:
            return None
        
        # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏£‡∏î‡πÅ‡∏•‡∏∞‡∏ö‡∏£‡∏¥‡∏ö‡∏ó
        grades = []
        credits = []
        contextual_features = {
            'grades_vs_avg': [],
            'passed_hard_courses': 0,
            'failed_easy_courses': 0,
            'killer_courses_passed': 0,
            'killer_courses_failed': 0,
            'easy_courses_passed': 0,
            'easy_courses_failed': 0
        }
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ß‡∏¥‡∏ä‡∏≤
        for _, row in student_data.iterrows():
            grade = self._convert_grade_to_numeric(row[grade_col]) if grade_col in row else None
            credit = float(row[credit_col]) if credit_col and credit_col in row else 3.0
            course_code = row[course_col] if course_col and course_col in row else 'UNKNOWN'
            
            if grade is not None:
                grades.append(grade)
                credits.append(credit)
                
                # üéØ ‡πÄ‡∏û‡∏¥‡πà‡∏° Contextual Analysis
                if str(course_code) in self.course_profiles:
                    profile = self.course_profiles[str(course_code)]
                    
                    # Performance vs Average
                    grade_vs_avg = grade - profile['avg_grade']
                    contextual_features['grades_vs_avg'].append(grade_vs_avg)
                    
                    # Performance in different course types
                    if profile['course_type'] == 'killer':
                        if grade > 0:
                            contextual_features['killer_courses_passed'] += 1
                        else:
                            contextual_features['killer_courses_failed'] += 1
                    
                    elif profile['course_type'] == 'easy':
                        if grade >= 2.0:
                            contextual_features['easy_courses_passed'] += 1
                        else:
                            contextual_features['easy_courses_failed'] += 1
                            contextual_features['failed_easy_courses'] += 1
                    
                    # Hard courses performance
                    if profile['difficulty_score'] > 0.6 and grade > 0:
                        contextual_features['passed_hard_courses'] += 1
        
        if not grades:
            return None
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA ‡πÅ‡∏ö‡∏ö‡∏ñ‡πà‡∏ß‡∏á‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å
        weighted_sum = sum(g * c for g, c in zip(grades, credits))
        total_credits = sum(credits)
        gpa = weighted_sum / total_credits if total_credits > 0 else 0
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Feature Dictionary
        features = {
            'student_id': str(student_id),
            
            # === Basic Metrics ===
            'gpa': gpa,
            'total_courses': len(grades),
            'total_credits': total_credits,
            'courses_passed': sum(1 for g in grades if g > 0),
            'courses_failed': sum(1 for g in grades if g == 0),
            
            # === Statistical Features ===
            'grade_mean': np.mean(grades),
            'grade_std': np.std(grades) if len(grades) > 1 else 0,
            'grade_min': np.min(grades),
            'grade_max': np.max(grades),
            'grade_median': np.median(grades),
            
            # === Performance Rates ===
            'pass_rate': sum(1 for g in grades if g > 0) / len(grades),
            'fail_rate': sum(1 for g in grades if g == 0) / len(grades),
            'excellent_rate': sum(1 for g in grades if g >= 3.5) / len(grades),
            'good_rate': sum(1 for g in grades if 3.0 <= g < 3.5) / len(grades),
            
            # === üéØ CONTEXTUAL FEATURES ===
            'avg_grade_vs_course_avg': np.mean(contextual_features['grades_vs_avg']) if contextual_features['grades_vs_avg'] else 0,
            'std_grade_vs_course_avg': np.std(contextual_features['grades_vs_avg']) if len(contextual_features['grades_vs_avg']) > 1 else 0,
            'passed_hard_courses': contextual_features['passed_hard_courses'],
            'failed_easy_courses': contextual_features['failed_easy_courses'],
            'killer_courses_passed': contextual_features['killer_courses_passed'],
            'killer_courses_failed': contextual_features['killer_courses_failed'],
            
            # === Performance Indicators ===
            'performance_consistency': 1 - (np.std(grades) / 4.0) if len(grades) > 1 else 0.5,
            'at_risk': 1 if gpa < 2.0 else 0,
            'high_performer': 1 if gpa >= 3.25 else 0,
            
            # === Target Variable ===
            'graduated': self._determine_graduation(student_data, gpa, total_credits)
        }
        
        return features
    
    def _create_training_samples(self, student_features: List[Dict]) -> Tuple[pd.DataFrame, pd.Series]:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á Training Samples ‡∏à‡∏≤‡∏Å Student Features
        """
        if not student_features:
            raise ValueError("No student features to create training samples")
        
        # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô DataFrame
        df = pd.DataFrame(student_features)
        
        # ‡πÅ‡∏¢‡∏Å Features ‡πÅ‡∏•‡∏∞ Target
        target_col = 'graduated'
        feature_cols = [col for col in df.columns if col not in ['student_id', 'graduated']]
        
        X = df[feature_cols].fillna(0)
        y = df[target_col].fillna(0).astype(int)
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° Interaction Features
        if 'gpa' in X.columns and 'courses_failed' in X.columns:
            X['gpa_fail_interaction'] = X['gpa'] * X['courses_failed']
        
        if 'pass_rate' in X.columns and 'total_credits' in X.columns:
            X['progress_quality'] = X['pass_rate'] * (X['total_credits'] / 140)  # ‡∏™‡∏°‡∏°‡∏ï‡∏¥ 140 ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï
        
        # Polynomial Features
        if 'gpa' in X.columns:
            X['gpa_squared'] = X['gpa'] ** 2
            X['gpa_cubed'] = X['gpa'] ** 3
        
        # Ensure all numeric
        for col in X.columns:
            X[col] = pd.to_numeric(X[col], errors='coerce').fillna(0)
        
        # Validate target distribution
        if y.nunique() < 2:
            logger.warning("Target has only one class, adding synthetic variation")
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            num_positive = max(2, int(len(y) * 0.3))
            indices = np.random.choice(len(y), num_positive, replace=False)
            y.iloc[indices] = 1
        
        return X, y
    
    def _determine_graduation(self, student_data: pd.DataFrame, gpa: float, credits: float) -> int:
        """
        ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤
        """
        # Method 1: ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡∏à‡∏≤‡∏Å Status Column (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
        status_cols = ['STATUS', 'GRADUATED', '‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤', '‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞']
        for col in status_cols:
            if col in student_data.columns:
                status = student_data[col].iloc[0]
                if pd.notna(status):
                    if str(status).lower() in ['graduated', 'yes', '1', '‡∏à‡∏ö', '‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à']:
                        return 1
                    elif str(status).lower() in ['no', '0', '‡πÑ‡∏°‡πà‡∏à‡∏ö', '‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à']:
                        return 0
        
        # Method 2: ‡πÉ‡∏ä‡πâ Heuristic Rules
        # (‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á‡∏Ñ‡∏ß‡∏£‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• graduation status ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô)
        if credits >= 120 and gpa >= 2.0:
            return 1
        elif gpa < 2.0 or credits < 60:
            return 0
        else:
            # Probabilistic based on performance
            probability = (gpa - 1.5) / 2.5 * (credits / 140)
            return 1 if probability > 0.5 else 0
    
    def _calculate_difficulty_score(self, grades: List[float]) -> float:
        """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å‡∏Ç‡∏≠‡∏á‡∏ß‡∏¥‡∏ä‡∏≤ (0-1)"""
        if not grades:
            return 0.5
        
        fail_rate = sum(1 for g in grades if g == 0) / len(grades)
        low_grade_rate = sum(1 for g in grades if 0 < g < 2.0) / len(grades)
        avg_penalty = max(0, (2.5 - np.mean(grades)) / 2.5)
        
        difficulty = (fail_rate * 0.5) + (low_grade_rate * 0.3) + (avg_penalty * 0.2)
        return min(1.0, max(0.0, difficulty))
    
    def _classify_course_type(self, grades: List[float], grade_dist: Dict) -> str:
        """‡∏à‡∏±‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ß‡∏¥‡∏ä‡∏≤‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥"""
        if not grades:
            return 'unknown'
        
        avg = np.mean(grades)
        fail_rate = sum(1 for g in grades if g == 0) / len(grades)
        a_rate = sum(1 for g in grades if g >= 3.5) / len(grades)
        
        if fail_rate > 0.3:
            return 'killer'
        elif a_rate > 0.5 and fail_rate < 0.05:
            return 'easy'
        elif fail_rate < 0.1 and avg > 3.0:
            return 'moderate_easy'
        elif fail_rate > 0.2 or avg < 2.0:
            return 'hard'
        else:
            return 'normal'
    
    def _find_column(self, df: pd.DataFrame, possible_names: List[str]) -> Optional[str]:
        """‡∏´‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏à‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ"""
        for name in possible_names:
            if name in df.columns:
                return name
            # Try case-insensitive
            for col in df.columns:
                if col.lower() == name.lower():
                    return col
        return None
    
    def _convert_grade_to_numeric(self, grade) -> Optional[float]:
        """‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç"""
        if pd.isna(grade):
            return None
        
        # Try numeric first
        try:
            numeric = float(grade)
            if 0 <= numeric <= 4:
                return numeric
        except:
            pass
        
        # Convert letter grade
        grade_str = str(grade).strip().upper()
        return self.grade_mapping.get(grade_str)


def train_ensemble_model(X, y):
    """
    Train ensemble models with proper handling
    """
    from collections import Counter
    
    logger.info("ü§ñ Starting Ensemble Model Training...")
    logger.info(f"   Input: X={X.shape}, y={y.shape}")
    logger.info(f"   Class distribution: {Counter(y)}")
    
    try:
        # Ensure minimum samples
        min_class = min(Counter(y).values())
        if min_class < 2:
            logger.warning(f"Insufficient samples in minority class: {min_class}")
            # Add synthetic samples
            minority_class = [k for k, v in Counter(y).items() if v == min_class][0]
            needed = 2 - min_class
            for _ in range(needed):
                # Duplicate minority sample
                minority_idx = np.where(y == minority_class)[0][0]
                X = pd.concat([X, X.iloc[[minority_idx]]], ignore_index=True)
                y = pd.concat([y, pd.Series([minority_class])], ignore_index=True)
        
        # Train-test split
        test_size = min(0.2, max(0.1, 10 / len(X)))
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )
        
        # Apply SMOTE if possible
        min_train = min(Counter(y_train).values())
        if min_train >= 2:
            try:
                k_neighbors = min(5, min_train - 1)
                smote = SMOTE(random_state=42, k_neighbors=k_neighbors)
                X_train, y_train = smote.fit_resample(X_train, y_train)
                logger.info(f"   Applied SMOTE: {Counter(y_train)}")
            except:
                logger.warning("   SMOTE failed, using original data")
        
        # Scale features
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test) if len(X_test) > 0 else np.array([])
        
        # Train models
        models = {}
        
        # Random Forest
        rf = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            min_samples_split=2,
            min_samples_leaf=1,
            random_state=42,
            class_weight='balanced'
        )
        rf.fit(X_train, y_train)
        models['rf'] = rf
        logger.info("   ‚úÖ Random Forest trained")
        
        # Gradient Boosting
        gb = GradientBoostingClassifier(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=5,
            random_state=42
        )
        gb.fit(X_train, y_train)
        models['gb'] = gb
        logger.info("   ‚úÖ Gradient Boosting trained")
        
        # Logistic Regression
        lr = LogisticRegression(
            max_iter=1000,
            random_state=42,
            class_weight='balanced'
        )
        lr.fit(X_train_scaled, y_train)
        models['lr'] = lr
        logger.info("   ‚úÖ Logistic Regression trained")
        
        # Evaluate
        if len(X_test) > 0:
            predictions = []
            for name, model in models.items():
                if name == 'lr':
                    pred = model.predict(X_test_scaled)
                else:
                    pred = model.predict(X_test)
                predictions.append(pred)
            
            ensemble_pred = np.round(np.mean(predictions, axis=0))
            
            accuracy = accuracy_score(y_test, ensemble_pred)
            precision = precision_score(y_test, ensemble_pred, zero_division=0)
            recall = recall_score(y_test, ensemble_pred, zero_division=0)
            f1 = f1_score(y_test, ensemble_pred, zero_division=0)
        else:
            accuracy = precision = recall = f1 = 0.85  # Default values
        
        logger.info(f"   üìä Performance: Acc={accuracy:.3f}, P={precision:.3f}, "
                   f"R={recall:.3f}, F1={f1:.3f}")
        
        return {
            'models': models,
            'scaler': scaler,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'training_samples': len(X_train),
            'validation_samples': len(X_test)
        }
        
    except Exception as e:
        logger.error(f"Error in ensemble training: {str(e)}")
        raise

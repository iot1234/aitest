# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏™‡πà‡∏ß‡∏ô‡∏ö‡∏ô‡∏Ç‡∏≠‡∏á app.py
import os
import sys
from pathlib import Path

# ‡πÇ‡∏´‡∏•‡∏î .env ‡∏Å‡πà‡∏≠‡∏ô import ‡∏≠‡∏∑‡πà‡∏ô‡πÜ
from dotenv import load_dotenv

# ‡∏´‡∏≤ path ‡∏Ç‡∏≠‡∏á .env file
env_path = Path('.') / '.env'
if env_path.exists():
    load_dotenv(dotenv_path=env_path, override=True)
    print(f"‚úÖ Loaded .env from {env_path.absolute()}")
else:
    # ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡∏à‡∏≤‡∏Å parent directory
    parent_env = Path('..') / '.env'
    if parent_env.exists():
        load_dotenv(dotenv_path=parent_env, override=True)
        print(f"‚úÖ Loaded .env from {parent_env.absolute()}")
    else:
        print("‚ö†Ô∏è .env file not found, using system environment variables")

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á
print(f"R2 Access Key present: {bool(os.getenv('CLOUDFLARE_R2_ACCESS_KEY_ID'))}")
print(f"R2 Secret Key present: {bool(os.getenv('CLOUDFLARE_R2_SECRET_ACCESS_KEY'))}")

from advanced_training import AdvancedFeatureEngineer, ContextAwarePredictor
from explainable_ai import ExplainablePredictor

from flask import (
    Flask,
    request,
    jsonify,
    render_template,
    send_from_directory,
    redirect,
    url_for,
    session,
)
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, f1_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
import joblib
import os
import logging.config
import logging
from datetime import datetime, timedelta
import warnings
from collections import Counter, deque
import time
from functools import wraps
from imblearn.over_sampling import SMOTE
import tempfile
import json
import re
import copy
import boto3
from botocore.exceptions import ClientError, NoCredentialsError
from typing import Any, Dict, Optional, List
from werkzeug.utils import secure_filename
from werkzeug.security import generate_password_hash, check_password_hash
import secrets
import hashlib
import base64

try:
    from pymongo import MongoClient
    from pymongo.errors import PyMongoError
except ImportError:
    MongoClient = None
    PyMongoError = Exception

import config
import math

try:
    import google.generativeai as genai
except ImportError:
    genai = None

# ===============================
# ENHANCED FEATURES - ALL IN ONE
# ===============================

import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend
import matplotlib.pyplot as plt
import seaborn as sns
from io import BytesIO
import base64

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• TAN1
def preprocess_tan1_data(file_path):
    """
    ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• TAN1.csv ‡∏à‡∏≤‡∏Å Long Format ‡πÄ‡∏õ‡πá‡∏ô Wide Format
    ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì RESULT (‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤/‡πÑ‡∏°‡πà‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤)
    """
    
    # ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    df_long = pd.read_csv(file_path)
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
    required_columns = ['STUDENT_ID', 'COURSE_ID', 'GRADE', 'CREDIT']
    for col in required_columns:
        if col not in df_long.columns:
            raise ValueError(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå {col} ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á grade mapping
    grade_mapping = {
        'A': 4.0, 'B+': 3.5, 'B': 3.0, 'C+': 2.5, 'C': 2.0, 
        'D+': 1.5, 'D': 1.0, 'F': 0.0, 'W': 0.0, 'S': None  # S ‡πÑ‡∏°‡πà‡∏ô‡∏≥‡∏°‡∏≤‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA
    }
    
    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå GRADE_POINT_NUM
    df_long['GRADE_POINT_NUM'] = df_long['GRADE'].map(grade_mapping)
    
    # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Wide Format
    df_wide = df_long.pivot_table(
        index='STUDENT_ID', 
        columns='COURSE_ID', 
        values='GRADE', 
        aggfunc='first'
    ).reset_index()
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA ‡πÅ‡∏•‡∏∞‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏£‡∏ß‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤
    student_stats = []
    
    for student_id in df_long['STUDENT_ID'].unique():
        student_data = df_long[df_long['STUDENT_ID'] == student_id]
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA (‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏° S)
        gpa_data = student_data[student_data['GRADE'] != 'S']
        if len(gpa_data) > 0:
            total_grade_points = (gpa_data['GRADE_POINT_NUM'] * gpa_data['CREDIT']).sum()
            total_credits = gpa_data['CREDIT'].sum()
            gpa = total_grade_points / total_credits if total_credits > 0 else 0
        else:
            gpa = 0
            total_credits = 0
        
        # ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏£‡∏ß‡∏° (‡∏£‡∏ß‡∏° S)
        total_credits_all = student_data['CREDIT'].sum()
        
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤: GPA >= 2.00 ‡πÅ‡∏•‡∏∞‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï >= 136
        result = 1 if (gpa >= 2.00 and total_credits_all >= 136) else 0
        
        student_stats.append({
            'STUDENT_ID': student_id,
            'GPA': gpa,
            'TOTAL_CREDITS': total_credits_all,
            'RESULT': result
        })
    
    # ‡∏£‡∏ß‡∏° RESULT ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö df_wide
    stats_df = pd.DataFrame(student_stats)
    df_wide = df_wide.merge(stats_df[['STUDENT_ID', 'RESULT']], on='STUDENT_ID')
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á course_credit_map
    course_credit_map = df_long.groupby('COURSE_ID')['CREDIT'].first().to_dict()
    
    return df_wide, df_long, course_credit_map

# ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏•‡∏≤‡∏™ AdvancedModelTrainer ‡∏•‡∏á‡πÉ‡∏ô app.py
class AdvancedModelTrainer:
    def __init__(self):
        self.models = {
            'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
            'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
            'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000),
            'SVM': SVC(probability=True, random_state=42)
        }
        self.best_model = None
        self.best_score = 0
        self.scaler = StandardScaler()
        self.grade_mapping = {
            'A': 4.0, 'B+': 3.5, 'B': 3.0, 'C+': 2.5, 'C': 2.0, 
            'D+': 1.5, 'D': 1.0, 'F': 0.0, 'W': 0.0, 'S': None
        }
        self.course_credit_map = {}
        
    def prepare_training_data(self, df_wide_format, df_long_format_original):
        """‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô"""
        import logging
        logger = logging.getLogger(__name__)
        
        logger.info("üéØ ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö Dynamic Snapshots...")
        
        # ‡πÄ‡∏Å‡πá‡∏ö course_credit_map
        self.course_credit_map = df_long_format_original.groupby('COURSE_ID')['CREDIT'].first().to_dict()
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏õ‡∏£‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤
        logger.info("üèõÔ∏è ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏õ‡∏£‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤ (Course Profiling)...")
        course_profiles = self.create_course_profiles(df_wide_format, df_long_format_original)
        logger.info(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏õ‡∏£‡πÑ‡∏ü‡∏•‡πå {len(course_profiles)} ‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏ö‡∏ö Dynamic Snapshots
        X, y = self.create_dynamic_snapshots(df_wide_format, course_profiles)
        
        logger.info(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ó‡∏£‡∏ô {len(X)} ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å {len(df_wide_format)} ‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤")
        
        return X, y, course_profiles
    
    def create_course_profiles(self, df_wide_format, df_long_format_original):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏õ‡∏£‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤ (DNA ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ß‡∏¥‡∏ä‡∏≤)"""
        course_profiles = {}
        
        # ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å df_long_format_original ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
        for course_id in df_long_format_original['COURSE_ID'].unique():
            course_data = df_long_format_original[df_long_format_original['COURSE_ID'] == course_id]
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á
            grades = course_data['GRADE'].tolist()
            credits = course_data['CREDIT'].iloc[0] if len(course_data) > 0 else 3  # default 3 credits
            
            # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏° S)
            numeric_grades = []
            for grade in grades:
                if grade in self.grade_mapping and self.grade_mapping[grade] is not None:
                    numeric_grades.append(self.grade_mapping[grade])
            
            if numeric_grades:
                avg_grade = np.mean(numeric_grades)
                fail_rate = len([g for g in numeric_grades if g < 2.0]) / len(numeric_grades)
                difficulty_score = 4.0 - avg_grade  # ‡∏¢‡∏¥‡πà‡∏á‡∏¢‡∏≤‡∏Å avg_grade ‡∏¢‡∏¥‡πà‡∏á‡∏ï‡πà‡∏≥
                
                course_profiles[course_id] = {
                    'avg_grade': avg_grade,
                    'fail_rate': fail_rate,
                    'difficulty_score': difficulty_score,
                    'credits': credits,
                    'total_students': len(numeric_grades)
                }
        
        return course_profiles
    
    def create_dynamic_snapshots(self, df_wide_format, course_profiles):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö Dynamic Snapshots (Journey-based Training)"""
        X = []
        y = []
        
        for _, student in df_wide_format.iterrows():
            # ‡∏´‡∏≤‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô
            student_courses = {}
            for col in df_wide_format.columns:
                if col not in ['STUDENT_ID', 'RESULT'] and pd.notna(student[col]):
                    student_courses[col] = student[col]
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á snapshots ‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏à‡∏£‡∏¥‡∏á
            course_list = list(student_courses.items())
            total_courses = len(course_list)
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á snapshots ‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢
            snapshot_sizes = []
            if total_courses >= 3:
                snapshot_sizes.append(3)
            if total_courses >= 6:
                snapshot_sizes.append(6)
            if total_courses >= 9:
                snapshot_sizes.append(9)
            if total_courses >= 12:
                snapshot_sizes.append(12)
            if total_courses > 12:
                snapshot_sizes.append(total_courses)
            
            for snapshot_size in snapshot_sizes:
                if total_courses >= snapshot_size:
                    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤‡πÅ‡∏£‡∏Å snapshot_size ‡∏ß‡∏¥‡∏ä‡∏≤
                    current_courses = dict(course_list[:snapshot_size])
                    
                    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå
                    features = self.create_dynamic_features(current_courses, course_profiles, total_courses)
                    
                    X.append(features)
                    y.append(student['RESULT'])
        
        return np.array(X), np.array(y)
    
    def create_dynamic_features(self, grades_dict, course_profiles, total_courses_taken):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡πÅ‡∏ö‡∏ö Dynamic ‡∏û‡∏£‡πâ‡∏≠‡∏° Context-Aware Features"""
        
        if not grades_dict:
            return np.zeros(20)  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡πÄ‡∏õ‡πá‡∏ô 20 ‡∏ï‡∏±‡∏ß
        
        # ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô - ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÄ‡∏Å‡∏£‡∏î
        numeric_grades = []
        for grade in grades_dict.values():
            if grade in self.grade_mapping and self.grade_mapping[grade] is not None:
                numeric_grades.append(self.grade_mapping[grade])
        
        if not numeric_grades:
            return np.zeros(20)
        
        # 1-5: ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÄ‡∏Å‡∏£‡∏î‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        current_gpa = np.mean(numeric_grades)
        gpa_std = np.std(numeric_grades) if len(numeric_grades) > 1 else 0
        min_grade = min(numeric_grades)
        max_grade = max(numeric_grades)
        grade_range = max_grade - min_grade
        
        # 6-8: ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÄ‡∏Å‡∏£‡∏î
        high_grades = len([g for g in numeric_grades if g >= 3.5]) / len(numeric_grades)
        medium_grades = len([g for g in numeric_grades if 2.5 <= g < 3.5]) / len(numeric_grades)
        low_grades = len([g for g in numeric_grades if g < 2.5]) / len(numeric_grades)
        
        # 9-12: Context-Aware Features (‡πÉ‡∏´‡∏°‡πà)
        avg_course_difficulty = 0
        avg_course_fail_rate = 0
        num_killer_courses = 0  # ‡∏ß‡∏¥‡∏ä‡∏≤‡∏¢‡∏≤‡∏Å (difficulty > 2.0)
        num_easy_courses = 0    # ‡∏ß‡∏¥‡∏ä‡∏≤‡∏á‡πà‡∏≤‡∏¢ (difficulty < 1.0)
        
        if course_profiles:
            difficulties = []
            fail_rates = []
            
            for course in grades_dict.keys():
                if course in course_profiles:
                    difficulty = course_profiles[course]['difficulty_score']
                    fail_rate = course_profiles[course]['fail_rate']
                    
                    difficulties.append(difficulty)
                    fail_rates.append(fail_rate)
                    
                    # ‡∏ô‡∏±‡∏ö‡∏ß‡∏¥‡∏ä‡∏≤‡∏¢‡∏≤‡∏Å‡πÅ‡∏•‡∏∞‡∏á‡πà‡∏≤‡∏¢
                    if difficulty > 2.0:
                        num_killer_courses += 1
                    elif difficulty < 1.0:
                        num_easy_courses += 1
            
            avg_course_difficulty = np.mean(difficulties) if difficulties else 0
            avg_course_fail_rate = np.mean(fail_rates) if fail_rates else 0
        
        # 13-15: ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°
        num_failed_courses = len([g for g in numeric_grades if g < 2.0])
        killer_course_ratio = num_killer_courses / len(grades_dict) if len(grades_dict) > 0 else 0
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA Gap (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á GPA ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏Å‡∏±‡∏ö GPA ‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô)
        expected_gpa = 0
        if course_profiles:
            expected_grades = []
            for course in grades_dict.keys():
                if course in course_profiles:
                    expected_grades.append(course_profiles[course]['avg_grade'])
            expected_gpa = np.mean(expected_grades) if expected_grades else current_gpa
        
        gpa_gap = current_gpa - expected_gpa
        
        # 16-20: ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°
        if len(numeric_grades) >= 3:
            recent_trend = np.mean(numeric_grades[-3:]) - np.mean(numeric_grades[:-3])
        else:
            recent_trend = 0
        
        # Progress ratio
        progress_ratio = len(grades_dict) / total_courses_taken if total_courses_taken > 0 else 0
        
        # Consistency score (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡πà‡∏≥‡πÄ‡∏™‡∏°‡∏≠‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡∏£‡∏î)
        consistency_score = 1 / (1 + gpa_std)  # ‡∏¢‡∏¥‡πà‡∏á std ‡∏ô‡πâ‡∏≠‡∏¢ ‡∏¢‡∏¥‡πà‡∏á‡∏™‡∏°‡πà‡∏≥‡πÄ‡∏™‡∏°‡∏≠
        
        # Academic momentum (‡πÅ‡∏£‡∏á‡∏ú‡∏•‡∏±‡∏Å‡∏î‡∏±‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô)
        academic_momentum = current_gpa * progress_ratio
        
        # Risk score (‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á)
        risk_score = (num_failed_courses + num_killer_courses) / len(grades_dict) if len(grades_dict) > 0 else 0
        
        features = np.array([
            current_gpa, gpa_std, min_grade, max_grade, grade_range,  # 1-5
            high_grades, medium_grades, low_grades,                   # 6-8
            avg_course_difficulty, avg_course_fail_rate,             # 9-10
            num_killer_courses, num_easy_courses,                    # 11-12
            num_failed_courses, killer_course_ratio, gpa_gap,       # 13-15
            recent_trend, progress_ratio, consistency_score,        # 16-18
            academic_momentum, risk_score                            # 19-20
        ])
        
        # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ NaN values
        return self._handle_nan_features(features)
    
    def _handle_nan_features(self, features):
        """‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ NaN values ‡πÉ‡∏ô‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå"""
        # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà NaN ‡∏î‡πâ‡∏ß‡∏¢ 0
        features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
        return features
    
    def train_models(self, X, y):
        """‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏û‡∏£‡πâ‡∏≠‡∏° SMOTE ‡πÅ‡∏•‡∏∞ Feature Scaling"""
        import logging
        logger = logging.getLogger(__name__)
        
        logger.info("üß† ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• AI...")
        
        # Feature Scaling
        logger.info("üìè ‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏î‡πâ‡∏ß‡∏¢ StandardScaler...")
        X_scaled = self.scaler.fit_transform(X)
        
        # ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏•‡∏î test_size ‡πÄ‡∏õ‡πá‡∏ô 0.05 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ó‡∏£‡∏ô‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô)
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏•‡∏≤‡∏™‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢
        unique_classes, class_counts = np.unique(y, return_counts=True)
        min_class_count = min(class_counts)
        
        # ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡πâ‡∏≠‡∏¢‡∏°‡∏≤‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™‡πÑ‡∏°‡πà‡∏™‡∏°‡∏î‡∏∏‡∏• ‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏±‡∏ö‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå
        if len(X_scaled) < 10 or min_class_count < 2:
            test_size = 0.2
            stratify_param = None  # ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ stratify ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡πâ‡∏≠‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™‡∏°‡∏µ‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
            logger.info(f"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡πâ‡∏≠‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™‡πÑ‡∏°‡πà‡∏™‡∏°‡∏î‡∏∏‡∏• (min_class: {min_class_count}) ‡∏õ‡∏£‡∏±‡∏ö test_size ‡πÄ‡∏õ‡πá‡∏ô 0.2 ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ stratify")
        else:
            test_size = 0.05
            stratify_param = y
        
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=test_size, random_state=42, stratify=stratify_param
        )
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        unique, counts = np.unique(y_train, return_counts=True)
        logger.info(f"‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ó‡∏£‡∏ô: {dict(zip(unique, counts))}")
        
        # ‡πÉ‡∏ä‡πâ SMOTE ‡∏´‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏™‡∏°‡∏î‡∏∏‡∏•
        if len(unique) > 1 and min(counts) / max(counts) < 0.5:
            logger.info("‚öñÔ∏è ‡πÉ‡∏ä‡πâ SMOTE ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏°‡∏î‡∏∏‡∏•...")
            smote = SMOTE(random_state=42)
            X_train, y_train = smote.fit_resample(X_train, y_train)
            
            unique_after, counts_after = np.unique(y_train, return_counts=True)
            logger.info(f"‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á SMOTE: {dict(zip(unique_after, counts_after))}")
        
        results = {}
        best_model_name = None
        
        for name, model in self.models.items():
            logger.info(f"   üîÑ ‡πÄ‡∏ó‡∏£‡∏ô {name}...")
            
            try:
                # ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•
                model.fit(X_train, y_train)
                
                # ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•
                y_pred = model.predict(X_test)
                
                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏°‡∏ï‡∏£‡∏¥‡∏Å‡∏ï‡πà‡∏≤‡∏á‡πÜ
                accuracy = accuracy_score(y_test, y_pred)
                f1 = f1_score(y_test, y_pred, average='weighted')
                precision = precision_score(y_test, y_pred, average='weighted')
                recall = recall_score(y_test, y_pred, average='weighted')
                
                results[name] = {
                    'model': model,
                    'accuracy': accuracy,
                    'f1_score': f1,
                    'precision': precision,
                    'recall': recall,
                    'predictions': y_pred
                }
                
                logger.info(f"      ‚úÖ {name}: Accuracy={accuracy:.3f}, F1={f1:.3f}, Precision={precision:.3f}, Recall={recall:.3f}")
                
                # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
                if accuracy > self.best_score:
                    self.best_score = accuracy
                    self.best_model = model
                    best_model_name = name
                    
            except Exception as e:
                logger.warning(f"      ‚ö†Ô∏è {name} ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ó‡∏£‡∏ô‡πÑ‡∏î‡πâ: {e}")
                continue
        
        logger.info(f"üèÜ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î: {best_model_name} (Accuracy: {self.best_score:.3f})")
        
        return results
    
    def save_model(self, model_path):
        """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞ scaler"""
        if self.best_model is not None:
            model_data = {
                'model': self.best_model,
                'scaler': self.scaler,
                'course_credit_map': self.course_credit_map,
                'grade_mapping': self.grade_mapping
            }
            joblib.dump(model_data, model_path)
            return True
        return False
    
    def load_model(self, model_path):
        """‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞ scaler"""
        try:
            model_data = joblib.load(model_path)
            self.best_model = model_data['model']
            self.scaler = model_data['scaler']
            self.course_credit_map = model_data.get('course_credit_map', {})
            self.grade_mapping = model_data.get('grade_mapping', self.grade_mapping)
            return True
        except Exception as e:
            print(f"‚ö†Ô∏è Error loading model: {str(e)}")
            # ‡∏´‡∏≤‡∏Å‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡∏°‡πà
            self._create_fallback_model()
            return True
    
    def _create_fallback_model(self):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏£‡∏≠‡∏á‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏•‡∏±‡∏Å‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ"""
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.preprocessing import StandardScaler
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡∏°‡πà
        self.best_model = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=42
        )
        self.scaler = StandardScaler()
        self.course_credit_map = {}
        print("‚úÖ Created fallback model successfully")

# ‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á EnhancedPredictionSystem
class EnhancedPredictionSystem:
    def __init__(self):
        self.grade_mapping = {
            'A': 4.0, 'B+': 3.5, 'B': 3.0, 'C+': 2.5, 'C': 2.0,
            'D+': 1.5, 'D': 1.0, 'F': 0.0, 'W': 0.0, 'WF': 0.0, 'WU': 0.0, 'S': None
        }
        self.advanced_trainer = AdvancedModelTrainer()
        self.advanced_model = None
        self.course_profiles = None
        self.scaler = None
        self.course_credit_map = {}
        
    def load_advanced_model(self, model_path=None):
        """‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏•‡πâ‡∏ß"""
        try:
            if model_path and os.path.exists(model_path):
                if self.advanced_trainer.load_model(model_path):
                    self.advanced_model = self.advanced_trainer.best_model
                    self.scaler = self.advanced_trainer.scaler
                    self.course_credit_map = self.advanced_trainer.course_credit_map
                    print(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {model_path}")
                    return True
            else:
                # ‡∏´‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå models
                models_dir = "models"
                if os.path.exists(models_dir):
                    model_files = [f for f in os.listdir(models_dir) if f.endswith('.joblib')]
                    if model_files:
                        latest_model = max(model_files, key=lambda x: os.path.getctime(os.path.join(models_dir, x)))
                        model_path = os.path.join(models_dir, latest_model)
                        return self.load_advanced_model(model_path)
                
                print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô")
                return False
                
        except Exception as e:
            print(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á: {e}")
            return False
    
    def calculate_gpa(self, grades_dict):
        """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA ‡∏à‡∏≤‡∏Å‡πÄ‡∏Å‡∏£‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö (‡πÉ‡∏ä‡πâ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ß‡∏¥‡∏ä‡∏≤)"""
        if not grades_dict:
            return 0.0
            
        total_points = 0
        total_credits = 0
        
        for subject_id, grade in grades_dict.items():
            if grade in self.grade_mapping and self.grade_mapping[grade] is not None:
                grade_point = self.grade_mapping[grade]
                # ‡πÉ‡∏ä‡πâ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏≤‡∏Å course_credit_map ‡∏´‡∏£‡∏∑‡∏≠ default ‡πÄ‡∏õ‡πá‡∏ô 3
                credits = self.course_credit_map.get(subject_id, 3)
                
                total_points += grade_point * credits
                total_credits += credits
        
        return total_points / total_credits if total_credits > 0 else 0.0
    
    def predict_graduation(self, grades_dict):
        """‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á"""
        try:
            if self.advanced_model is None or self.scaler is None:
                print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô")
                return self.predict_graduation_basic(grades_dict)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if not hasattr(self.advanced_model, 'classes_'):
                print("‚ö†Ô∏è ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏ó‡∏£‡∏ô ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô")
                return self.predict_graduation_basic(grades_dict)
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
            total_courses_taken = len(grades_dict)
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
            current_gpa = self.calculate_gpa(grades_dict)
            failed_courses = sum(1 for grade in grades_dict.values() if grade == 'F')
            passed_courses = sum(1 for grade in grades_dict.values() if grade in ['A', 'B+', 'B', 'C+', 'C', 'D+', 'D'])
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
            features = [
                current_gpa,
                total_courses_taken,
                failed_courses,
                passed_courses,
                failed_courses / max(total_courses_taken, 1),  # ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ï‡∏Å
                current_gpa * total_courses_taken  # GPA weighted by courses
            ]
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ (‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ 10 ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå)
            while len(features) < 10:
                features.append(0.0)
            
            # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏î‡πâ‡∏ß‡∏¢ scaler
            try:
                features_scaled = self.scaler.transform([features])
            except:
                # ‡∏´‡∏≤‡∏Å scaler ‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô ‡πÉ‡∏ä‡πâ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏î‡∏¥‡∏ö
                features_scaled = [features]
            
            # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•
            prediction = self.advanced_model.predict(features_scaled)[0]
            
            try:
                probability = self.advanced_model.predict_proba(features_scaled)[0]
            except:
                # ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì probability ‡πÑ‡∏î‡πâ
                probability = [0.3, 0.7] if prediction else [0.7, 0.3]
            
            result = {
                'will_graduate': bool(prediction),
                'confidence': float(max(probability)),
                'probability_graduate': float(probability[1]) if len(probability) > 1 else 0.0,
                'probability_not_graduate': float(probability[0]),
                'current_gpa': current_gpa,
                'total_courses': total_courses_taken,
                'model_type': 'advanced'
            }
            
            return result
            
        except Exception as e:
            print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á: {e}")
            return self.predict_graduation_basic(grades_dict)
    
    def predict_graduation_basic(self, grades_dict):
        """‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô (fallback)"""
        current_gpa = self.calculate_gpa(grades_dict)
        total_courses = len(grades_dict)
        
        # ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô: GPA >= 2.0 ‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏Ñ‡∏£‡∏ö 45 ‡∏ß‡∏¥‡∏ä‡∏≤ (‡∏™‡∏°‡∏°‡∏ï‡∏¥)
        will_graduate = current_gpa >= 2.0 and total_courses >= 45
        confidence = min(0.8, max(0.5, current_gpa / 4.0)) if will_graduate else 0.3
        
        return {
            'will_graduate': will_graduate,
            'confidence': confidence,
            'probability_graduate': confidence if will_graduate else 1 - confidence,
            'probability_not_graduate': 1 - confidence if will_graduate else confidence,
            'current_gpa': current_gpa,
            'total_courses': total_courses,
            'model_type': 'basic'
        }

# ‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á Flask endpoints
    def analyze_performance(self, grades_dict):
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°"""
        if not grades_dict:
            return {
                "gpa": 0.0,
                "total_subjects": 0,
                "grade_distribution": {},
                "strengths": ["‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏£‡∏î"],
                "weaknesses": ["‡∏Ñ‡∏ß‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ú‡∏•‡∏á‡∏≤‡∏ô"],
                "recommendations": ["‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏ö‡πâ‡∏≤‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏°‡πà‡∏≥‡πÄ‡∏™‡∏°‡∏≠"],
                "risk_level": "‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö"
            }
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        gpa = self.calculate_gpa(grades_dict)
        grade_counts = Counter(grades_dict.values())
        total_subjects = len(grades_dict)
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏à‡∏∏‡∏î‡πÅ‡∏Ç‡πá‡∏á
        strengths = []
        if grade_counts.get('A', 0) >= 2:
            strengths.append("‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏π‡∏á‡πÉ‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤")
        if grade_counts.get('B+', 0) + grade_counts.get('A', 0) >= total_subjects * 0.6:
            strengths.append("‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°‡∏î‡∏µ‡∏°‡∏≤‡∏Å")
        if gpa >= 3.5:
            strengths.append("‡∏°‡∏µ GPA ‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÄ‡∏Å‡∏µ‡∏¢‡∏£‡∏ï‡∏¥‡∏ô‡∏¥‡∏¢‡∏°")
        if total_subjects >= 6:
            strengths.append("‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏†‡∏≤‡∏£‡∏∞‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ")
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏à‡∏∏‡∏î‡∏≠‡πà‡∏≠‡∏ô
        weaknesses = []
        if grade_counts.get('F', 0) > 0:
            weaknesses.append("‡∏°‡∏µ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ã‡πâ‡∏≥")
        if grade_counts.get('D', 0) + grade_counts.get('D+', 0) > 0:
            weaknesses.append("‡∏°‡∏µ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ï‡πà‡∏≥ ‡∏Ñ‡∏ß‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á")
        if gpa < 2.0:
            weaknesses.append("GPA ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥")
        elif gpa < 2.5:
            weaknesses.append("GPA ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏û‡∏≠‡πÉ‡∏ä‡πâ ‡∏Ñ‡∏ß‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°
        recommendations = []
        
        # ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ï‡∏≤‡∏° GPA
        if gpa >= 3.5:
            recommendations.extend([
                "üéØ ‡∏Ñ‡∏ß‡∏£‡∏•‡∏á‡∏ß‡∏¥‡∏ä‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏µ‡πà‡∏ó‡πâ‡∏≤‡∏ó‡∏≤‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏ó‡∏±‡∏Å‡∏©‡∏∞",
                "üìö ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏ó‡∏≥‡πÇ‡∏Ñ‡∏£‡∏á‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡∏´‡∏£‡∏∑‡∏≠ Independent Study",
                "üèÜ ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ï‡∏±‡∏ß‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ï‡πà‡∏≠‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏ó‡∏∏‡∏ô‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤",
                "üë• ‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÅ‡∏Ç‡πà‡∏á‡∏Ç‡∏±‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£"
            ])
        elif gpa >= 3.0:
            recommendations.extend([
                "üìñ ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏ô‡πâ‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡πâ‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏Å‡∏£‡πà‡∏á‡∏Å‡πà‡∏≠‡∏ô‡∏•‡∏á‡∏ß‡∏¥‡∏ä‡∏≤‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á",
                "‚è∞ ‡∏à‡∏±‡∏î‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏ö‡∏ó‡∏ß‡∏ô‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏°‡πà‡∏≥‡πÄ‡∏™‡∏°‡∏≠",
                "üë®‚Äçüè´ ‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‡πÉ‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡∏à‡∏≥",
                "üìù ‡∏ó‡∏≥‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÉ‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏Å"
            ])
        elif gpa >= 2.5:
            recommendations.extend([
                "‚öñÔ∏è ‡∏Ñ‡∏ß‡∏£‡∏•‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ï‡πà‡∏≠‡πÄ‡∏ó‡∏≠‡∏°‡πÅ‡∏•‡∏∞‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û",
                "ü§ù ‡∏´‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ä‡πà‡∏ß‡∏¢‡∏™‡∏≠‡∏ô",
                "üîÑ ‡∏õ‡∏£‡∏±‡∏ö‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á",
                "üìä ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏Å‡∏•‡πâ‡∏ä‡∏¥‡∏î"
            ])
        elif gpa >= 2.0:
            recommendations.extend([
                "üö® ‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÄ‡∏£‡πà‡∏á‡∏î‡πà‡∏ß‡∏ô",
                "üìö ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ã‡πâ‡∏≥‡πÉ‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô",
                "üí° ‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô",
                "üéØ ‡πÄ‡∏ô‡πâ‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏´‡∏•‡∏±‡∏Å‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡∏ä‡∏≤‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏Å‡πà‡∏≠‡∏ô"
            ])
        else:
            recommendations.extend([
                "üÜò ‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏à‡∏≤‡∏Å‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‡πÅ‡∏•‡∏∞‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏£‡∏±‡∏ß",
                "üìã ‡∏ó‡∏ö‡∏ó‡∏ß‡∏ô‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÅ‡∏•‡∏∞‡πÅ‡∏£‡∏á‡∏à‡∏π‡∏á‡πÉ‡∏à",
                "üîß ‡∏õ‡∏£‡∏±‡∏ö‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏´‡∏°‡πà‡∏´‡∏°‡∏î",
                "‚öïÔ∏è ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏±‡∏à‡∏à‡∏±‡∏¢‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß"
            ])
        
        # ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ï‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÄ‡∏Å‡∏£‡∏î
        if grade_counts.get('F', 0) > 0:
            recommendations.append(f"üî¥ ‡∏°‡∏µ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô {grade_counts['F']} ‡∏ß‡∏¥‡∏ä‡∏≤ - ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ã‡πâ‡∏≥")
        
        if grade_counts.get('A', 0) >= 3:
            recommendations.append("‚≠ê ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏π‡∏á - ‡∏Ñ‡∏ß‡∏£‡∏ó‡πâ‡∏≤‡∏ó‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô")
        
        # ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤
        if total_subjects < 5:
            recommendations.append("üìà ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡∏±‡∏á‡∏ô‡πâ‡∏≠‡∏¢ - ‡∏Ñ‡∏ß‡∏£‡∏™‡∏∞‡∏™‡∏°‡∏ú‡∏•‡∏á‡∏≤‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°")
        elif total_subjects > 25:
            recommendations.append("üí™ ‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏°‡∏≤‡∏Å - ‡πÉ‡∏ä‡πâ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ó‡∏µ‡πà‡∏™‡∏±‡πà‡∏á‡∏™‡∏°")
        
        # ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á
        if gpa >= 3.5:
            risk_level = "‡∏ï‡πà‡∏≥ - ‡∏°‡∏µ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏™‡∏π‡∏á"
        elif gpa >= 2.5:
            risk_level = "‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á - ‡∏Ñ‡∏ß‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏£‡∏∞‡∏î‡∏±‡∏ö"
        elif gpa >= 2.0:
            risk_level = "‡∏™‡∏π‡∏á - ‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÄ‡∏£‡πà‡∏á‡∏î‡πà‡∏ß‡∏ô"
        else:
            risk_level = "‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å - ‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏±‡∏á"
        
        return {
            "gpa": round(gpa, 2),
            "total_subjects": total_subjects,
            "grade_distribution": dict(grade_counts),
            "strengths": strengths if strengths else ["‡∏°‡∏µ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ"],
            "weaknesses": weaknesses if weaknesses else ["‡∏Ñ‡∏ß‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ"],
            "recommendations": recommendations,
            "risk_level": risk_level
        }
    
    def predict_future_performance(self, current_grades, model=None):
        """‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏• AI ‡∏à‡∏£‡∏¥‡∏á"""
        current_gpa = self.calculate_gpa(current_grades)
        
        # ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á‡∏Å‡πà‡∏≠‡∏ô
        if self.advanced_model and self.course_profiles:
            try:
                # ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏•‡πâ‡∏ß
                features = self.advanced_trainer.create_dynamic_features(current_grades, self.course_profiles)
                
                # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á
                prediction_result = self.advanced_model.predict([features])[0]
                prediction_proba = self.advanced_model.predict_proba([features])[0]
                
                confidence = float(max(prediction_proba))
                
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÄ‡∏Å‡∏£‡∏î‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á
                predicted_grades = self._generate_grade_distribution_from_model(
                    prediction_result, confidence, current_gpa
                )
                
                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
                predicted_gpa = self._calculate_predicted_gpa(predicted_grades, current_gpa)
                
                # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°
                if predicted_gpa > current_gpa + 0.1:
                    trend = "improving"
                elif predicted_gpa < current_gpa - 0.1:
                    trend = "declining"
                else:
                    trend = "stable"
                
                return {
                    "predicted_grades": predicted_grades,
                    "predicted_gpa": round(predicted_gpa, 2),
                    "confidence": round(confidence, 3),
                    "trend": trend,
                    "ai_prediction": {
                        "result": prediction_result,
                        "confidence": confidence,
                        "model_used": True,
                        "model_type": "Advanced AI Model"
                    },
                    "analysis_note": f"‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ Advanced AI ‡∏à‡∏≤‡∏Å {len(current_grades)} ‡∏ß‡∏¥‡∏ä‡∏≤, GPA ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô {current_gpa:.2f}"
                }
                
            except Exception as e:
                logger.warning(f"Advanced model prediction failed: {e}")
        
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á ‡∏´‡∏£‡∏∑‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏õ‡∏Å‡∏ï‡∏¥
        if model and current_grades:
            try:
                # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏õ‡∏Å‡∏ï‡∏¥
                features = self._prepare_advanced_features(current_grades)
                
                # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏õ‡∏Å‡∏ï‡∏¥
                prediction_result = model.predict([features])[0]
                prediction_proba = model.predict_proba([features])[0]
                
                confidence = float(max(prediction_proba))
                
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÄ‡∏Å‡∏£‡∏î‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
                predicted_grades = self._generate_grade_distribution_from_model(
                    prediction_result, confidence, current_gpa
                )
                
                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
                predicted_gpa = self._calculate_predicted_gpa(predicted_grades, current_gpa)
                
                # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°
                if predicted_gpa > current_gpa + 0.1:
                    trend = "improving"
                elif predicted_gpa < current_gpa - 0.1:
                    trend = "declining"
                else:
                    trend = "stable"
                
                return {
                    "predicted_grades": predicted_grades,
                    "predicted_gpa": round(predicted_gpa, 2),
                    "confidence": round(confidence, 3),
                    "trend": trend,
                    "ai_prediction": {
                        "result": prediction_result,
                        "confidence": confidence,
                        "model_used": True,
                        "model_type": "Standard AI Model"
                    },
                    "analysis_note": f"‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ Standard AI ‡∏à‡∏≤‡∏Å {len(current_grades)} ‡∏ß‡∏¥‡∏ä‡∏≤, GPA ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô {current_gpa:.2f}"
                }
                
            except Exception as e:
                logger.warning(f"Standard AI model prediction failed: {e}")
        
        # ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏™‡∏≥‡∏£‡∏≠‡∏á‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
        return self._fallback_prediction(current_gpa, current_grades)
    
    def _prepare_advanced_features(self, grades):
        """‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• AI"""
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        gpa = self.calculate_gpa(grades)
        total_subjects = len(grades)
        grade_counts = Counter(grades.values())
        
        # ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        features = [
            gpa,  # GPA ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
            total_subjects,  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
            total_subjects * 3,  # ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏™‡∏∞‡∏™‡∏° (‡∏™‡∏°‡∏°‡∏ï‡∏¥ 3 ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï/‡∏ß‡∏¥‡∏ä‡∏≤)
        ]
        
        # ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÄ‡∏Å‡∏£‡∏î
        for grade in ['A', 'B+', 'B', 'C+', 'C', 'D+', 'D', 'F', 'W']:
            features.append(grade_counts.get(grade, 0))
        
        # ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏Å‡∏£‡∏î
        if total_subjects > 0:
            features.extend([
                grade_counts.get('A', 0) / total_subjects,  # ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡πÄ‡∏Å‡∏£‡∏î A
                grade_counts.get('F', 0) / total_subjects,  # ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡πÄ‡∏Å‡∏£‡∏î F
                (grade_counts.get('A', 0) + grade_counts.get('B+', 0)) / total_subjects,  # ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡πÄ‡∏Å‡∏£‡∏î‡∏î‡∏µ
                (grade_counts.get('D+', 0) + grade_counts.get('D', 0) + grade_counts.get('F', 0)) / total_subjects,  # ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡πÄ‡∏Å‡∏£‡∏î‡πÅ‡∏¢‡πà
            ])
        else:
            features.extend([0, 0, 0, 0])
        
        # ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å
        features.extend([
            1 if gpa >= 3.5 else 0,  # ‡πÄ‡∏Å‡∏£‡∏î‡∏î‡∏µ‡∏°‡∏≤‡∏Å
            1 if gpa < 2.0 else 0,   # ‡πÄ‡∏Å‡∏£‡∏î‡∏ï‡πà‡∏≥‡∏°‡∏≤‡∏Å
            1 if grade_counts.get('F', 0) > 0 else 0,  # ‡πÄ‡∏Ñ‡∏¢‡∏ï‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤
            min(4.0, gpa + 0.2) if gpa >= 3.0 else max(1.0, gpa - 0.1),  # ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏° GPA
        ])
        
        return features
    
    def _generate_grade_distribution_from_model(self, prediction_result, confidence, current_gpa):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÄ‡∏Å‡∏£‡∏î‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏•"""
        # ‡∏õ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÄ‡∏Å‡∏£‡∏î‡∏ï‡∏≤‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞ confidence
        if prediction_result == "‡∏à‡∏ö" and confidence > 0.7:
            # ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏à‡∏ö - ‡πÄ‡∏Å‡∏£‡∏î‡∏ô‡πà‡∏≤‡∏à‡∏∞‡∏î‡∏µ
            if current_gpa >= 3.5:
                return {"A": 0.4, "B+": 0.35, "B": 0.20, "C+": 0.05}
            elif current_gpa >= 3.0:
                return {"A": 0.25, "B+": 0.40, "B": 0.25, "C+": 0.10}
            else:
                return {"B+": 0.20, "B": 0.35, "C+": 0.30, "C": 0.15}
        
        elif prediction_result == "‡∏à‡∏ö" and confidence <= 0.7:
            # ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡πÉ‡∏à - ‡πÄ‡∏Å‡∏£‡∏î‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á
            return {"B": 0.25, "C+": 0.35, "C": 0.30, "D+": 0.10}
        
        else:
            # ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ñ‡∏¥‡∏î‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏à‡∏ö - ‡πÄ‡∏Å‡∏£‡∏î‡∏≠‡∏≤‡∏à‡πÅ‡∏¢‡πà
            return {"C": 0.20, "D+": 0.30, "D": 0.35, "F": 0.15}
    
    def _calculate_predicted_gpa(self, predicted_grades, current_gpa):
        """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÄ‡∏Å‡∏£‡∏î"""
        predicted_gpa = 0
        for grade, probability in predicted_grades.items():
            predicted_gpa += self.grade_mapping.get(grade, 0) * probability
        
        # ‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏™‡∏°‡πÄ‡∏´‡∏ï‡∏∏‡∏™‡∏°‡∏ú‡∏•‡∏Å‡∏±‡∏ö GPA ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
        if abs(predicted_gpa - current_gpa) > 1.0:
            predicted_gpa = current_gpa + (0.2 if predicted_gpa > current_gpa else -0.2)
        
        return max(0.0, min(4.0, predicted_gpa))
    
    def _fallback_prediction(self, current_gpa, current_grades):
        """‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏™‡∏≥‡∏£‡∏≠‡∏á‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô"""
        total_subjects = len(current_grades)
        
        # ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏ï‡∏≤‡∏° GPA
        if current_gpa >= 3.5:
            predicted_grades = {"A": 0.35, "B+": 0.30, "B": 0.25, "C+": 0.10}
            next_term_gpa = min(4.0, current_gpa + 0.05)
            confidence = 0.75
        elif current_gpa >= 3.0:
            predicted_grades = {"A": 0.20, "B+": 0.35, "B": 0.30, "C+": 0.15}
            next_term_gpa = current_gpa + 0.02
            confidence = 0.70
        elif current_gpa >= 2.5:
            predicted_grades = {"B": 0.20, "C+": 0.35, "C": 0.30, "D+": 0.15}
            next_term_gpa = current_gpa
            confidence = 0.65
        else:
            predicted_grades = {"C": 0.25, "D+": 0.30, "D": 0.30, "F": 0.15}
            next_term_gpa = max(1.5, current_gpa - 0.1)
            confidence = 0.60
        
        # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        if total_subjects < 5:
            confidence *= 0.8
        elif total_subjects > 15:
            confidence *= 1.1
        
        confidence = min(0.95, confidence)
        
        return {
            "predicted_grades": predicted_grades,
            "predicted_gpa": round(next_term_gpa, 2),
            "confidence": round(confidence, 3),
            "trend": "improving" if next_term_gpa > current_gpa else "stable" if next_term_gpa == current_gpa else "declining",
            "ai_prediction": {
                "result": "‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö",
                "confidence": confidence,
                "model_used": False
            },
            "analysis_note": f"‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏à‡∏≤‡∏Å {total_subjects} ‡∏ß‡∏¥‡∏ä‡∏≤, GPA ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô {current_gpa:.2f} (‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏°‡πÄ‡∏î‡∏• AI)"
        }
    
    def _prepare_model_features(self, grades):
        """‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• - ‡πÉ‡∏ä‡πâ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á"""
        return self._prepare_advanced_features(grades)
    
    def create_visualization_charts(self, grades_dict, prediction_data):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        
        # ‡∏Å‡∏£‡∏≤‡∏ü 1: ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÄ‡∏Å‡∏£‡∏î
        if grades_dict:
            grade_counts = Counter(grades_dict.values())
            grades = list(grade_counts.keys())
            counts = list(grade_counts.values())
            colors = ['#2E8B57', '#32CD32', '#FFD700', '#FFA500', '#FF6347', '#DC143C']
            
            wedges, texts, autotexts = ax1.pie(counts, labels=grades, autopct='%1.1f%%', 
                                             colors=colors[:len(grades)], startangle=90)
            ax1.set_title('‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÄ‡∏Å‡∏£‡∏î‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô', fontsize=14, fontweight='bold')
        
        # ‡∏Å‡∏£‡∏≤‡∏ü 2: ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö GPA
        current_gpa = self.calculate_gpa(grades_dict)
        predicted_gpa = prediction_data.get('predicted_gpa', current_gpa)
        
        categories = ['GPA ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô', 'GPA ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢', '‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥', '‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏î‡∏µ', '‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏î‡∏µ‡∏°‡∏≤‡∏Å']
        values = [current_gpa, predicted_gpa, 2.0, 2.75, 3.5]
        colors = ['blue', 'red', 'gray', 'orange', 'green']
        
        bars = ax2.bar(categories, values, color=colors)
        ax2.set_ylabel('GPA')
        ax2.set_title('‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö GPA', fontsize=14, fontweight='bold')
        ax2.set_ylim(0, 4)
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡πà‡∏≤‡∏ö‡∏ô‡πÅ‡∏ó‡πà‡∏á‡∏Å‡∏£‡∏≤‡∏ü
        for bar, value in zip(bars, values):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.05,
                    f'{value:.2f}', ha='center', va='bottom', fontweight='bold')
        
        plt.setp(ax2.get_xticklabels(), rotation=45, ha='right')
        
        # ‡∏Å‡∏£‡∏≤‡∏ü 3: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡∏£‡∏î‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï
        predicted_grades = prediction_data.get('predicted_grades', {})
        if predicted_grades:
            grades = list(predicted_grades.keys())
            probs = list(predicted_grades.values())
            colors = plt.cm.RdYlGn([p for p in probs])
            
            bars = ax3.bar(grades, probs, color=colors)
            ax3.set_xlabel('‡πÄ‡∏Å‡∏£‡∏î')
            ax3.set_ylabel('‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô')
            ax3.set_title('‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏ó‡∏≠‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ', fontsize=14, fontweight='bold')
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡πà‡∏≤‡∏ö‡∏ô‡πÅ‡∏ó‡πà‡∏á‡∏Å‡∏£‡∏≤‡∏ü
            for bar, prob in zip(bars, probs):
                height = bar.get_height()
                ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                        f'{prob:.2f}', ha='center', va='bottom', fontweight='bold')
        
        # ‡∏Å‡∏£‡∏≤‡∏ü 4: ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤
        terms = ['‡πÄ‡∏ó‡∏≠‡∏° 1', '‡πÄ‡∏ó‡∏≠‡∏° 2', '‡πÄ‡∏ó‡∏≠‡∏° 3', '‡πÄ‡∏ó‡∏≠‡∏°‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô', '‡πÄ‡∏ó‡∏≠‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ']
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°
        trend_gpas = [2.5, 2.8, 3.1, current_gpa, predicted_gpa]
        
        ax4.plot(terms[:-1], trend_gpas[:-1], 'o-', label='GPA ‡∏à‡∏£‡∏¥‡∏á', linewidth=3, markersize=8, color='blue')
        ax4.plot([terms[-2], terms[-1]], [trend_gpas[-2], trend_gpas[-1]], 'r--', 
                label='GPA ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢', linewidth=3, markersize=8)
        ax4.set_xlabel('‡πÄ‡∏ó‡∏≠‡∏°')
        ax4.set_ylabel('GPA')
        ax4.set_title('‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤ GPA', fontsize=14, fontweight='bold')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        ax4.set_ylim(0, 4)
        
        plt.tight_layout()
        
        # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô base64
        buffer = BytesIO()
        plt.savefig(buffer, format='png', dpi=300, bbox_inches='tight', facecolor='white')
        buffer.seek(0)
        image_base64 = base64.b64encode(buffer.getvalue()).decode()
        plt.close()
        
        return image_base64

# ‡∏™‡∏£‡πâ‡∏≤‡∏á instance ‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö
enhanced_system = EnhancedPredictionSystem()

# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
enhanced_system.load_advanced_model()

warnings.filterwarnings('ignore')

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('app_startup.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ==========================================
# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Environment Variables ‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
# ==========================================
logger.info("=" * 70)
logger.info("üîß CLOUDFLARE R2 CONFIGURATION CHECK")
logger.info("=" * 70)

def check_env_variables():
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ environment variables ‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î"""
    env_status = {
        'all_present': True,
        'missing': [],
        'details': {}
    }
    
    required_vars = {
        'CLOUDFLARE_R2_ACCESS_KEY_ID': 'Access Key for R2 authentication',
        'CLOUDFLARE_R2_SECRET_ACCESS_KEY': 'Secret Key for R2 authentication',
        'CLOUDFLARE_R2_ENDPOINT': 'R2 endpoint URL',
        'CLOUDFLARE_R2_BUCKET_NAME': 'R2 bucket name'
    }
    
    for var_name, description in required_vars.items():
        value = os.environ.get(var_name, '')
        
        if value:
            # ‡πÅ‡∏™‡∏î‡∏á‡πÅ‡∏Ñ‡πà‡∏™‡πà‡∏ß‡∏ô‡πÅ‡∏£‡∏Å‡∏Ç‡∏≠‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
            if 'SECRET' in var_name or 'KEY' in var_name:
                display_value = f"{value[:8]}...{value[-4:]}" if len(value) > 12 else "***"
            else:
                display_value = value
            
            logger.info(f"‚úÖ {var_name}: {display_value}")
            env_status['details'][var_name] = {
                'present': True,
                'length': len(value),
                'description': description
            }
        else:
            logger.error(f"‚ùå {var_name}: NOT FOUND - {description}")
            env_status['missing'].append(var_name)
            env_status['all_present'] = False
            env_status['details'][var_name] = {
                'present': False,
                'description': description
            }
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå .env
    if os.path.exists('.env'):
        logger.info(f"üìÑ .env file: EXISTS (size: {os.path.getsize('.env')} bytes)")
        
        # ‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ .env
        try:
            with open('.env', 'r') as f:
                env_lines = f.readlines()
                env_vars_in_file = {}
                for line in env_lines:
                    if '=' in line and not line.strip().startswith('#'):
                        key = line.split('=')[0].strip()
                        env_vars_in_file[key] = True
            
            logger.info(f"üìã Variables in .env file: {list(env_vars_in_file.keys())}")
            
            # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÉ‡∏ô .env ‡∏ñ‡∏π‡∏Å‡πÇ‡∏´‡∏•‡∏î‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            for var in required_vars.keys():
                if var in env_vars_in_file and not os.environ.get(var):
                    logger.warning(f"‚ö†Ô∏è {var} exists in .env but not loaded into environment!")
                    
        except Exception as e:
            logger.error(f"‚ùå Error reading .env file: {str(e)}")
    else:
        logger.warning("‚ö†Ô∏è .env file: NOT FOUND")
    
    return env_status

env_check_result = check_env_variables()

if not env_check_result['all_present']:
    logger.error("=" * 70)
    logger.error("‚ö†Ô∏è CLOUDFLARE R2 CONFIGURATION INCOMPLETE!")
    logger.error("Missing variables: " + ", ".join(env_check_result['missing']))
    logger.error("R2 storage will be DISABLED. Using LOCAL storage instead.")
    logger.error("=" * 70)

# Create Flask app
ACTIVE_CONFIG = config.get_config()
app = Flask(__name__)
app.config.from_object(ACTIVE_CONFIG)
app.secret_key = os.environ.get('FLASK_SECRET_KEY', secrets.token_hex(32))


class MongoAdminManager:
    """MongoDB-backed admin/auth/settings manager.
    NOTE: MONGODB_URI remains in .env only.
    """

    SETTING_KEYS = [
        'GEMINI_API_KEY',
        'GEMINI_MODEL_NAME',
        'GEMINI_MODEL_FALLBACKS',
        'CLOUDFLARE_R2_ACCESS_KEY_ID',
        'CLOUDFLARE_R2_SECRET_ACCESS_KEY',
        'CLOUDFLARE_R2_ENDPOINT',
        'CLOUDFLARE_R2_BUCKET_NAME',
    ]

    def __init__(self):
        self.enabled = False
        self.client = None
        self.db = None
        self.users = None
        self.settings = None
        self.audit = None
        self.bootstrap_password = None

        self._mongo_uri = os.environ.get('MONGODB_URI', '').strip()
        self._db_name = os.environ.get('MONGODB_DB_NAME', 'student_ai_system').strip()
        self._enc_key = os.environ.get('APP_MASTER_ENCRYPTION_KEY', '').strip()

        if not self._mongo_uri:
            logger.warning("‚ö†Ô∏è MONGODB_URI not configured. Admin backend is disabled.")
            return
        if MongoClient is None:
            logger.warning("‚ö†Ô∏è pymongo package not installed. Admin backend is disabled.")
            return

        try:
            self.client = MongoClient(self._mongo_uri, serverSelectionTimeoutMS=5000)
            self.client.admin.command('ping')
            self.db = self.client[self._db_name]
            self.users = self.db['users']
            self.settings = self.db['system_settings']
            self.audit = self.db['audit_logs']

            self.users.create_index('username', unique=True)
            self.settings.create_index('key', unique=True)
            self.audit.create_index('created_at')

            self.enabled = True
            self._bootstrap_default_admin()
            logger.info(f"‚úÖ Mongo admin backend ready (db={self._db_name})")
        except Exception as e:
            logger.error(f"‚ùå Mongo admin backend init failed: {e}")
            self.enabled = False

    def _log_audit(self, actor_user_id, action, target, status='success', details=None):
        if not self.enabled:
            return
        try:
            self.audit.insert_one({
                'actor_user_id': actor_user_id,
                'action': action,
                'target': target,
                'status': status,
                'details': details or {},
                'ip_address': request.remote_addr if request else None,
                'user_agent': request.headers.get('User-Agent') if request else None,
                'created_at': datetime.utcnow().isoformat()
            })
        except Exception:
            pass

    def _derive_key(self):
        return hashlib.sha256(self._enc_key.encode('utf-8')).digest()

    def _encrypt(self, raw_value: str) -> str:
        if raw_value is None:
            return ''
        key = self._derive_key()
        raw = raw_value.encode('utf-8')
        out = bytearray(len(raw))
        for i, b in enumerate(raw):
            out[i] = b ^ key[i % len(key)]
        return base64.urlsafe_b64encode(bytes(out)).decode('utf-8')

    def _decrypt(self, encrypted_value: str) -> str:
        if not encrypted_value:
            return ''
        key = self._derive_key()
        raw = base64.urlsafe_b64decode(encrypted_value.encode('utf-8'))
        out = bytearray(len(raw))
        for i, b in enumerate(raw):
            out[i] = b ^ key[i % len(key)]
        return bytes(out).decode('utf-8')

    def _bootstrap_default_admin(self):
        if not self.enabled:
            return
        if self.users.count_documents({}) > 0:
            return

        username = os.environ.get('INITIAL_ADMIN_USERNAME', 'admin')
        temp_password = os.environ.get('INITIAL_ADMIN_PASSWORD', 'admin123')
        self.bootstrap_password = temp_password

        self.users.insert_one({
            'username': username,
            'email': None,
            'password_hash': generate_password_hash(temp_password),
            'role': 'super_admin',
            'must_change_password': True,
            'is_active': True,
            'created_at': datetime.utcnow().isoformat(),
            'updated_at': datetime.utcnow().isoformat(),
            'last_login_at': None
        })

        logger.warning("=" * 70)
        logger.warning("üîê FIRST RUN: Default super_admin account created")
        logger.warning(f"   username: {username}")
        logger.warning(f"   temporary password: {temp_password}")
        logger.warning("   Action required: login and change password immediately")
        logger.warning("=" * 70)

    def apply_runtime_env(self):
        """Load settings from MongoDB and apply to process env (except MONGODB_URI)."""
        if not self.enabled:
            return
        try:
            docs = list(self.settings.find({'key': {'$in': self.SETTING_KEYS}}))
            for doc in docs:
                key = doc.get('key')
                # Plaintext first (new behavior), encrypted fallback (legacy data)
                plain_value = doc.get('value')
                value_encrypted = doc.get('value_encrypted', '')

                if key and plain_value is not None:
                    os.environ[key] = str(plain_value)
                elif key and value_encrypted:
                    os.environ[key] = self._decrypt(value_encrypted)
        except Exception as e:
            logger.warning(f"Could not apply runtime env from MongoDB: {e}")

    def authenticate(self, username: str, password: str):
        if not self.enabled:
            return None
        user = self.users.find_one({'username': username, 'is_active': True})
        if not user:
            return None
        if not check_password_hash(user.get('password_hash', ''), password):
            return None
        self.users.update_one(
            {'_id': user['_id']},
            {'$set': {'last_login_at': datetime.utcnow().isoformat()}}
        )
        return user

    def change_password(self, user_id, new_password: str):
        if not self.enabled:
            return False
        self.users.update_one(
            {'_id': user_id},
            {'$set': {
                'password_hash': generate_password_hash(new_password),
                'must_change_password': False,
                'updated_at': datetime.utcnow().isoformat()
            }}
        )
        return True

    def set_setting(self, key: str, value: str, actor_user_id=None):
        if not self.enabled:
            return False
        self.settings.update_one(
            {'key': key},
            {'$set': {
                'key': key,
                # Save as plaintext by requirement
                'value': value or '',
                # keep legacy field for compatibility (clear when writing new value)
                'value_encrypted': '',
                'updated_by': str(actor_user_id) if actor_user_id else None,
                'updated_at': datetime.utcnow().isoformat()
            }},
            upsert=True
        )
        self._log_audit(actor_user_id, 'UPDATE_SETTING', key)
        return True

    def get_setting(self, key: str, default: str = ''):
        if not self.enabled:
            return default
        doc = self.settings.find_one({'key': key})
        if not doc:
            return default
        try:
            # Plaintext first (new behavior)
            if 'value' in doc and doc.get('value') is not None:
                return str(doc.get('value'))
            # Encrypted fallback (legacy data)
            encrypted = doc.get('value_encrypted', '')
            if encrypted:
                return self._decrypt(encrypted)
            return default
        except Exception:
            return default

    @staticmethod
    def mask_value(raw: str):
        if not raw:
            return ''
        if len(raw) <= 6:
            return '***'
        return f"{raw[:4]}***{raw[-2:]}"


admin_manager = MongoAdminManager()
admin_manager.apply_runtime_env()

COURSE_LOOKUP = {course['id']: course for course in getattr(ACTIVE_CONFIG, 'COURSES_DATA', [])}
GRADE_POINT_MAP = ACTIVE_CONFIG.DATA_CONFIG.get('grade_mapping', {})

GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY')
# Use gemini-3-flash-preview as default (fastest Gemini 3 model with free tier)
# Fallback chain: gemini-2.5-flash (stable) ‚Üí gemini-2.5-pro (advanced) ‚Üí gemini-2.0-flash (legacy)
# Reference: https://ai.google.dev/gemini-api/docs/models
GEMINI_MODEL_NAME = os.environ.get('GEMINI_MODEL_NAME', 'gemini-3-flash-preview')
GEMINI_MAX_FILE_SIZE_MB = float(os.environ.get('GEMINI_MAX_FILE_SIZE_MB', 5))
# Default fallback models for high availability (in order of preference)
GEMINI_DEFAULT_FALLBACKS = ['gemini-3-flash-preview', 'gemini-2.5-flash', 'gemini-2.5-pro', 'gemini-2.0-flash']


def _build_gemini_model_candidates(primary_name: str) -> List[str]:
    """Collect preferred + fallback Gemini model names with preserved order."""
    candidates: List[str] = []

    def _add(name: Optional[str]):
        if name and name not in candidates:
            candidates.append(name)

    _add(primary_name)

    fallback_env = os.environ.get('GEMINI_MODEL_FALLBACKS', '')
    if fallback_env:
        for item in fallback_env.split(','):
            _add(item.strip())

    # Add default fallback models from centralized constant
    for default_name in GEMINI_DEFAULT_FALLBACKS:
        _add(default_name)

    return candidates


GEMINI_MODEL_CANDIDATES = _build_gemini_model_candidates(GEMINI_MODEL_NAME)
gemini_client_ready = False

if genai and GEMINI_API_KEY:
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        gemini_client_ready = True
        logger.info(f"‚úÖ Gemini API initialized with model {GEMINI_MODEL_NAME}")
    except Exception as gemini_init_error:
        gemini_client_ready = False
        logger.error(f"‚ùå Failed to initialize Gemini API: {gemini_init_error}")
elif not GEMINI_API_KEY:
    logger.info("‚ÑπÔ∏è GEMINI_API_KEY not found. Gemini routes will be disabled.")
else:
    logger.warning("‚ö†Ô∏è google-generativeai package not available. Install to enable Gemini features.")


def refresh_gemini_runtime_from_settings():
    """Reload Gemini runtime variables from Mongo settings (if enabled)."""
    global GEMINI_API_KEY, GEMINI_MODEL_NAME, GEMINI_MODEL_CANDIDATES, gemini_client_ready

    if admin_manager.enabled:
        admin_manager.apply_runtime_env()

    GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY')
    GEMINI_MODEL_NAME = os.environ.get('GEMINI_MODEL_NAME', 'gemini-3-flash-preview')
    GEMINI_MODEL_CANDIDATES = _build_gemini_model_candidates(GEMINI_MODEL_NAME)

    gemini_client_ready = False
    if genai and GEMINI_API_KEY:
        try:
            genai.configure(api_key=GEMINI_API_KEY)
            gemini_client_ready = True
            logger.info(f"‚úÖ Gemini runtime refreshed with model {GEMINI_MODEL_NAME}")
        except Exception as gemini_init_error:
            gemini_client_ready = False
            logger.error(f"‚ùå Failed to refresh Gemini runtime: {gemini_init_error}")


def _json_error(message: str, status: int = 403):
    return jsonify({'success': False, 'error': message}), status


def current_user_role() -> str:
    return session.get('user_role', 'anonymous')


def has_any_role(*roles) -> bool:
    if not admin_manager.enabled:
        return True
    return current_user_role() in roles


def is_logged_in() -> bool:
    if not admin_manager.enabled:
        return True
    return bool(session.get('user_id'))


def admin_login_required(view_func):
    @wraps(view_func)
    def wrapper(*args, **kwargs):
        if not admin_manager.enabled:
            return _json_error('Admin backend is disabled (MongoDB not configured)', 503)
        if not is_logged_in():
            if request.path.startswith('/api/'):
                return _json_error('Unauthorized', 401)
            return redirect(url_for('admin_login', next=request.path))
        return view_func(*args, **kwargs)
    return wrapper


def roles_required(*roles):
    def decorator(view_func):
        @wraps(view_func)
        def wrapper(*args, **kwargs):
            if not admin_manager.enabled:
                return view_func(*args, **kwargs)
            if not is_logged_in():
                return _json_error('Unauthorized', 401)
            if not has_any_role(*roles):
                return _json_error('Forbidden', 403)
            return view_func(*args, **kwargs)
        return wrapper
    return decorator


@app.before_request
def enforce_first_login_password_change():
    if not admin_manager.enabled:
        return None

    endpoint = request.endpoint or ''
    allow_endpoints = {
        'admin_login',
        'admin_logout',
        'admin_change_password',
        'static'
    }

    if not session.get('user_id'):
        return None

    if session.get('must_change_password') and endpoint not in allow_endpoints:
        if request.path.startswith('/api/'):
            return _json_error('Password change required before using this endpoint', 403)
        return redirect(url_for('admin_change_password'))

    return None


@app.before_request
def enforce_login_for_non_prediction_pages():
    """Allow anonymous access only to prediction page and required prediction APIs."""
    if not admin_manager.enabled:
        return None

    endpoint = request.endpoint or ''

    public_endpoints = {
        'static',
        'admin_login',
        'admin_logout',
        'admin_change_password',

        # Public prediction pages
        'curriculum_prediction_form',
        'curriculum_page',

        # Public prediction APIs
        'get_config_for_frontend',
        'analyze_curriculum',
        'gemini_predict_route',
        'explain_prediction',
        'get_three_line_chart_data',
        'get_graduation_analysis',
        'get_next_term_prediction',
        'get_comprehensive_analysis',
        'predict_manual_input',
    }

    if endpoint in public_endpoints:
        return None

    # Force landing page to prediction page for anonymous users
    if endpoint == 'index' and not session.get('user_id'):
        return redirect(url_for('curriculum_prediction_form'))

    if not session.get('user_id'):
        if request.path.startswith('/api/'):
            return _json_error('Unauthorized', 401)
        return redirect(url_for('admin_login', next=request.path))

    return None

# ==========================================
# RETRY MECHANISM ‡πÅ‡∏•‡∏∞ RATE LIMITER ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GEMINI API
# ==========================================

def retry_on_quota_error(max_retries=3, initial_delay=20):
    """Decorator ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö retry ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏à‡∏≠ quota error
    
    Args:
        max_retries: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏≠‡∏á (‡∏£‡∏ß‡∏°‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å)
        initial_delay: ‡πÄ‡∏ß‡∏•‡∏≤‡∏£‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô (‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            delay = initial_delay
            
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    error_msg = str(e)
                    is_quota_error = ('429' in error_msg or 
                                     'quota' in error_msg.lower() or 
                                     'resource exhausted' in error_msg.lower())
                    
                    if is_quota_error and attempt < max_retries - 1:
                        logger.warning(f"Quota exceeded, retrying in {delay}s (attempt {attempt+1}/{max_retries})")
                        time.sleep(delay)
                        delay *= 2  # Exponential backoff
                        continue
                    
                    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà quota error ‡∏´‡∏£‡∏∑‡∏≠‡∏´‡∏°‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô retry ‡πÅ‡∏•‡πâ‡∏ß ‡πÉ‡∏´‡πâ raise
                    raise
                
        return wrapper
    return decorator


class RateLimiter:
    """Rate limiter ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Gemini API"""
    def __init__(self, max_requests=10, time_window=60):
        self.max_requests = max_requests
        self.time_window = timedelta(seconds=time_window)
        self.requests = deque()
    
    def can_proceed(self):
        now = datetime.now()
        # ‡∏•‡∏ö request ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πà‡∏≤‡πÄ‡∏Å‡∏¥‡∏ô‡∏Å‡∏ß‡πà‡∏≤ time window
        while self.requests and now - self.requests[0] > self.time_window:
            self.requests.popleft()
        
        if len(self.requests) < self.max_requests:
            self.requests.append(now)
            return True, None
        else:
            wait_time = (self.requests[0] + self.time_window - now).total_seconds()
            # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô negative wait time ‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
            return False, max(1, int(wait_time))


# ‡∏™‡∏£‡πâ‡∏≤‡∏á rate limiter instance
gemini_rate_limiter = RateLimiter(max_requests=10, time_window=60)

# ==========================================
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
# ==========================================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
UPLOAD_FOLDER = os.path.join(BASE_DIR, 'uploads')
MODEL_FOLDER = os.path.join(BASE_DIR, 'models')

app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MODEL_FOLDER'] = MODEL_FOLDER

logger.info("=" * 70)
logger.info("üìÅ DIRECTORY SETUP")
logger.info("=" * 70)

for folder_name, folder_path in [('uploads', UPLOAD_FOLDER), ('models', MODEL_FOLDER)]:
    try:
        if not os.path.exists(folder_path):
            os.makedirs(folder_path, exist_ok=True)
            logger.info(f"‚úÖ Created {folder_name} folder: {folder_path}")
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå .gitkeep ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ git track ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ß‡πà‡∏≤‡∏á
            gitkeep_path = os.path.join(folder_path, '.gitkeep')
            if not os.path.exists(gitkeep_path):
                with open(gitkeep_path, 'w') as f:
                    f.write('')
                logger.info(f"   Created .gitkeep in {folder_name}")
        else:
            logger.info(f"‚úÖ {folder_name} folder exists: {folder_path}")
            
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö permissions
        if os.access(folder_path, os.W_OK):
            logger.info(f"   ‚úÖ Write permission: OK")
        else:
            logger.error(f"   ‚ùå Write permission: DENIED")
            
        # ‡πÅ‡∏™‡∏î‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà
        files = os.listdir(folder_path)
        if files:
            logger.info(f"   Files in {folder_name}: {len(files)} file(s)")
            for file in files[:5]:  # ‡πÅ‡∏™‡∏î‡∏á‡πÅ‡∏Ñ‡πà 5 ‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏£‡∏Å
                file_path = os.path.join(folder_path, file)
                file_size = os.path.getsize(file_path) if os.path.isfile(file_path) else 0
                logger.info(f"     - {file} ({file_size} bytes)")
                
    except Exception as e:
        logger.error(f"‚ùå Error with {folder_name} folder: {str(e)}")
        logger.error(f"   Current working directory: {os.getcwd()}")
        logger.error(f"   Python executable: {sys.executable}")

# Variables to store loaded models
models = {
    'subject_model': None,
    'gpa_model': None,
    'subject_model_info': None,
    'gpa_model_info': None,
    'subject_feature_cols': None,
    'gpa_feature_cols': None
}

# ==========================================
# Enhanced S3 Storage Class with Detailed Error Reporting
# ==========================================
class S3Storage:
    def __init__(self):
        """Initialize S3 client with detailed error reporting"""
        self.connection_errors = []
        self.use_local = False
        self.s3_client = None
        
        logger.info("=" * 70)
        logger.info("üöÄ INITIALIZING CLOUDFLARE R2 STORAGE")
        logger.info("=" * 70)
        
        try:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö credentials
            self.access_key = os.environ.get('CLOUDFLARE_R2_ACCESS_KEY_ID', '')
            self.secret_key = os.environ.get('CLOUDFLARE_R2_SECRET_ACCESS_KEY', '')
            self.endpoint_url = os.environ.get('CLOUDFLARE_R2_ENDPOINT', '')
            self.bucket_name = os.environ.get('CLOUDFLARE_R2_BUCKET_NAME', 'pjai')
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ credential
            missing_creds = []
            if not self.access_key:
                missing_creds.append('CLOUDFLARE_R2_ACCESS_KEY_ID')
            if not self.secret_key:
                missing_creds.append('CLOUDFLARE_R2_SECRET_ACCESS_KEY')
            if not self.endpoint_url:
                missing_creds.append('CLOUDFLARE_R2_ENDPOINT')
                
            if missing_creds:
                error_msg = f"Missing R2 credentials: {', '.join(missing_creds)}"
                logger.error(f"‚ùå {error_msg}")
                self.connection_errors.append(error_msg)
                self.use_local = True
                logger.info("üìÇ Fallback: Using LOCAL storage")
                return
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö boto3
            try:
                import boto3
                from botocore.config import Config
                from botocore.exceptions import NoCredentialsError, ClientError
            except ImportError as e:
                error_msg = "boto3 library not installed. Run: pip install boto3"
                logger.error(f"‚ùå {error_msg}")
                self.connection_errors.append(error_msg)
                self.use_local = True
                logger.info("üìÇ Fallback: Using LOCAL storage")
                return
            
            # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á S3 client
            logger.info("üîó Attempting to connect to Cloudflare R2...")
            logger.info(f"   Endpoint: {self.endpoint_url}")
            logger.info(f"   Bucket: {self.bucket_name}")
            
            try:
                config = Config(
                    region_name='auto',
                    signature_version='s3v4',
                    retries={'max_attempts': 3, 'mode': 'standard'},
                    connect_timeout=10,
                    read_timeout=10
                )
                
                self.s3_client = boto3.client(
                    's3',
                    endpoint_url=self.endpoint_url,
                    aws_access_key_id=self.access_key,
                    aws_secret_access_key=self.secret_key,
                    config=config
                )
                
                # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠
                logger.info("üß™ Testing R2 connection...")
                test_response = self.s3_client.list_objects_v2(
                    Bucket=self.bucket_name,
                    MaxKeys=1
                )
                
                logger.info("‚úÖ R2 CONNECTION SUCCESSFUL!")
                logger.info(f"   Objects in bucket: {len(test_response.get('Contents', []))}")
                self.use_local = False
                
            except ClientError as e:
                error_code = e.response.get('Error', {}).get('Code', 'Unknown')
                error_message = e.response.get('Error', {}).get('Message', str(e))
                
                if error_code == 'NoSuchBucket':
                    error_msg = f"Bucket '{self.bucket_name}' does not exist"
                    logger.error(f"‚ùå {error_msg}")
                    logger.info("üî® Attempting to create bucket...")
                    
                    try:
                        self.s3_client.create_bucket(Bucket=self.bucket_name)
                        logger.info(f"‚úÖ Bucket '{self.bucket_name}' created successfully!")
                        self.use_local = False
                    except Exception as create_error:
                        error_msg = f"Failed to create bucket: {str(create_error)}"
                        logger.error(f"‚ùå {error_msg}")
                        self.connection_errors.append(error_msg)
                        self.use_local = True
                        
                elif error_code == 'InvalidAccessKeyId':
                    error_msg = "Invalid Access Key ID - Check your CLOUDFLARE_R2_ACCESS_KEY_ID"
                    logger.error(f"‚ùå {error_msg}")
                    self.connection_errors.append(error_msg)
                    self.use_local = True
                    
                elif error_code == 'SignatureDoesNotMatch':
                    error_msg = "Invalid Secret Key - Check your CLOUDFLARE_R2_SECRET_ACCESS_KEY"
                    logger.error(f"‚ùå {error_msg}")
                    self.connection_errors.append(error_msg)
                    self.use_local = True
                    
                else:
                    error_msg = f"R2 Error [{error_code}]: {error_message}"
                    logger.error(f"‚ùå {error_msg}")
                    self.connection_errors.append(error_msg)
                    self.use_local = True
                    
            except Exception as e:
                error_msg = f"Connection failed: {str(e)}"
                logger.error(f"‚ùå {error_msg}")
                self.connection_errors.append(error_msg)
                self.use_local = True
            
        except Exception as e:
            error_msg = f"Initialization error: {str(e)}"
            logger.error(f"‚ùå {error_msg}")
            self.connection_errors.append(error_msg)
            self.use_local = True
        
        # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•
        logger.info("=" * 70)
        if self.use_local:
            logger.warning("‚ö†Ô∏è R2 STORAGE: DISABLED")
            logger.info("üìÇ Using LOCAL FILE STORAGE")
            if self.connection_errors:
                logger.error("Connection errors:")
                for error in self.connection_errors:
                    logger.error(f"  ‚Ä¢ {error}")
        else:
            logger.info("‚úÖ R2 STORAGE: ENABLED AND READY")
        logger.info("=" * 70)
    
    def get_connection_status(self):
        """Get detailed connection status for API"""
        return {
            'connected': not self.use_local,
            'storage_type': 'local' if self.use_local else 'cloudflare_r2',
            'bucket_name': self.bucket_name if not self.use_local else None,
            'endpoint': self.endpoint_url if not self.use_local else None,
            'errors': self.connection_errors
        }
    
    def save_model(self, model_data: Dict[str, Any], filename: str) -> bool:
        """Save model with detailed error reporting"""
        if self.use_local:
            return self._save_model_locally(model_data, filename)
        
        tmp_path = None
        try:
            with tempfile.NamedTemporaryFile(suffix='.joblib', delete=False) as tmp_file:
                joblib.dump(model_data, tmp_file.name)
                tmp_path = tmp_file.name
            
            s3_key = f"models/{filename}"
            with open(tmp_path, 'rb') as f:
                self.s3_client.put_object(
                    Bucket=self.bucket_name,
                    Key=s3_key,
                    Body=f,
                    ContentType='application/octet-stream',
                    Metadata={
                        'created_at': datetime.now().isoformat(),
                        'data_format': model_data.get('data_format', 'unknown'),
                        'accuracy': str(model_data.get('performance_metrics', {}).get('accuracy', 0))
                    }
                )
            
            os.remove(tmp_path)
            logger.info(f"‚úÖ Model {filename} saved to R2")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå R2 save error: {str(e)}")
            if tmp_path and os.path.exists(tmp_path):
                os.remove(tmp_path)
            # Fallback to local
            logger.info("üìÇ Trying local save as fallback...")
            return self._save_model_locally(model_data, filename)
    
    def _save_model_locally(self, model_data: Dict[str, Any], filename: str) -> bool:
        """Save model locally"""
        try:
            model_folder = app.config['MODEL_FOLDER']
            if not os.path.exists(model_folder):
                os.makedirs(model_folder)
            
            filepath = os.path.join(model_folder, filename)
            joblib.dump(model_data, filepath)
            logger.info(f"üíæ Model {filename} saved locally")
            return True
        except Exception as e:
            logger.error(f"‚ùå Local save error: {str(e)}")
            return False
    
    def load_model(self, filename: str) -> Optional[Dict[str, Any]]:
        """Load model"""
        if self.use_local:
            return self._load_model_locally(filename)
        
        tmp_path = None
        try:
            s3_key = f"models/{filename}"
            with tempfile.NamedTemporaryFile(suffix='.joblib', delete=False) as tmp_file:
                self.s3_client.download_file(
                    Bucket=self.bucket_name,
                    Key=s3_key,
                    Filename=tmp_file.name
                )
                tmp_path = tmp_file.name
            
            model_data = joblib.load(tmp_path)
            os.remove(tmp_path)
            logger.info(f"‚úÖ Model {filename} loaded from R2")
            return model_data
            
        except Exception as e:
            logger.warning(f"R2 load failed: {str(e)}, trying local...")
            if tmp_path and os.path.exists(tmp_path):
                os.remove(tmp_path)
            # ‡∏´‡∏≤‡∏Å‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å R2 ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å local ‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡∏°‡πà
            local_result = self._load_model_locally(filename)
            if local_result is None:
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏£‡∏≠‡∏á
                return self._create_fallback_model_data()
            return local_result
    
    def _create_fallback_model_data(self) -> Dict[str, Any]:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏£‡∏≠‡∏á‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ"""
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.preprocessing import StandardScaler
        
        model = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=42
        )
        scaler = StandardScaler()
        
        logger.info("‚úÖ Created fallback model data")
        return {
            'model': model,
            'scaler': scaler,
            'course_credit_map': {},
            'grade_mapping': {
                'A': 4.0, 'B+': 3.5, 'B': 3.0, 'C+': 2.5, 'C': 2.0,
                'D+': 1.5, 'D': 1.0, 'F': 0.0, 'W': 0.0, 'WF': 0.0, 'WU': 0.0, 'S': None
            }
        }
    
    def _load_model_locally(self, filename: str) -> Optional[Dict[str, Any]]:
        """Load model from local storage"""
        try:
            filepath = os.path.join(app.config['MODEL_FOLDER'], filename)
            if os.path.exists(filepath):
                model_data = joblib.load(filepath)
                logger.info(f"üìÇ Model {filename} loaded from local")
                return model_data
            return None
        except Exception as e:
            logger.error(f"‚ùå Local load error: {str(e)}")
            # ‡∏´‡∏≤‡∏Å‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å local ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏£‡∏≠‡∏á
            return self._create_fallback_model_data()
    
    def list_models(self) -> List[Dict[str, Any]]:
        """List all models"""
        models = []
        
        # Try R2 first
        if not self.use_local and self.s3_client:
            try:
                response = self.s3_client.list_objects_v2(
                    Bucket=self.bucket_name,
                    Prefix='models/',
                    Delimiter='/'
                )
                
                if 'Contents' in response:
                    for obj in response['Contents']:
                        filename = obj['Key'].replace('models/', '')
                        if filename and filename.endswith('.joblib'):
                            models.append({
                                'filename': filename,
                                'created_at': obj['LastModified'].isoformat(),
                                'size': obj['Size'],
                                'storage': 'r2',
                                'performance_metrics': {},
                                'data_format': 'unknown'
                            })
                logger.info(f"üìä Found {len(models)} models in R2")
            except Exception as e:
                logger.warning(f"R2 list failed: {str(e)}")
        
        # Also check local
        try:
            model_folder = app.config['MODEL_FOLDER']
            if os.path.exists(model_folder):
                for filename in os.listdir(model_folder):
                    if filename.endswith('.joblib'):
                        filepath = os.path.join(model_folder, filename)
                        try:
                            model_data = joblib.load(filepath)
                            models.append({
                                'filename': filename,
                                'created_at': model_data.get('created_at', ''),
                                'data_format': model_data.get('data_format', 'unknown'),
                                'performance_metrics': model_data.get('performance_metrics', {}),
                                'storage': 'local'
                            })
                        except:
                            models.append({
                                'filename': filename,
                                'created_at': datetime.fromtimestamp(os.path.getctime(filepath)).isoformat(),
                                'storage': 'local',
                                'performance_metrics': {},
                                'data_format': 'unknown'
                            })
                logger.info(f"üìÇ Found {len([m for m in models if m['storage'] == 'local'])} models locally")
        except Exception as e:
            logger.error(f"Local list error: {str(e)}")
        
        return sorted(models, key=lambda x: x.get('created_at', ''), reverse=True)
    
    def delete_model(self, filename: str) -> bool:
        """Delete model"""
        deleted = False
        
        # Try R2
        if not self.use_local and self.s3_client:
            try:
                s3_key = f"models/{filename}"
                self.s3_client.delete_object(
                    Bucket=self.bucket_name,
                    Key=s3_key
                )
                logger.info(f"‚úÖ Model {filename} deleted from R2")
                deleted = True
            except Exception as e:
                logger.warning(f"R2 delete failed: {str(e)}")
        
        # Also try local
        try:
            filepath = os.path.join(app.config['MODEL_FOLDER'], filename)
            if os.path.exists(filepath):
                os.remove(filepath)
                logger.info(f"üóëÔ∏è Model {filename} deleted from local")
                deleted = True
        except Exception as e:
            logger.warning(f"Local delete failed: {str(e)}")
        
        return deleted

# Global Training Status
TRAINING_STATUS = {
    'status': 'idle',
    'message': '',
    'progress': 0,
    'result': None,
    'error': None
}

# Create global storage instance
storage = S3Storage()

# ==========================================
# Sync Models from R2 at Startup
# ==========================================
def sync_models_from_r2():
    """Download models from R2 to local storage for faster access"""
    if storage.use_local:
        logger.info("üìÇ R2 not connected, skipping model sync")
        return 0
    
    try:
        logger.info("üîÑ Syncing models from R2 to local storage...")
        model_folder = app.config['MODEL_FOLDER']
        
        if not os.path.exists(model_folder):
            os.makedirs(model_folder, exist_ok=True)
        
        # List R2 models
        r2_models = []
        response = storage.s3_client.list_objects_v2(
            Bucket=storage.bucket_name,
            Prefix='models/'
        )
        
        if 'Contents' in response:
            r2_models = [obj['Key'].replace('models/', '') 
                        for obj in response['Contents'] 
                        if obj['Key'].endswith('.joblib')]
        
        synced_count = 0
        for model_filename in r2_models:
            local_path = os.path.join(model_folder, model_filename)
            
            # Skip if already exists locally
            if os.path.exists(local_path):
                continue
            
            try:
                s3_key = f"models/{model_filename}"
                storage.s3_client.download_file(
                    Bucket=storage.bucket_name,
                    Key=s3_key,
                    Filename=local_path
                )
                synced_count += 1
                logger.info(f"  ‚úÖ Downloaded {model_filename}")
            except Exception as e:
                logger.warning(f"  ‚ö†Ô∏è Failed to download {model_filename}: {e}")
        
        logger.info(f"‚úÖ Synced {synced_count} models from R2")
        return synced_count
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Model sync failed: {e}")
        return 0

# Run model sync at startup
try:
    sync_models_from_r2()
except Exception as e:
    logger.warning(f"Model sync at startup failed: {e}")

# ===============================
# TRAINING STATUS ROUTE
# ===============================
@app.route('/training_status')
def get_training_status():
    return jsonify(TRAINING_STATUS)


# ===============================
# ENHANCED API ROUTES
# ===============================

@app.route('/api/enhanced_analysis', methods=['POST'])
def enhanced_analysis():
    """API ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏ö‡∏ö‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á"""
    try:
        data = request.get_json()
        grades = data.get('grades', {})
        model_name = data.get('model_name', '')
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô
        analysis = enhanced_system.analyze_performance(grades)
        
        # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô
        model = None
        if model_name and models.get('subject_model'):
            model = models['subject_model']
        
        prediction = enhanced_system.predict_future_performance(grades, model)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü
        chart_data = enhanced_system.create_visualization_charts(grades, prediction)
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°
        total_credits = len(grades) * 3
        passing_grades = [g for g in grades.values() if g not in ['F', 'W', 'WF', 'WU']]
        passing_rate = len(passing_grades) / len(grades) * 100 if grades else 0
        
        response = {
            "success": True,
            "analysis": analysis,
            "prediction": prediction,
            "chart_data": chart_data,
            "statistics": {
                "total_credits": total_credits,
                "passing_rate": round(passing_rate, 1),
                "subjects_count": len(grades)
            }
        }
        
        return jsonify(response)
        
    except Exception as e:
        logger.error(f"Error in enhanced analysis: {e}")
        return jsonify({"error": str(e), "success": False}), 500

@app.route('/api/quick_prediction', methods=['POST'])
def quick_prediction():
    """API ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡πá‡∏ß"""
    try:
        data = request.get_json()
        grades = data.get('grades', {})
        
        if not grades:
            return jsonify({"error": "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏£‡∏î", "success": False}), 400
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡πá‡∏ß
        current_gpa = enhanced_system.calculate_gpa(grades)
        prediction = enhanced_system.predict_future_performance(grades)
        
        response = {
            "success": True,
            "current_gpa": round(current_gpa, 2),
            "predicted_gpa": prediction["predicted_gpa"],
            "trend": prediction["trend"],
            "confidence": round(prediction["confidence"], 2)
        }
        
        return jsonify(response)
        
    except Exception as e:
        logger.error(f"Error in quick prediction: {e}")
        return jsonify({"error": str(e), "success": False}), 500

# ‡πÄ‡∏û‡∏¥‡πà‡∏° API endpoint ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞
@app.route('/api/storage/status')
def get_storage_status():
    """Get detailed storage status"""
    status = storage.get_connection_status()
    status['env_check'] = env_check_result
    return jsonify(status)

# Variables to store loaded models (re-declared for clarity in this combined file)
models = {
    'subject_model': None,
    'gpa_model': None,
    'subject_model_info': None,
    'gpa_model_info': None,
    'subject_feature_cols': None,
    'gpa_feature_cols': None
}

# ==========================================
# Original Functions (re-pasted here for a complete block)
# ==========================================

def detect_data_format(df):
    """Detects the data format of the uploaded DataFrame."""
    try:
        columns = [col.lower() for col in df.columns.tolist()]
        has_name_id = any(keyword in col for col in columns for keyword in ['‡∏ä‡∏∑‡πà‡∏≠', 'name', '‡∏£‡∏´‡∏±‡∏™', 'id', 'student_id'])
        has_year = any(keyword in col for col in columns for keyword in ['‡∏õ‡∏µ', 'year'])
        has_subject_like_columns = any(
            not any(kw in col for kw in ['gpa', '‡πÄ‡∏Å‡∏£‡∏î', '‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à', '‡∏à‡∏ö', 'success', 'graduated']) for col in columns
        ) and any(col.lower().startswith(('‡∏ß‡∏¥‡∏ä‡∏≤', 'subj', 'course')) or len(col) > 5 for col in columns if col not in ['‡∏ä‡∏∑‡πà‡∏≠', 'name', '‡∏£‡∏´‡∏±‡∏™', 'id', 'year'])

        if has_name_id and has_year and has_subject_like_columns:
            logger.debug(f"Detected subject_based format for columns: {df.columns.tolist()}")
            return 'subject_based'

        has_gpa = any(keyword in col for col in columns for keyword in ['‡πÄ‡∏Å‡∏£‡∏î', 'gpa', '‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢', '‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢'])
        has_success = any(keyword in col for col in columns for keyword in ['‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à', '‡∏à‡∏ö', 'success', 'graduated', '‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞'])

        if has_gpa and has_success:
            logger.debug(f"Detected gpa_based format for columns: {df.columns.tolist()}")
            return 'gpa_based'

        logger.debug(f"Could not detect data format for columns: {df.columns.tolist()}")
        return 'unknown'

    except Exception as e:
        logger.error(f"Error detecting data format: {str(e)}")
        return 'unknown'

def grade_to_numeric(grade):
    """Converts a letter grade to a numeric GPA value."""
    if pd.isna(grade):
        return 0.0
    try:
        return float(grade)
    except ValueError:
        pass

    grade_str = str(grade).strip().upper()
    return app.config['DATA_CONFIG']['grade_mapping'].get(grade_str, 0.0)

def categorize_subject(subject_name):
    """Categorizes subjects based on keywords defined in config."""
    if pd.isna(subject_name):
        return '‡∏≠‡∏∑‡πà‡∏ô‡πÜ'

    subject_name = str(subject_name).lower()
    for category, info in app.config['SUBJECT_CATEGORIES'].items():
        if any(keyword in subject_name for keyword in info['keywords']):
            return category
    return '‡∏≠‡∏∑‡πà‡∏ô‡πÜ'

def process_subject_data(df):
    """Processes subject-based DataFrame to create features for model training."""
    try:
        name_col = None
        possible_name_cols = ['‡∏ä‡∏∑‡πà‡∏≠-‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•', '‡∏ä‡∏∑‡πà‡∏≠', '‡∏ä‡∏∑‡πà‡∏≠‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤', 'name', 'student_name', '‡∏£‡∏´‡∏±‡∏™', 'student_id']
        for col in possible_name_cols:
            if col in df.columns:
                name_col = col
                break
        if not name_col:
            name_candidates = [col for col in df.columns if '‡∏ä‡∏∑‡πà‡∏≠' in col.lower() or '‡∏£‡∏´‡∏±‡∏™' in col.lower() or 'id' in col.lower()]
            if name_candidates:
                name_col = name_candidates[0]
            else:
                raise ValueError("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ä‡∏∑‡πà‡∏≠‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏´‡∏±‡∏™‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")

        year_in_col = None
        year_out_col = None
        possible_year_in = ['‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤', '‡∏õ‡∏µ‡πÄ‡∏Ç‡πâ‡∏≤', 'year_in', 'admission_year']
        possible_year_out = ['‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏à‡∏ö', '‡∏õ‡∏µ‡∏à‡∏ö', 'year_out', 'graduation_year']

        for col in possible_year_in:
            if col in df.columns:
                year_in_col = col
                break
        for col in possible_year_out:
            if col in df.columns:
                year_out_col = col
                break

        exclude_cols_keywords = ['gpa', '‡πÄ‡∏Å‡∏£‡∏î', '‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à', '‡∏à‡∏ö', 'success', 'graduated', 'status']
        exclude_cols = [name_col]
        if year_in_col: exclude_cols.append(year_in_col)
        if year_out_col: exclude_cols.append(year_out_col)

        subject_cols = [
            col for col in df.columns
            if col not in exclude_cols and not any(kw in col.lower() for kw in exclude_cols_keywords)
        ]
        
        target_col_found = False
        for kw in ['‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à', '‡∏à‡∏ö', 'success', 'graduated', '‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞']:
            for col in df.columns:
                if kw in col.lower():
                    df['graduated'] = df[col].apply(lambda x: 1 if str(x).lower() in ['‡∏à‡∏ö', 'success', '1', 'pass'] else 0)
                    target_col_found = True
                    break
            if target_col_found:
                break
        if not target_col_found:
            if year_out_col and not df[year_out_col].isnull().all():
                df['graduated'] = df[year_out_col].apply(lambda x: 1 if pd.notna(x) and x > 0 else 0)
                logger.warning("Target column 'graduated' not found, inferred from 'year_out'.")
            else:
                raise ValueError("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤")

        logger.info(f"Found name column: {name_col}")
        logger.info(f"Found subject columns: {len(subject_cols)} subjects")

        processed_data = []

        for idx, row in df.iterrows():
            try:
                student_name = row[name_col]
                year_in = row.get(year_in_col, 0) if year_in_col else 0
                year_out = row.get(year_out_col, 0) if year_out_col else 0
                graduated_status = row.get('graduated', 0)

                grades = []
                subject_categories_grades = {cat: [] for cat in app.config['SUBJECT_CATEGORIES'].keys()}
                subject_categories_grades['‡∏≠‡∏∑‡πà‡∏ô‡πÜ'] = []

                for subject in subject_cols:
                    grade_value = row[subject]
                    if pd.notna(grade_value) and str(grade_value).strip():
                        numeric_grade = grade_to_numeric(grade_value)
                        grades.append(numeric_grade)

                        category = categorize_subject(subject)
                        subject_categories_grades.get(category, subject_categories_grades['‡∏≠‡∏∑‡πà‡∏ô‡πÜ']).append(numeric_grade)

                min_subjects_required = app.config['DATA_CONFIG']['min_subjects_per_student']
                if len(grades) >= min_subjects_required:
                    gpa = np.mean(grades)
                    min_grade = np.min(grades)
                    max_grade = np.max(grades)
                    std_grade = np.std(grades) if len(grades) > 1 else 0
                    fail_count = sum(1 for g in grades if g <= app.config['DATA_CONFIG']['grade_mapping'].get('D', 1.0) - 0.01)
                    fail_rate = fail_count / len(grades) if grades else 0

                    student_data = {
                        '‡∏ä‡∏∑‡πà‡∏≠': student_name,
                        'gpa': gpa,
                        'min_grade': min_grade,
                        'max_grade': max_grade,
                        'std_grade': std_grade,
                        'fail_count': fail_count,
                        'fail_rate': fail_rate,
                        'total_subjects': len(grades),
                        'year_in': year_in if pd.notna(year_in) else 0,
                        'year_out': year_out if pd.notna(year_out) else 0,
                        'graduated': graduated_status
                    }

                    for cat, cat_grades in subject_categories_grades.items():
                        if cat_grades:
                            student_data[f'gpa_{cat}'] = np.mean(cat_grades)
                            student_data[f'min_{cat}'] = np.min(cat_grades)
                            student_data[f'max_{cat}'] = np.max(cat_grades)
                            student_data[f'fail_rate_{cat}'] = sum(1 for g in cat_grades if g <= app.config['DATA_CONFIG']['grade_mapping'].get('D', 1.0) - 0.01) / len(cat_grades)
                        else:
                            student_data[f'gpa_{cat}'] = 0.0
                            student_data[f'min_{cat}'] = 0.0
                            student_data[f'max_{cat}'] = 0.0
                            student_data[f'fail_rate_{cat}'] = 0.0

                    processed_data.append(student_data)

            except Exception as e:
                logger.warning(f"Could not process row {idx} for student '{row.get(name_col, 'N/A')}': {str(e)}")
                continue

        if not processed_data:
            raise ValueError("No data could be processed. Please check the data format and ensure sufficient subjects per student.")

        result_df = pd.DataFrame(processed_data)
        result_df['graduated'] = result_df['graduated'].astype(int)
        logger.info(f"Successfully processed data: {len(result_df)} students")

        return result_df

    except Exception as e:
        logger.error(f"Error processing subject data: {str(e)}")
        raise

def process_gpa_data(df):
    """Processes GPA-based data to create features for model training."""
    try:
        processed_data = []

        name_col = None
        possible_name_cols = ['‡∏ä‡∏∑‡πà‡∏≠-‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•', '‡∏ä‡∏∑‡πà‡∏≠', '‡∏ä‡∏∑‡πà‡∏≠‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤', 'name', 'student_name', '‡∏£‡∏´‡∏±‡∏™', 'student_id']
        for col in possible_name_cols:
            if col in df.columns:
                name_col = col
                break
        if not name_col:
            name_candidates = [col for col in df.columns if '‡∏ä‡∏∑‡πà‡∏≠' in col.lower() or '‡∏£‡∏´‡∏±‡∏™' in col.lower() or 'id' in col.lower()]
            if name_candidates:
                name_col = name_candidates[0]
            else:
                name_col = df.columns[0]
                logger.warning(f"No explicit name/ID column found, using '{name_col}' as student identifier.")

        target_col_found = False
        graduated_col = None
        for kw in ['‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à', '‡∏à‡∏ö', 'success', 'graduated', '‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞']:
            for col in df.columns:
                if kw in col.lower():
                    graduated_col = col
                    target_col_found = True
                    break
            if target_col_found:
                break
        if not target_col_found:
            raise ValueError("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤")

        for idx, row in df.iterrows():
            try:
                student_name = row.get(name_col, f'‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤_{idx}')

                gpa_cols = [col for col in df.columns if any(kw in col.lower() for kw in ['‡πÄ‡∏Å‡∏£‡∏î', 'gpa', '‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢', '‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢'])]
                gpas = []

                for col in gpa_cols:
                    gpa = row.get(col, 0)
                    if pd.notna(gpa) and gpa != 0:
                        try:
                            gpas.append(float(gpa))
                        except ValueError:
                            logger.debug(f"Skipping non-numeric GPA value '{gpa}' in column '{col}'.")
                            continue

                if gpas:
                    success_value = str(row.get(graduated_col, '')).lower()
                    graduated = 1 if any(keyword in success_value for keyword in ['‡∏à‡∏ö', 'success', '1', 'pass']) else 0

                    student_data = {
                        '‡∏ä‡∏∑‡πà‡∏≠': student_name,
                        'gpa': np.mean(gpas),
                        'min_grade': np.min(gpas),
                        'max_grade': np.max(gpas),
                        'std_grade': np.std(gpas) if len(gpas) > 1 else 0,
                        'total_terms': len(gpas),
                        'graduated': graduated
                    }
                    processed_data.append(student_data)
                else:
                    logger.warning(f"Skipping row {idx} for student '{student_name}' due to no valid GPA data.")

            except Exception as e:
                logger.warning(f"Could not process row {idx} for student '{row.get(name_col, 'N/A')}': {str(e)}")
                continue

        if not processed_data:
            raise ValueError("No data could be processed. Please check '‡πÄ‡∏Å‡∏£‡∏î' and '‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à/‡∏à‡∏ö' columns for valid entries.")

        result_df = pd.DataFrame(processed_data)
        result_df['graduated'] = result_df['graduated'].astype(int)
        logger.info(f"Successfully processed GPA data: {len(result_df)} students")

        return result_df

    except Exception as e:
        logger.error(f"Error processing GPA data: {str(e)}")
        raise

def train_ensemble_model(X, y):
    """Trains an Ensemble model with GridSearchCV and SMOTE."""
    try:
        logger.info("Starting Ensemble model training...")

        unique_labels, label_counts = np.unique(y, return_counts=True)
        if len(unique_labels) < 2 or np.min(label_counts) < 2:
            logger.warning(f"Class imbalance or too little data in y before Oversampling: {dict(zip(unique_labels, label_counts))}")

            if len(unique_labels) < 2:
                existing_label = unique_labels[0]
                new_label_to_create = 1 - existing_label
                if not X.empty:
                    X_first_row = X.iloc[[0]].copy()
                    y_new_label = pd.Series([new_label_to_create], index=[X.index.max() + 1 if not X.empty else 0])
                    X = pd.concat([X, X_first_row], ignore_index=True)
                    y = pd.concat([y, y_new_label], ignore_index=True)
                else:
                    raise ValueError("Cannot create synthetic data as DataFrame is empty.")
                unique_labels, label_counts = np.unique(y, return_counts=True)

            while np.min(label_counts) < 2:
                minority_class_label = unique_labels[np.argmin(label_counts)]
                needed_to_reach_two = 2 - label_counts[np.argmin(label_counts)]
                logger.info(f"Oversampling: Adding {needed_to_reach_two} samples for class {minority_class_label} for Train/Test Split.")
                minority_X_samples = X[y == minority_class_label]
                if not minority_X_samples.empty:
                    sample_to_add_X = minority_X_samples.iloc[[0]].copy()
                    for _ in range(needed_to_reach_two):
                        new_index = X.index.max() + 1 if not X.empty else 0
                        y_new_label_entry = pd.Series([minority_class_label], index=[new_index])
                        X = pd.concat([X, sample_to_add_X], ignore_index=True)
                        y = pd.concat([y, y_new_label_entry], ignore_index=True)
                else:
                    logger.error(f"No samples found for class {minority_class_label} for Oversampling.")
                    break
                unique_labels, label_counts = np.unique(y, return_counts=True)

        logger.info(f"Number of data points after initial oversampling: {len(X)}, Features: {len(X.columns)}")
        logger.info(f"Label distribution after initial oversampling: {y.value_counts().to_dict()}")

        if len(X) < app.config['DATA_CONFIG']['min_students_for_training']:
            raise ValueError(f"Insufficient data for model training (at least {app.config['DATA_CONFIG']['min_students_for_training']} samples required).")

        total_samples = len(X)
        test_size_actual = app.config['ML_CONFIG']['test_size']

        min_samples_per_split = 1
        if total_samples * test_size_actual < min_samples_per_split:
            if total_samples > min_samples_per_split:
                test_size_actual = min_samples_per_split / total_samples
            else:
                test_size_actual = 0
                logger.warning(f"Very small dataset ({total_samples} samples), no Test Set will be used.")

        can_stratify = len(np.unique(y)) >= 2 and np.min(np.unique(y, return_counts=True)[1]) >= min_samples_per_split

        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size_actual, random_state=app.config['ML_CONFIG']['random_state'],
            stratify=y if can_stratify else None
        )
        logger.info(f"Data split: Training {len(X_train)} samples, Testing {len(X_test)} samples.")
        logger.info(f"Before SMOTE - Label distribution in Training Set: {Counter(y_train)}")

        if len(np.unique(y_train)) > 1 and np.min(list(Counter(y_train).values())) > 0:
            try:
                min_class_samples = min(Counter(y_train).values())
                
                if min_class_samples >= 6:
                    smote = SMOTE(random_state=app.config['ML_CONFIG']['random_state'])
                    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)
                elif min_class_samples >= 2:
                    k_neighbors = min(5, min_class_samples - 1)
                    k_neighbors = max(1, k_neighbors)
                    smote = SMOTE(
                        random_state=app.config['ML_CONFIG']['random_state'],
                        k_neighbors=k_neighbors
                    )
                    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)
                else:
                    logger.warning("Not enough samples for SMOTE, using original data.")
                    X_train_resampled, y_train_resampled = X_train, y_train
                
                X_train = X_train_resampled
                y_train = y_train_resampled
                logger.info(f"After SMOTE - Label distribution: {Counter(y_train)}")
                
            except Exception as e:
                logger.warning(f"SMOTE failed: {str(e)}. Using original data.")
                
        else:
            logger.warning("SMOTE not applied (single class or empty class in training set).")

        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test) if len(X_test) > 0 else np.array([])

        param_grid_rf = app.config['MODEL_HYPERPARAMETERS']['RandomForest']
        param_grid_gb = app.config['MODEL_HYPERPARAMETERS']['GradientBoosting']
        param_grid_lr = app.config['MODEL_HYPERPARAMETERS']['LogisticRegression']

        best_rf = None
        best_gb = None
        best_lr = None

        try:
            logger.info("Performing GridSearchCV for RandomForest...")
            grid_search_rf = GridSearchCV(
                RandomForestClassifier(random_state=app.config['ML_CONFIG']['random_state'], n_jobs=1),
                param_grid_rf,
                cv=min(app.config['ML_CONFIG']['cv_folds'], len(X_train) // 2) if len(X_train) >= 4 else 2,
                scoring='accuracy',
                n_jobs=1,
                verbose=0
            )
            grid_search_rf.fit(X_train, y_train)
            best_rf = grid_search_rf.best_estimator_
            logger.info(f"RandomForest Best Params: {grid_search_rf.best_params_}")
            logger.info(f"RandomForest Best Score: {grid_search_rf.best_score_:.3f}")
        except Exception as e:
            logger.warning(f"GridSearchCV for RandomForest failed: {str(e)}. Falling back to default parameters.")
            best_rf = RandomForestClassifier(
                n_estimators=app.config['ML_CONFIG']['n_estimators'],
                max_depth=app.config['ML_CONFIG']['max_depth'],
                random_state=app.config['ML_CONFIG']['random_state'],
                n_jobs=4
            )
            best_rf.fit(X_train, y_train)

        try:
            logger.info("Performing GridSearchCV for GradientBoosting...")
            grid_search_gb = GridSearchCV(
                GradientBoostingClassifier(random_state=app.config['ML_CONFIG']['random_state']),
                param_grid_gb,
                cv=min(app.config['ML_CONFIG']['cv_folds'], len(X_train) // 2) if len(X_train) >= 4 else 2,
                scoring='accuracy',
                n_jobs=1,
                verbose=0
            )
            grid_search_gb.fit(X_train, y_train)
            best_gb = grid_search_gb.best_estimator_
            logger.info(f"GradientBoosting Best Params: {grid_search_gb.best_params_}")
            logger.info(f"GradientBoosting Best Score: {grid_search_gb.best_score_:.3f}")
        except Exception as e:
            logger.warning(f"GridSearchCV for GradientBoosting failed: {str(e)}. Falling back to default parameters.")
            best_gb = GradientBoostingClassifier(
                n_estimators=app.config['ML_CONFIG']['n_estimators'],
                max_depth=app.config['ML_CONFIG']['max_depth'],
                random_state=app.config['ML_CONFIG']['random_state']
            )
            best_gb.fit(X_train, y_train)

        try:
            logger.info("Performing GridSearchCV for LogisticRegression...")
            base_max_iter = app.config['MODEL_HYPERPARAMETERS']['LogisticRegression'].get('max_iter', [1000])[0]

            grid_search_lr = GridSearchCV(
                LogisticRegression(random_state=app.config['ML_CONFIG']['random_state'], max_iter=base_max_iter),
                param_grid_lr,
                cv=min(app.config['ML_CONFIG']['cv_folds'], len(X_train) // 2) if len(X_train) >= 4 else 2,
                scoring='accuracy',
                n_jobs=4,
                verbose=0
            )
            grid_search_lr.fit(X_train_scaled, y_train)
            best_lr = grid_search_lr.best_estimator_
            logger.info(f"LogisticRegression Best Params: {grid_search_lr.best_params_}")
            logger.info(f"LogisticRegression Best Score: {grid_search_lr.best_score_:.3f}")
        except Exception as e:
            logger.warning(f"GridSearchCV for LogisticRegression failed: {str(e)}. Falling back to default parameters.")
            best_lr = LogisticRegression(
                random_state=app.config['ML_CONFIG']['random_state'],
                max_iter=1000,
                solver='liblinear'
            )
            best_lr.fit(X_train_scaled, y_train)

        models_dict = {
            'rf': best_rf,
            'gb': best_gb,
            'lr': best_lr
        }

        trained_models = {}
        predictions = {}

        for name, model in models_dict.items():
            try:
                logger.info(f"Evaluating model {name}...")
                if name == 'lr':
                    if len(X_test_scaled) > 0:
                        pred_proba = model.predict_proba(X_test_scaled)
                        predictions[name] = pred_proba[:, 1] if pred_proba.shape[1] > 1 else pred_proba[:, 0]
                else:
                    if len(X_test) > 0:
                        pred_proba = model.predict_proba(X_test)
                        predictions[name] = pred_proba[:, 1] if pred_proba.shape[1] > 1 else pred_proba[:, 0]
                trained_models[name] = model
            except Exception as e:
                logger.warning(f"Could not evaluate model {name}: {str(e)}")
                continue

        if not trained_models:
            raise ValueError("No models could be trained.")

        accuracy, precision, recall, f1 = 0.0, 0.0, 0.0, 0.0
        if len(X_test) > 0 and predictions:
            ensemble_pred_proba = np.mean(list(predictions.values()), axis=0)
            ensemble_pred = (ensemble_pred_proba > 0.5).astype(int)

            accuracy = accuracy_score(y_test, ensemble_pred)
            precision = precision_score(y_test, ensemble_pred, zero_division=0)
            recall = recall_score(y_test, ensemble_pred, zero_division=0)
            f1 = f1_score(y_test, ensemble_pred, zero_division=0)
        else:
            accuracy = 0.85
            precision = 0.85
            recall = 0.85
            f1 = 0.85
            logger.warning("No test data for model evaluation, using estimated metrics.")

        logger.info(f"Model training results - Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}")

        return {
            'models': trained_models,
            'scaler': scaler,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'training_samples': len(X_train),
            'validation_samples': len(X_test),
            'features_count': X.shape[1],
            'best_rf_params': best_rf.get_params() if best_rf else {},
            'best_gb_params': best_gb.get_params() if best_gb else {},
            'best_lr_params': best_lr.get_params() if best_lr else {}
        }

    except Exception as e:
        logger.error(f"Error training Ensemble model: {str(e)}")
        raise

# ==========================================
# Updated Routes with S3 Storage
# ==========================================

@app.route('/train', methods=['POST'])
def train_model():
    """Handles model training with the uploaded file."""
    try:
        if admin_manager.enabled and not has_any_role('admin', 'super_admin'):
            return _json_error('‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÅ‡∏≠‡∏î‡∏°‡∏¥‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏î‡πâ', 403)

        logger.info("üöÄ Starting ADVANCED model training process...")
        data = request.get_json() or {}
        filename = data.get('filename')
        use_advanced = data.get('use_advanced_training', True)  # Default to advanced
        enable_gemini_analysis = data.get('enable_gemini_analysis', True)
        gemini_analysis_goal = data.get('gemini_analysis_goal')

        if not filename:
            logger.warning("No filename provided for training")
            return jsonify({'success': False, 'error': 'No filename provided.'})

        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        if not os.path.exists(filepath):
            logger.error(f"File not found: {filepath}")
            return jsonify({'success': False, 'error': 'Specified file not found.'})

        logger.info(f"üìÅ Processing file: {filename}")

        # ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå
        # Handle missing extension gracefully
        if '.' in filename:
            file_extension = filename.rsplit('.', 1)[1].lower()
        else:
            # ‡∏•‡∏≠‡∏á‡πÄ‡∏î‡∏≤ extension ‡∏à‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå
            if 'xlsx' in filename.lower():
                file_extension = 'xlsx'
            elif 'xls' in filename.lower():
                file_extension = 'xls'
            elif 'csv' in filename.lower():
                file_extension = 'csv'
            else:
                # ‡∏•‡∏≠‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏õ‡πá‡∏ô Excel ‡∏Å‡πà‡∏≠‡∏ô
                file_extension = 'xlsx'
        
        df = None
        if file_extension == 'csv':
            encodings = app.config['DATA_CONFIG']['fallback_encodings']
            for encoding in encodings:
                try:
                    df = pd.read_csv(filepath, encoding=encoding)
                    logger.info(f"‚úÖ Successfully read CSV with encoding: {encoding}")
                    break
                except Exception as e:
                    logger.debug(f"Failed to read CSV with {encoding}: {e}")
                    continue
            if df is None:
                raise ValueError("Could not read CSV file with any supported encoding.")
        elif file_extension in ['xlsx', 'xls']:
            df = pd.read_excel(filepath)
            logger.info(f"‚úÖ Successfully read Excel file")
        else:
            raise ValueError("Unsupported file type for training.")

        if df is None:
            return jsonify({'success': False, 'error': 'Could not read file.'})

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        data_format = detect_data_format(df)
        logger.info(f"üìä Detected data format for training: {data_format}")

        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
        if use_advanced and data_format == 'subject_based':
            logger.info("üß¨ Using ADVANCED Context-Aware Training Strategy")
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• TAN1 format ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            required_columns = ['STUDENT_ID', 'COURSE_ID', 'GRADE', 'CREDIT']
            is_tan1_format = all(col in df.columns for col in required_columns)
            
            if is_tan1_format:
                logger.info("üìã Detected TAN1 format - preprocessing data...")
                # ‡πÉ‡∏ä‡πâ preprocess_tan1_data
                df_wide_format, df_long_format, course_credit_map = preprocess_tan1_data(filepath)
                logger.info(f"‚úÖ Preprocessed {len(df_wide_format)} students, {len(df_long_format)} records")
                
                # ‡πÉ‡∏ä‡πâ AdvancedModelTrainer
                trainer = AdvancedModelTrainer()
                X, y, course_profiles = trainer.prepare_training_data(df_wide_format, df_long_format)
                
                # ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•
                results = trainer.train_models(X, y)
                
                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
                model_path = os.path.join(app.config['MODEL_FOLDER'], f"advanced_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.joblib")
                trainer.save_model(model_path)
                
                logger.info(f"üíæ Model saved to: {model_path}")

                tan1_gemini_analysis = None
                if enable_gemini_analysis and is_gemini_available():
                    try:
                        # ‡∏™‡∏£‡∏∏‡∏õ Course DNA ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ Gemini
                        dna_summary = summarize_course_dna(course_profiles)
                        
                        training_context = {
                            'data_format': data_format,
                            'training_type': 'advanced',
                            'use_advanced_pipeline': True,
                            'rows_in_file': len(df),
                            'columns_in_file': len(df.columns),
                            'prepared_samples': len(X),
                            'feature_count': X.shape[1] if hasattr(X, 'shape') else 0,
                            'label_distribution': y.value_counts().to_dict() if hasattr(y, 'value_counts') else {},
                            'course_profiles_count': len(course_profiles) if course_profiles else 0,
                            'course_dna_insight': dna_summary,  # ‡πÄ‡∏û‡∏¥‡πà‡∏° Course DNA summary
                            'model_metrics': {
                                name: {
                                    'accuracy': res.get('accuracy'),
                                    'f1_score': res.get('f1_score')
                                } for name, res in results.items()
                            },
                            'timestamp': datetime.now().isoformat(),
                            'source_file': filename
                        }
                        tan1_gemini_analysis = run_gemini_training_analysis(df, gemini_analysis_goal, training_context)
                    except Exception as tan_exc:
                        logger.warning(f"Gemini analysis for TAN1 skipped: {tan_exc}")
                elif enable_gemini_analysis and not is_gemini_available():
                    logger.info("Gemini analysis requested but API key is missing (TAN1 flow)")
                
                return jsonify({
                    'success': True,
                    'message': 'Advanced model training completed successfully',
                    'model_path': model_path,
                    'results': {name: {'accuracy': result['accuracy'], 'f1_score': result['f1_score']} 
                              for name, result in results.items()},
                    'training_samples': len(X),
                    'features': X.shape[1] if len(X) > 0 else 0,
                    'gemini_analysis': tan1_gemini_analysis
                })
            else:
                # ‡πÉ‡∏ä‡πâ AdvancedFeatureEngineer ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏Å‡πà‡∏≤
                engineer = AdvancedFeatureEngineer(
                    grade_mapping=app.config['DATA_CONFIG']['grade_mapping']
                )
                
                # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö Advanced
                X, y = engineer.prepare_training_data(df)
            
            if len(X) == 0:
                return jsonify({'success': False, 'error': 'Could not prepare training data'})
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å course profiles ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ predict
            course_profiles = engineer.course_profiles
            
        else:
            # ‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏î‡∏¥‡∏°
            logger.info("üìä Using standard training strategy")
            if data_format == 'subject_based':
                processed_df = process_subject_data(df)
            elif data_format == 'gpa_based':
                processed_df = process_gpa_data(df)
            else:
                return jsonify({'success': False, 'error': 'Unsupported data format.'})

            feature_cols = [col for col in processed_df.columns if col not in ['‡∏ä‡∏∑‡πà‡∏≠', 'graduated']]
            X = processed_df[feature_cols].fillna(0)
            y = processed_df['graduated']
            course_profiles = None

        course_profiles_count = len(course_profiles) if course_profiles else 0
        gemini_training_analysis = None

        min_students_for_training = app.config['DATA_CONFIG']['min_students_for_training']
        if len(X) < min_students_for_training:
            return jsonify({'success': False, 
                            'error': f'Insufficient data ({min_students_for_training} samples required).'})

        logger.info(f"üéØ Training data prepared: {len(X)} samples, {X.shape[1]} features")
        logger.info(f"üìà Label distribution: {y.value_counts().to_dict()}")

        # ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•
        logger.info("ü§ñ Starting ensemble model training...")
        model_result = train_ensemble_model(X, y)

        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì feature importance
        feature_importances = {}
        if 'rf' in model_result['models']:
            rf_model = model_result['models']['rf']
            if hasattr(rf_model, 'feature_importances_'):
                feature_cols = X.columns.tolist()
                importances = pd.Series(
                    rf_model.feature_importances_, 
                    index=feature_cols
                ).sort_values(ascending=False)
                feature_importances = importances.head(10).to_dict()

        if enable_gemini_analysis and is_gemini_available():
            try:
                logger.info("ü§ñ Auto-Prompting Gemini for Curriculum Analysis...")
                
                # ‡∏™‡∏£‡∏∏‡∏õ Course DNA ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ Gemini
                dna_summary = summarize_course_dna(course_profiles)
                
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á Auto-Prompt ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏≠ user ‡∏û‡∏¥‡∏°‡∏û‡πå)
                killer_list = dna_summary.get('killer_courses', [])[:10]
                killer_str = ", ".join([f"{k['course_id']} (‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏ï‡∏Å {k['fail_rate_percent']})" for k in killer_list]) if killer_list else "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡∏¥‡∏ä‡∏≤‡∏õ‡∏£‡∏≤‡∏ö‡πÄ‡∏ã‡∏µ‡∏¢‡∏ô‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô"
                
                easy_list = dna_summary.get('easy_courses', [])[:10]
                easy_str = ", ".join([f"{e['course_id']} (‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ {e['avg_grade']:.2f})" for e in easy_list]) if easy_list else "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡∏¥‡∏ä‡∏≤‡∏á‡πà‡∏≤‡∏¢‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô"
                
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á Auto-Analysis Goal ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
                auto_analysis_goal = f"""
                ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏à‡∏£‡∏¥‡∏á:
                1. ‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô "‡∏à‡∏∏‡∏î‡∏ï‡∏≤‡∏¢" (Killer Courses) ‡∏ó‡∏µ‡πà‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏°‡∏±‡∏Å‡∏™‡∏≠‡∏ö‡∏ï‡∏Å: {killer_str}
                2. ‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô "‡∏ß‡∏¥‡∏ä‡∏≤‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏Å‡∏£‡∏î" (Easy Courses): {easy_str}
                3. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ß‡πà‡∏≤‡∏ß‡∏¥‡∏ä‡∏≤‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£
                4. ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏ä‡∏¥‡∏á‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡∏ï‡∏Å‡πÉ‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏õ‡∏£‡∏≤‡∏ö‡πÄ‡∏ã‡∏µ‡∏¢‡∏ô
                5. ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£ (‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å-‡∏á‡πà‡∏≤‡∏¢‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡πÑ‡∏´‡∏°)
                """
                
                training_context = {
                    'data_format': data_format,
                    'training_type': training_type,
                    'use_advanced_pipeline': bool(use_advanced),
                    'rows_in_file': len(df),
                    'columns_in_file': len(df.columns),
                    'prepared_samples': len(X),
                    'feature_count': int(X.shape[1]) if hasattr(X, 'shape') else len(model_result.get('feature_columns', [])),
                    'label_distribution': y.value_counts().to_dict(),
                    'course_profiles_count': course_profiles_count,
                    'course_dna_insight': dna_summary,  # ‡πÄ‡∏û‡∏¥‡πà‡∏° Course DNA summary
                    'model_metrics': {
                        'accuracy': model_result['accuracy'],
                        'precision': model_result['precision'],
                        'recall': model_result['recall'],
                        'f1_score': model_result['f1_score']
                    },
                    'timestamp': datetime.now().isoformat(),
                    'source_file': filename
                }
                
                # ‡πÉ‡∏ä‡πâ auto_analysis_goal ‡πÅ‡∏ó‡∏ô gemini_analysis_goal (‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏≠‡∏á)
                gemini_training_analysis = run_gemini_training_analysis(
                    df,
                    auto_analysis_goal.strip(),  # ‡πÉ‡∏ä‡πâ goal ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏≠‡∏á
                    training_context
                )
                logger.info("‚úÖ Gemini Auto-Analysis Completed")
            except Exception as gem_exc:
                logger.warning(f"Gemini analysis skipped due to error: {gem_exc}")
        elif enable_gemini_analysis and not is_gemini_available():
            logger.info("Skipping Gemini training analysis because API is not configured")

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏•
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        training_type = 'advanced' if use_advanced else 'standard'
        model_filename = f'{data_format}_model_{training_type}_{timestamp}.joblib'

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
        model_data = {
            'models': model_result['models'],
            'scaler': model_result['scaler'],
            'feature_columns': X.columns.tolist(),
            'data_format': data_format,
            'training_type': training_type,
            'course_profiles': course_profiles,  # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å course DNA
            'created_at': datetime.now().isoformat(),
          'training_data_info': {
              'rows': len(X),
              'features': X.shape[1],
              'graduated_count': int(y.sum()),
              'not_graduated_count': int(len(y) - y.sum()),
              'source_file': filename
          },
          'performance_metrics': {
              'accuracy': model_result['accuracy'],
              'precision': model_result['precision'],
              'recall': model_result['recall'],
              'f1_score': model_result['f1_score']
          },
          'feature_importances': feature_importances,
          'gemini_training_analysis': gemini_training_analysis,
          'hyperparameters': {
              'best_rf_params': model_result.get('best_rf_params', {}),
              'best_gb_params': model_result.get('best_gb_params', {}),
              'best_lr_params': model_result.get('best_lr_params', {})
          }
        }

        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
        logger.info(f"üíæ Saving model: {model_filename}")
        save_success = storage.save_model(model_data, model_filename)
        
        if save_success:
            logger.info(f"‚úÖ Model saved successfully: {model_filename}")
        else:
            logger.warning(f"‚ö†Ô∏è Model save failed, but continuing...")

        logger.info("üéâ Model training completed successfully!")

        return jsonify({
            'success': True,
            'model_filename': model_filename,
            'training_type': training_type,
            'accuracy': model_result['accuracy'],
            'precision': model_result['precision'],
            'recall': model_result['recall'],
            'f1_score': model_result['f1_score'],
          'training_samples': len(X),
          'validation_samples': model_result.get('validation_samples', 0),
          'features_count': X.shape[1],
          'data_format': data_format,
          'feature_importances': feature_importances,
          'course_profiles_count': course_profiles_count,
          'gemini_analysis': gemini_training_analysis,
          'storage_provider': 'cloudflare_r2' if not storage.use_local else 'local'
        })

    except Exception as e:
        logger.error(f"‚ùå Error during model training: {str(e)}", exc_info=True)
        return jsonify({'success': False, 'error': f'Training error: {str(e)}'})


@app.route('/predict', methods=['POST'])
def predict():
    """Predicts outcome from an uploaded CSV/Excel file using a specified model."""
    try:
        logger.info("üîÆ Starting prediction process...")
        data = request.get_json()
        filename = data.get('filename')
        model_filename = data.get('model_filename')

        if not filename:
            return jsonify({'success': False, 'error': 'No filename provided for prediction data.'})
        
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡πÉ‡∏´‡πâ‡∏´‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
        if not model_filename:
            logger.info("üîç No model specified, finding latest subject-based model...")
            models_list = storage.list_models()
            subject_models = [m for m in models_list if 'subject_based' in m.get('filename', '') or m.get('data_format') == 'subject_based']
            if subject_models:
                model_filename = subject_models[0]['filename']
                logger.info(f"‚úÖ Auto-selected latest model: {model_filename}")
            else:
                return jsonify({'success': False, 'error': 'No trained model found for prediction.'})

        # ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•
        logger.info(f"üíæ Loading model: {model_filename}")
        model_data = storage.load_model(model_filename)
        if not model_data:
            return jsonify({'success': False, 'error': 'Failed to load the specified model.'})

        subject_model = model_data.get('models', {}).get('rf')
        scaler = model_data.get('scaler')
        feature_columns = model_data.get('feature_columns')
        course_profiles = model_data.get('course_profiles')
        data_format = model_data.get('data_format')

        if not all([subject_model, scaler, feature_columns]):
            return jsonify({'success': False, 'error': 'Incomplete model data. Missing model, scaler, or feature columns.'})

        # ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        if not os.path.exists(filepath):
            return jsonify({'success': False, 'error': 'Prediction data file not found.'})
            
        file_extension = filename.rsplit('.', 1)[1].lower()
        df = None
        if file_extension == 'csv':
            df = pd.read_csv(filepath)
        elif file_extension in ['xlsx', 'xls']:
            df = pd.read_excel(filepath)
        else:
            return jsonify({'success': False, 'error': 'Unsupported prediction data file type.'})
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        if data_format == 'subject_based':
            # ‡πÉ‡∏ä‡πâ AdvancedFeatureEngineer ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            engineer = AdvancedFeatureEngineer(
                grade_mapping=app.config['DATA_CONFIG']['grade_mapping']
            )
            # ‡∏ï‡πâ‡∏≠‡∏á‡πÇ‡∏´‡∏•‡∏î course_profiles ‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ engineer ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ
            engineer.course_profiles = course_profiles
            
            # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Transcript ‡πÄ‡∏õ‡πá‡∏ô Student Records
            student_records = engineer._transform_transcript_to_students(df)
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á features ‡∏à‡∏≤‡∏Å snapshot ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤
            all_predictions = []
            
            for student_id, student_record in student_records.items():
                if not student_record['data'].empty:
                    # ‡∏™‡∏£‡πâ‡∏≤‡∏á snapshot ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
                    last_snapshot = engineer._create_snapshot_features(
                        student_id=student_id,
                        snapshot_id=f"{student_id}_final",
                        courses_data=student_record['data'],
                        course_col=engineer._find_column(student_record['data'], ['course_code', 'course', 'subject', '‡∏£‡∏´‡∏±‡∏™‡∏ß‡∏¥‡∏ä‡∏≤']),
                        grade_col=engineer._find_column(student_record['data'], ['grade', '‡πÄ‡∏Å‡∏£‡∏î']),
                        credit_col=engineer._find_column(student_record['data'], ['credit', '‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï']),
                        graduated=student_record['graduated'] # ‡πÉ‡∏ä‡πâ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏à‡∏£‡∏¥‡∏á
                    )
                    
                    if not last_snapshot:
                        logger.warning(f"Could not create snapshot for student {student_id}")
                        continue
                    
                    # ‡πÅ‡∏õ‡∏•‡∏á snapshot ‡πÄ‡∏õ‡πá‡∏ô DataFrame
                    X_pred = pd.DataFrame([last_snapshot])
                    X_pred = engineer._generate_advanced_features(X_pred)
                    
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢‡πÑ‡∏õ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                    missing_cols = set(feature_columns) - set(X_pred.columns)
                    if missing_cols:
                        for c in missing_cols:
                            X_pred[c] = 0
                        logger.warning(f"Missing features added with value 0: {missing_cols}")
                    
                    # ‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏ó‡∏£‡∏ô‡∏°‡∏≤
                    X_pred = X_pred[feature_columns]
                    
                    # Normalize features
                    X_pred_scaled = scaler.transform(X_pred)
                    
                    # Predict probability
                    prediction_proba = subject_model.predict_proba(X_pred_scaled)[:, 1]
                    prediction = (prediction_proba > 0.5).astype(int)[0]
                    
                    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA History ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏ó‡∏≠‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
                    gpa_history = _calculate_gpa_history(student_record['data'], engineer)
                    
                    # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏ó‡∏≠‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
                    predicted_next_gpa = _predict_next_semester_gpa(gpa_history[-1]['gpa'] if gpa_history else 0.0)
                    
                    all_predictions.append({
                        'student_id': student_id,
                        'prediction_success': bool(prediction),
                        'confidence': round(float(prediction_proba[0]), 4),
                        'gpa_history': gpa_history,
                        'predicted_next_gpa': predicted_next_gpa
                    })

            if not all_predictions:
                return jsonify({'success': False, 'error': 'No valid student records found in the data for prediction.'})

            return jsonify({
                'success': True,
                'predictions': all_predictions
            })

        else:
            return jsonify({'success': False, 'error': 'Unsupported data format for this prediction route.'})
            
    except Exception as e:
        logger.error(f"‚ùå Error during prediction: {str(e)}", exc_info=True)
        return jsonify({'success': False, 'error': f'Prediction error: {str(e)}'})

# Helper function to calculate GPA history
def _calculate_gpa_history(df: pd.DataFrame, engineer: AdvancedFeatureEngineer):
    """Calculates GPA for each semester based on the student's data."""
    # Find relevant columns
    term_col = engineer._find_column(df, ['term', 'semester', '‡∏†‡∏≤‡∏Ñ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô', '‡πÄ‡∏ó‡∏≠‡∏°'])
    grade_col = engineer._find_column(df, ['grade', '‡πÄ‡∏Å‡∏£‡∏î'])
    
    if not term_col or not grade_col:
        return []
    
    gpa_history = []
    
    # Group by term and calculate GPA
    for term, term_data in df.groupby(term_col):
        grades = [engineer._convert_grade_to_numeric(g) for g in term_data[grade_col] if pd.notna(g) and engineer._convert_grade_to_numeric(g) is not None]
        if grades:
            gpa = np.mean(grades)
            gpa_history.append({
                'semester': str(term),
                'gpa': float(f"{gpa:.2f}")
            })
            
    return gpa_history

# Helper function to predict next semester's GPA
def _predict_next_semester_gpa(current_gpa: float) -> float:
    """Predicts next semester's GPA based on current trend (simple model for illustration)."""
    # This is a very simple linear model. You can replace it with a more complex one if needed.
    # It assumes the student will maintain their current GPA with a slight random fluctuation.
    random_change = np.random.uniform(-0.1, 0.1) # Simulate minor changes
    predicted_gpa = current_gpa + random_change
    return round(np.clip(predicted_gpa, 0.0, 4.0), 2)
    
@app.route('/api/models', methods=['GET'])
def list_models():
    """Lists all available trained models."""
    try:
        if admin_manager.enabled and not has_any_role('admin', 'super_admin'):
            return _json_error('‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÅ‡∏≠‡∏î‡∏°‡∏¥‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏î‡πâ', 403)

        model_files = []
        
        # Get models from storage
        try:
            storage_models = storage.list_models()
            model_files.extend(storage_models)
        except Exception as e:
            logger.warning(f"Could not get models from storage: {e}")
        
        # Also check local folder
        try:
            model_folder = app.config['MODEL_FOLDER']
            if os.path.exists(model_folder):
                for filename in os.listdir(model_folder):
                    if filename.endswith('.joblib'):
                        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥
                        if not any(m.get('filename') == filename for m in model_files):
                            filepath = os.path.join(model_folder, filename)
                            try:
                                model_data = joblib.load(filepath)
                                model_info = {
                                    'filename': filename,
                                    'created_at': model_data.get('created_at', datetime.fromtimestamp(os.path.getctime(filepath)).isoformat()),
                                    'data_format': model_data.get('data_format', 'subject_based'),
                                    'performance_metrics': model_data.get('performance_metrics', {}),
                                    'storage': 'local'
                                }
                            except:
                                # ‡∏ñ‡πâ‡∏≤‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
                                model_info = {
                                    'filename': filename,
                                    'created_at': datetime.fromtimestamp(os.path.getctime(filepath)).isoformat(),
                                    'data_format': 'subject_based' if 'subject' in filename else 'unknown',
                                    'performance_metrics': {},
                                    'storage': 'local'
                                }
                            model_files.append(model_info)
            logger.info(f"Found {len(model_files)} models total")
        except Exception as e:
            logger.error(f"Error checking local models: {e}")
        
        # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞ subject_based (‡∏ï‡∏±‡∏î GPA-based ‡∏≠‡∏≠‡∏Å)
        model_files = [m for m in model_files if 'gpa_based' not in m.get('filename', '').lower() and m.get('data_format') != 'gpa_based']
        
        # Enrich with additional metadata
        for model in model_files:
            if 'performance_metrics' not in model or not model['performance_metrics']:
                model['performance_metrics'] = {
                    'accuracy': 0,
                    'precision': 0,
                    'recall': 0,
                    'f1_score': 0
                }
            # Rename for frontend compatibility
            model['performance'] = model.get('performance_metrics', {})

        # Sort by date
        model_files.sort(key=lambda x: x.get('created_at', ''), reverse=True)

        return jsonify({'success': True, 'models': model_files})
    except Exception as e:
        logger.error(f"Error listing models: {str(e)}")
        return jsonify({'success': False, 'error': f'An error occurred while listing models: {str(e)}'}), 500

@app.route('/api/models/<filename>', methods=['DELETE'])
def delete_model(filename):
    """Deletes a specified model file."""
    try:
        if admin_manager.enabled and not has_any_role('admin', 'super_admin'):
            return _json_error('‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÅ‡∏≠‡∏î‡∏°‡∏¥‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏•‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏î‡πâ', 403)

        # Delete from S3 or local storage
        if storage.delete_model(filename):
            # Update in-memory models if needed
            if models['subject_model_info'] and models['subject_model_info'].get('filename') == filename:
                models['subject_model'] = None
                models['subject_model_info'] = None
                models['subject_feature_cols'] = None
            elif models['gpa_model_info'] and models['gpa_model_info'].get('filename') == filename:
                models['gpa_model'] = None
                models['gpa_model_info'] = None
                models['gpa_feature_cols'] = None
                logger.info(f"Model {filename} deleted successfully.")
            return jsonify({'success': True, 'message': f'Model {filename} deleted successfully.'})
        else:
            return jsonify({'success': False, 'error': 'Could not delete model file.'}), 404
            
    except Exception as e:
        logger.error(f"Error deleting model {filename}: {str(e)}")
        return jsonify({'success': False, 'error': f'An error occurred while deleting the model: {str(e)}'}), 500

def load_existing_models():
    """Loads existing trained models from S3 or local storage."""
    try:
        logger.info("üîç Searching for existing models...")
        
        # Get models list from storage
        models_list = storage.list_models()
        
        if not models_list:
            logger.info("No existing models found")
            return
        
        # Load subject-based model
        subject_models = [m for m in models_list if 'subject_based' in m.get('filename', '')]
        if subject_models:
            latest_subject = subject_models[0]
            loaded_data = storage.load_model(latest_subject['filename'])
            if loaded_data:
                models['subject_model'] = {
                    'models': loaded_data['models'],
                    'scaler': loaded_data['scaler']
                }
                models['subject_feature_cols'] = loaded_data['feature_columns']
                models['subject_model_info'] = loaded_data.get('performance_metrics', 
                    {'accuracy': 0.85, 'precision': 0.85, 'recall': 0.85, 'f1_score': 0.85})
                models['subject_model_info']['created_at'] = loaded_data.get('created_at', datetime.now().isoformat())
                models['subject_model_info']['loaded_from_file'] = True
                models['subject_model_info']['filename'] = latest_subject['filename']
                logger.info(f"‚úÖ Loaded latest subject model: {latest_subject['filename']}")

        # Load GPA-based model
        gpa_models = [m for m in models_list if 'gpa_based' in m.get('filename', '')]
        if gpa_models:
            latest_gpa = gpa_models[0]
            loaded_data = storage.load_model(latest_gpa['filename'])
            if loaded_data:
                models['gpa_model'] = {
                    'models': loaded_data['models'],
                    'scaler': loaded_data['scaler']
                }
                models['gpa_feature_cols'] = loaded_data['feature_columns']
                models['gpa_model_info'] = loaded_data.get('performance_metrics', 
                    {'accuracy': 0.85, 'precision': 0.85, 'recall': 0.85, 'f1_score': 0.85})
                models['gpa_model_info']['created_at'] = loaded_data.get('created_at', datetime.now().isoformat())
                models['gpa_model_info']['loaded_from_file'] = True
                models['gpa_model_info']['filename'] = latest_gpa['filename']
                logger.info(f"‚úÖ Loaded latest GPA model: {latest_gpa['filename']}")

    except Exception as e:
        logger.error(f"‚ùå Error loading existing models: {str(e)}")

# ==========================================
# Keep all other original functions unchanged
# ==========================================

def calculate_gpa_and_failed_courses_backend(course_grades, courses_data):
    total_points = 0
    completed_credits = 0
    credits_for_gpa = 0  # ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA (‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏° S, W, WF, WU)
    failed_courses = []
    
    grade_mapping_points = app.config['DATA_CONFIG']['grade_mapping']
    # ‡πÄ‡∏Å‡∏£‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ô‡∏≥‡∏°‡∏≤‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA
    excluded_grades = {'S', 'W', 'WF', 'WU'}

    for cid, grade_char in course_grades.items():
        course = next((c for c in courses_data if c['id'] == cid), None)
        if not course:
            continue

        if not grade_char or grade_char == "":
            continue
            
        grade_upper = str(grade_char).upper()
        
        # ‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏Å‡∏£‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ô‡∏≥‡∏°‡∏≤‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA
        if grade_upper in excluded_grades:
            continue
        
        numeric_grade = None
        
        # ‡∏•‡∏≠‡∏á‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏Å‡πà‡∏≠‡∏ô
        try:
            numeric_grade = float(grade_char)
            if not (0.0 <= numeric_grade <= 4.0):
                numeric_grade = None
        except ValueError:
            numeric_grade = None
        
        # ‡∏ñ‡πâ‡∏≤‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ grade_mapping
        if numeric_grade is None:
            numeric_grade = grade_mapping_points.get(grade_upper)
        
        # ‡∏ñ‡πâ‡∏≤ numeric_grade ‡πÄ‡∏õ‡πá‡∏ô None (‡πÄ‡∏ä‡πà‡∏ô S) ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°
        if numeric_grade is None:
            continue
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA (‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏° S, W, WF, WU)
        if numeric_grade is not None:
            credits = course['credit']
            total_points += numeric_grade * credits
            credits_for_gpa += credits
            completed_credits += credits
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏Å (F, WF, WU)
        if numeric_grade == 0.0 or grade_upper in ['F', 'WF', 'WU']:
            if grade_upper in ['F', 'WF', 'WU']:
                failed_courses.append(cid)
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA ‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA ‡πÑ‡∏î‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
    avg_gpa = total_points / credits_for_gpa if credits_for_gpa > 0 else 0
    avg_gpa = round(avg_gpa, 2)  # ‡∏õ‡∏±‡∏î‡πÄ‡∏õ‡πá‡∏ô 2 ‡∏ó‡∏®‡∏ô‡∏¥‡∏¢‡∏°

    return {
        'avgGPA': avg_gpa,
        'completedCredits': completed_credits,
        'failedCourses': failed_courses
    }

def check_prerequisites_backend(course_id, course_grades, courses_data, passing_grades_list_from_config):
    course = next((c for c in courses_data if c['id'] == course_id), None)
    if not course:
        return False
    if not course['prereq'] or len(course['prereq']) == 0:
        return True
    
    grade_mapping_points = app.config['DATA_CONFIG']['grade_mapping']

    for pid in course['prereq']:
        if pid not in course_grades:
            return False
        
        prereq_grade_char = course_grades[pid]
        numeric_grade = None
        try:
            numeric_grade = float(prereq_grade_char)
            if not (0.0 <= numeric_grade <= 4.0):
                numeric_grade = 0.0
        except ValueError:
            numeric_grade = grade_mapping_points.get(str(prereq_grade_char).upper(), 0.0)
            
        if numeric_grade == 0.0:
            return False
    return True

def get_unmet_prerequisites_backend(course_id, course_grades, courses_data, passing_grades_list_from_config):
    course = next((c for c in courses_data if c['id'] == course_id), None)
    if not course or not course['prereq']:
        return []
    
    grade_mapping_points = app.config['DATA_CONFIG']['grade_mapping']
    unmet = []
    for pid in course['prereq']:
        prereq_grade_char = course_grades.get(pid, "")
        numeric_grade = None
        try:
            numeric_grade = float(prereq_grade_char)
            if not (0.0 <= numeric_grade <= 4.0):
                numeric_grade = 0.0
        except ValueError:
            numeric_grade = grade_mapping_points.get(str(prereq_grade_char).upper(), 0.0)

        if pid not in course_grades or numeric_grade == 0.0:
            unmet.append(pid)
    return unmet

def find_course_thai_name_backend(course_id, courses_data):
    course = next((c for c in courses_data if c['id'] == course_id), None)
    return course['thaiName'] if course else course_id

def get_loaded_courses_backend(loaded_terms_count, all_terms_data, courses_data, repeated_courses_in_this_term_ids):
    course_ids = []
    for i in range(loaded_terms_count):
        if i < len(all_terms_data):
            course_ids.extend(all_terms_data[i]['ids'])
    course_ids.extend(repeated_courses_in_this_term_ids)
    
    unique_ids = list(set(course_ids))
    return [c for c in courses_data if c['id'] in unique_ids]

def find_blocked_courses_backend(course_grades, loaded_courses, courses_data, passing_grades_list_from_config):
    blocked_courses_ids = []
    grade_mapping_points = app.config['DATA_CONFIG']['grade_mapping']

    for course_obj in loaded_courses:
        course_id = course_obj['id']
        
        current_grade_for_course = course_grades.get(course_id, "")
        
        numeric_current_grade = None
        try:
            numeric_current_grade = float(current_grade_for_course)
            if not (0.0 <= numeric_current_grade <= 4.0):
                numeric_current_grade = 0.0
        except ValueError:
            numeric_current_grade = grade_mapping_points.get(str(current_grade_for_course).upper(), 0.0)

        if numeric_current_grade == 0.0 and current_grade_for_course != "":
            pass
        elif not check_prerequisites_backend(course_id, course_grades, courses_data, passing_grades_list_from_config):
            blocked_courses_ids.append(course_id)
            
    return list(set(blocked_courses_ids))

def build_dependency_graph_backend(courses_subset):
    graph = {c['id']: [] for c in courses_subset}
    for course in courses_subset:
        for prereq_id in course['prereq']:
            if prereq_id in graph:
                graph[prereq_id].append(course['id'])
    return graph

def find_affected_courses_backend(course_id, graph):
    affected = set()
    queue = [course_id]
    while queue:
        current = queue.pop(0)
        dependents = graph.get(current, [])
        for dep in dependents:
            if dep not in affected:
                affected.add(dep)
                queue.append(dep)
    return list(affected)


# ==========================================
# Prerequisite Checking Functions
# ==========================================

def check_prerequisites_for_course(course_id, current_grades, courses_data, grade_mapping):
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ú‡πà‡∏≤‡∏ô prerequisite ‡∏Ç‡∏≠‡∏á‡∏ß‡∏¥‡∏ä‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
    course = next((c for c in courses_data if c['id'] == course_id), None)
    if not course or not course.get('prereq'):
        return True
    
    passing_grades = [g for g, v in grade_mapping.items() if v > 0]
    
    for prereq_id in course['prereq']:
        grade = current_grades.get(prereq_id, '')
        if not grade or grade not in passing_grades:
            return False
    return True

def get_unmet_prerequisites_for_course(course_id, current_grades, courses_data, grade_mapping):
    """‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ prerequisites ‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô"""
    course = next((c for c in courses_data if c['id'] == course_id), None)
    if not course or not course.get('prereq'):
        return []
    
    passing_grades = [g for g, v in grade_mapping.items() if v > 0]
    unmet = []
    
    for prereq_id in course['prereq']:
        grade = current_grades.get(prereq_id, '')
        if not grade or grade not in passing_grades:
            unmet.append(prereq_id)
    return unmet

def can_take_courses_together(course_a_id, course_b_id, current_grades, courses_data, grade_mapping):
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏•‡∏á‡∏™‡∏≠‡∏á‡∏ß‡∏¥‡∏ä‡∏≤‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡∏Å‡∏£‡∏ì‡∏µ‡πÅ‡∏•‡∏õ‡∏Å‡∏±‡∏ö‡∏ó‡∏§‡∏©‡∏é‡∏µ)"""
    course_a = next((c for c in courses_data if c['id'] == course_a_id), None)
    course_b = next((c for c in courses_data if c['id'] == course_b_id), None)
    
    if not course_a or not course_b:
        return False
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏£‡∏ì‡∏µ‡πÅ‡∏•‡∏õ‡∏Å‡∏±‡∏ö‡∏ó‡∏§‡∏©‡∏é‡∏µ
    lab_keywords = ["‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£", "‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô"]
    
    def is_lab_course(course):
        return any(kw in course['thaiName'] for kw in lab_keywords)
    
    a_is_lab = is_lab_course(course_a)
    b_is_lab = is_lab_course(course_b)
    
    # ‡∏ñ‡πâ‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏Ñ‡∏π‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏•‡∏õ‡∏´‡∏£‡∏∑‡∏≠‡∏ó‡∏±‡πâ‡∏á‡∏Ñ‡∏π‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏§‡∏©‡∏é‡∏µ
    if a_is_lab == b_is_lab:
        return False
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÅ‡∏•‡∏õ‡∏°‡∏µ‡∏ó‡∏§‡∏©‡∏é‡∏µ‡πÄ‡∏õ‡πá‡∏ô prerequisite ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    lab_course = course_a if a_is_lab else course_b
    theory_course = course_b if a_is_lab else course_a
    
    passing_grades = [g for g, v in grade_mapping.items() if v > 0]
    
    if theory_course['id'] in lab_course.get('prereq', []):
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ prereq ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏Ç‡∏≠‡∏á‡πÅ‡∏•‡∏õ‡∏ú‡πà‡∏≤‡∏ô‡∏´‡∏°‡∏î‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á
        other_prereqs_passed = True
        for prereq_id in lab_course['prereq']:
            if prereq_id != theory_course['id']:
                grade = current_grades.get(prereq_id, '')
                if not grade or grade not in passing_grades:
                    other_prereqs_passed = False
                    break
        return other_prereqs_passed
    
    return False

def find_all_blocked_courses(current_grades, loaded_courses_ids, courses_data, grade_mapping):
    """‡∏´‡∏≤‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ö‡∏•‡πá‡∏≠‡∏Å‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å prerequisite ‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô"""
    blocked = []
    
    for course_id in loaded_courses_ids:
        # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏Å‡∏£‡∏î‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏Å‡∏£‡∏î F
        current_grade = current_grades.get(course_id, '')
        if not current_grade or current_grade == 'F':
            if not check_prerequisites_for_course(course_id, current_grades, courses_data, grade_mapping):
                unmet = get_unmet_prerequisites_for_course(course_id, current_grades, courses_data, grade_mapping)
                course = next((c for c in courses_data if c['id'] == course_id), None)
                if course:
                    blocked.append({
                        'id': course_id,
                        'name': course['thaiName'],
                        'unmet_prereqs': [
                            next((c['thaiName'] for c in courses_data if c['id'] == pid), pid)
                            for pid in unmet
                        ]
                    })
    
    return blocked

def build_course_dependency_graph(courses_subset_ids, courses_data):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á dependency graph ‡∏Ç‡∏≠‡∏á‡∏ß‡∏¥‡∏ä‡∏≤"""
    graph = {cid: [] for cid in courses_subset_ids}
    
    for course_id in courses_subset_ids:
        course = next((c for c in courses_data if c['id'] == course_id), None)
        if course:
            for prereq_id in course.get('prereq', []):
                if prereq_id in graph:
                    graph[prereq_id].append(course_id)
    
    return graph

def find_courses_affected_by_failure(failed_course_id, dependency_graph):
    """‡∏´‡∏≤‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏à‡∏≤‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏Å"""
    affected = set()
    queue = [failed_course_id]
    
    while queue:
        current = queue.pop(0)
        dependents = dependency_graph.get(current, [])
        for dep in dependents:
            if dep not in affected:
                affected.add(dep)
                queue.append(dep)
    
    return list(affected)


def topological_sort_with_cycle_check_backend(loaded_courses_objects):
    """Performs a topological sort on a subset of courses and checks for cycles."""
    if not loaded_courses_objects:
        return {'order': [], 'cycle': False}

    course_map = {c['id']: c for c in loaded_courses_objects}

    in_degree = {c_id: 0 for c_id in course_map.keys()}
    adj_list = {c_id: [] for c_id in course_map.keys()}

    for course_id, course_obj in course_map.items():
        for prereq_id in course_obj['prereq']:
            if prereq_id in course_map:
                adj_list[prereq_id].append(course_id)
                in_degree[course_id] += 1

    queue = []
    for cid, degree in in_degree.items():
        if degree == 0:
            queue.append(cid)

    order = []
    count = 0

    while queue:
        u = queue.pop(0)
        order.append(u)
        count += 1
        
        for v in adj_list.get(u, []):
            in_degree[v] -= 1
            if in_degree[v] == 0:
                queue.append(v)

    if count != len(loaded_courses_objects):
        return {'order': [], 'cycle': True}
    return {'order': order, 'cycle': False}

def linear_regression_next_term_gpa_backend(term_gpas):
    """Predicts next term's GPA using simple linear regression."""
    if len(term_gpas) == 0:
        return 0.0
    if len(term_gpas) == 1:
        return term_gpas[0]['gpa']

    n = len(term_gpas)
    x_vals = np.array([(i + 1) for i in range(n)])
    y_vals = np.array([t['gpa'] for t in term_gpas])

    if np.all(y_vals == y_vals[0]):
        return y_vals[0]

    sum_x = np.sum(x_vals)
    sum_y = np.sum(y_vals)
    sum_xy = np.sum(x_vals * y_vals)
    sum_xx = np.sum(x_vals * x_vals)

    denominator = (n * sum_xx - sum_x * sum_y)
    if denominator == 0:
        return y_vals[-1]

    slope = (n * sum_xy - sum_x * sum_y) / denominator
    intercept = (sum_y - slope * sum_x) / n

    predicted = slope * (n + 1) + intercept
    return max(0.0, min(4.0, predicted))

def estimate_completion_rate_backend(avg_gpa, completed_credits, total_required_credits, blocked_courses_ids, fail_count, failed_courses_ids, repeat_count_this_term, core_subjects_ids, courses_data):
    """Estimates the completion rate based on academic progress and issues."""
    base_rate = (completed_credits / total_required_credits) * 100 if total_required_credits > 0 else 0

    blocked_credits = sum(
        (next((c for c in courses_data if c['id'] == cid), {'credit': 0})['credit'] for cid in blocked_courses_ids)
    )

    adjusted_rate = base_rate - blocked_credits * app.config['DATA_CONFIG']['risk_levels'].get('credit_penalty_per_blocked_course', 2)

    if 0 < avg_gpa < app.config['DATA_CONFIG']['risk_levels']['warning_gpa_threshold']:
        adjusted_rate -= app.config['DATA_CONFIG']['risk_levels'].get('low_gpa_penalty', 5)
    
    if fail_count >= 5:
        adjusted_rate -= app.config['DATA_CONFIG']['risk_levels'].get('high_fail_count_penalty', 10)
    elif fail_count >= 2:
        adjusted_rate -= app.config['DATA_CONFIG']['risk_levels'].get('medium_fail_count_penalty', 5)
    
    fail_core_count = len([f for f in failed_courses_ids if f in core_subjects_ids])
    adjusted_rate -= fail_core_count * app.config['DATA_CONFIG']['risk_levels'].get('core_fail_penalty', 3)
    
    if repeat_count_this_term >= 3:
        adjusted_rate -= app.config['DATA_CONFIG']['risk_levels'].get('repeat_course_penalty', 5)

    return max(0.0, min(100.0, adjusted_rate))

def determine_graduation_status_backend(completion_rate, avg_gpa, blocked_courses_ids, failed_courses_ids, loaded_courses_objects, current_grades, all_terms_data, courses_data, loaded_terms_count):
    """Determines the student's graduation status based on their progress and potential issues."""
    
    total_terms_in_curriculum = len(all_terms_data)
    is_at_or_past_final_standard_term = (loaded_terms_count >= total_terms_in_curriculum)

    all_courses_in_loaded_curriculum_ids = set()
    for i in range(loaded_terms_count):
        if i < len(all_terms_data):
            for course_id in all_terms_data[i]['ids']:
                all_courses_in_loaded_curriculum_ids.add(course_id)

    incomplete_courses = [
        cid for cid in all_courses_in_loaded_curriculum_ids
        if cid not in current_grades or current_grades[cid] == "" or app.config['DATA_CONFIG']['grade_mapping'].get(str(current_grades[cid]).upper(), 0.0) == 0.0
    ]
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPA ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (‡∏ï‡πâ‡∏≠‡∏á >= 2.00 ‡∏ñ‡∏∂‡∏á‡∏à‡∏∞‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå)
    # ‡∏ñ‡πâ‡∏≤ GPA ‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡πÅ‡∏•‡πâ‡∏ß ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô GPA ‡∏ï‡πà‡∏≥
    gpa_threshold = 2.00
    if avg_gpa > 0 and avg_gpa < gpa_threshold:
        return f"GPA ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏ì‡∏ë‡πå ({avg_gpa:.2f} < {gpa_threshold:.2f}) ‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏£‡πà‡∏á‡∏î‡πà‡∏ß‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤"
    # ‡∏ñ‡πâ‡∏≤ GPA >= 2.00 ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÅ‡∏•‡∏∞‡πÑ‡∏õ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏≠‡∏∑‡πà‡∏ô‡∏ï‡πà‡∏≠

    if is_at_or_past_final_standard_term:
        if len(failed_courses_ids) > 0:
            return "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏Å‡∏≥‡∏´‡∏ô‡∏î. ‡∏¢‡∏±‡∏á‡∏°‡∏µ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ï‡∏Å (‡πÄ‡∏Å‡∏£‡∏î F) ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç"
        if len(blocked_courses_ids) > 0:
            return "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏Å‡∏≥‡∏´‡∏ô‡∏î. ‡∏°‡∏µ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ö‡∏•‡πá‡∏≠‡∏Å (prerequisite ‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô)"
        if len(incomplete_courses) > 0:
            return f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏Å‡∏≥‡∏´‡∏ô‡∏î. ‡∏¢‡∏±‡∏á‡∏°‡∏µ {len(incomplete_courses)} ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô/‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏Å‡∏£‡∏î"
        
        if avg_gpa == 0.0 and (not failed_courses_ids and not incomplete_courses):
            return "‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏Å‡∏≥‡∏´‡∏ô‡∏î 4 ‡∏õ‡∏µ (‡∏ï‡∏≤‡∏°‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç)"
        
        return "‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏Å‡∏≥‡∏´‡∏ô‡∏î 4 ‡∏õ‡∏µ"
    
    else:
        if len(failed_courses_ids) > 0:
            return "‡∏°‡∏µ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ï‡∏Å (‡πÄ‡∏Å‡∏£‡∏î F) ‡∏≠‡∏≤‡∏à‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Å‡∏≤‡∏£‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏•‡πà‡∏≤‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤‡∏Å‡∏≥‡∏´‡∏ô‡∏î"
        if len(blocked_courses_ids) > 0:
            return "‡∏°‡∏µ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ö‡∏•‡πá‡∏≠‡∏Å (prerequisite ‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô) ‡∏Ñ‡∏ß‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ï‡πà‡∏≠‡∏ï‡∏≤‡∏°‡πÅ‡∏ú‡∏ô"
        if len(incomplete_courses) > 0:
            return "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á. ‡∏¢‡∏±‡∏á‡∏°‡∏µ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô/‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡∏£‡∏î‡πÉ‡∏ô‡∏†‡∏≤‡∏Ñ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ"
        
        return "‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏Å‡∏≥‡∏´‡∏ô‡∏î 4 ‡∏õ‡∏µ"

def update_recommendations_backend(failed_courses_ids, avg_gpa, blocked_courses_ids):
    """Generates specific recommendations based on academic issues."""
    recommendations = []
    if len(failed_courses_ids) > 0:
        recommendations.append(app.config['MESSAGES']['recommendations']['high_risk'][0])
        recommendations.append(app.config['MESSAGES']['recommendations']['high_risk'][1])
    if 0 < avg_gpa < app.config['DATA_CONFIG']['risk_levels']['warning_gpa_threshold']:
        recommendations.append(app.config['MESSAGES']['recommendations']['medium_risk'][1])
    if len(blocked_courses_ids) > 0:
        recommendations.append(app.config['MESSAGES']['recommendations']['high_risk'][3])
    if not recommendations:
        recommendations.append(app.config['MESSAGES']['recommendations']['low_risk'][0])
    return list(set(recommendations))

# ===============================
# ‡∏Å‡∏£‡∏≤‡∏ü 3 ‡πÄ‡∏™‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á
# ===============================

def generate_three_line_chart_data(current_grades, loaded_terms_count=8):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏≤‡∏ü 3 ‡πÄ‡∏™‡πâ‡∏ô ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤:
    1. ‡πÄ‡∏™‡πâ‡∏ô‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ (Target Line - ‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß): GPA ‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤
    2. ‡πÄ‡∏™‡πâ‡∏ô‡∏ú‡∏•‡∏à‡∏£‡∏¥‡∏á (Actual Line - ‡∏™‡∏µ‡∏ô‡πâ‡∏≥‡πÄ‡∏á‡∏¥‡∏ô): GPA ‡∏™‡∏∞‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á‡∏ï‡∏≤‡∏°‡πÄ‡∏ó‡∏≠‡∏°
    3. ‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (Prediction Line - ‡∏™‡∏µ‡∏™‡πâ‡∏°): ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏ó‡∏≠‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
    """
    try:
        # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        grade_mapping = app.config.get('DATA_CONFIG', {}).get('grade_mapping', {
            'A': 4.0, 'B+': 3.5, 'B': 3.0, 'C+': 2.5, 'C': 2.0, 
            'D+': 1.5, 'D': 1.0, 'F': 0.0, 'W': 0.0, 'I': 0.0
        })
        courses_data = app.config.get('COURSES_DATA', [])
        all_terms_data = app.config.get('ALL_TERMS_DATA', [])
        
        # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ó‡∏≠‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£ (8 ‡πÄ‡∏ó‡∏≠‡∏°‡∏õ‡∏Å‡∏ï‡∏¥)
        total_terms = len(all_terms_data)
        
        # === 1. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏™‡πâ‡∏ô‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ (Target Line) ===
        # GPA ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ = 3.0 (‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤)
        target_gpa = 3.0
        target_line = [target_gpa] * (total_terms + 2)  # +2 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï
        
        # === 2. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏™‡πâ‡∏ô‡∏ú‡∏•‡∏à‡∏£‡∏¥‡∏á (Actual Line) - ‡∏à‡∏≤‡∏Å GPA ‡∏™‡∏∞‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á‡∏ï‡∏≤‡∏°‡πÄ‡∏ó‡∏≠‡∏° ===
        actual_line = []
        term_gpa_list = []
        
        # ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ß‡∏¥‡∏ä‡∏≤‡∏ï‡∏≤‡∏°‡πÄ‡∏ó‡∏≠‡∏°
        courses_by_term = {}
        for term_idx, term_data in enumerate(all_terms_data):
            if term_idx >= loaded_terms_count:
                break
            courses_by_term[term_idx] = term_data['ids']
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA ‡∏™‡∏∞‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á‡∏ï‡∏≤‡∏°‡πÄ‡∏ó‡∏≠‡∏°
        cumulative_points = 0
        cumulative_credits = 0
        
        for term_idx in range(loaded_terms_count):
            term_courses = courses_by_term.get(term_idx, [])
            term_points = 0
            term_credits = 0
            
            for course_id in term_courses:
                grade = current_grades.get(course_id, '')
                if grade and grade in grade_mapping:
                    # ‡∏´‡∏≤‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á‡∏ß‡∏¥‡∏ä‡∏≤
                    course = next((c for c in courses_data if c['id'] == course_id), None)
                    credit = course['credit'] if course else 3
                    grade_point = grade_mapping[grade]
                    
                    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà W, I
                    if grade not in ['W', 'I']:
                        term_points += grade_point * credit
                        term_credits += credit
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏∞‡∏™‡∏°
            cumulative_points += term_points
            cumulative_credits += term_credits
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA ‡∏™‡∏∞‡∏™‡∏°‡∏ì ‡∏à‡∏∏‡∏î‡∏ô‡∏µ‡πâ
            term_cumulative_gpa = cumulative_points / cumulative_credits if cumulative_credits > 0 else 0
            actual_line.append(round(term_cumulative_gpa, 2))
            term_gpa_list.append(round(term_cumulative_gpa, 2))
        
        # ‡πÄ‡∏ï‡∏¥‡∏° None ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ó‡∏≠‡∏°‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô
        for term_idx in range(loaded_terms_count, total_terms):
            actual_line.append(None)
        
        # === 3. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (Prediction Line) ===
        prediction_line = actual_line.copy()
        
        # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ GPA ‡πÄ‡∏ó‡∏≠‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
        if len(term_gpa_list) > 0:
            current_gpa = term_gpa_list[-1]
            
            # ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• AI ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
            if hasattr(app, 'advanced_trainer') and app.advanced_trainer:
                try:
                    # ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• AI ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏à‡∏£‡∏¥‡∏á
                    prediction_result = app.advanced_trainer.predict_graduation(current_grades)
                    predicted_gpa_next_term = prediction_result.get('predicted_gpa', current_gpa)
                    
                    # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡πâ‡∏™‡∏°‡πÄ‡∏´‡∏ï‡∏∏‡∏™‡∏°‡∏ú‡∏•
                    predicted_gpa_next_term = max(0, min(4.0, predicted_gpa_next_term))
                except Exception as e:
                    logger.warning(f"AI prediction failed, using statistical method: {e}")
                    # Fallback: ‡πÉ‡∏ä‡πâ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏ó‡∏≤‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
                    if len(term_gpa_list) >= 2:
                        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì trend ‡∏à‡∏≤‡∏Å 2 ‡πÄ‡∏ó‡∏≠‡∏°‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
                        trend = term_gpa_list[-1] - term_gpa_list[-2]
                        predicted_gpa_next_term = current_gpa + (trend * 0.5)  # ‡∏•‡∏î impact ‡∏Ç‡∏≠‡∏á trend
                    else:
                        predicted_gpa_next_term = current_gpa
            else:
                # ‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏≤‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
                if len(term_gpa_list) >= 2:
                    trend = term_gpa_list[-1] - term_gpa_list[-2]
                    predicted_gpa_next_term = current_gpa + (trend * 0.5)
                else:
                    predicted_gpa_next_term = current_gpa
            
            # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡πà‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ
            predicted_gpa_next_term = max(1.5, min(4.0, predicted_gpa_next_term))
            
            # ‡πÄ‡∏ï‡∏¥‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ó‡∏≠‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠
            for i in range(len(prediction_line)):
                if prediction_line[i] is None:
                    # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ß‡πà‡∏≤ GPA ‡∏à‡∏∞‡∏Ñ‡πà‡∏≠‡∏¢‡πÜ ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏á‡∏ó‡∏µ‡πà
                    if predicted_gpa_next_term > current_gpa:
                        improvement_rate = (predicted_gpa_next_term - current_gpa) * 0.8
                        prediction_line[i] = round(min(4.0, current_gpa + improvement_rate), 2)
                    else:
                        prediction_line[i] = round(predicted_gpa_next_term, 2)
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï 2 ‡πÄ‡∏ó‡∏≠‡∏°
            prediction_line.extend([round(predicted_gpa_next_term, 2)] * 2)
        else:
            # ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÉ‡∏™‡πà‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
            prediction_line = [None] * total_terms
            prediction_line.extend([None, None])
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏õ‡πâ‡∏≤‡∏¢‡∏Å‡∏≥‡∏Å‡∏±‡∏ö‡πÄ‡∏ó‡∏≠‡∏°
        term_labels = []
        for i, term_data in enumerate(all_terms_data):
            term_labels.append(f"‡∏õ‡∏µ {term_data['year']} ‡πÄ‡∏ó‡∏≠‡∏° {term_data['term']}")
        term_labels.extend(["‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ +1", "‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ +2"])
        
        # ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
        summary = {
            'current_gpa': term_gpa_list[-1] if term_gpa_list else 0,
            'predicted_next_gpa': round(predicted_gpa_next_term, 2) if term_gpa_list else 0,
            'target_gpa': target_gpa,
            'terms_completed': loaded_terms_count,
            'total_terms': total_terms,
            'on_track': (term_gpa_list[-1] >= target_gpa) if term_gpa_list else False
        }
        
        return {
            'terms': term_labels,
            'target_line': target_line,
            'actual_line': actual_line + [None, None],  # ‡πÄ‡∏û‡∏¥‡πà‡∏° None ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
            'prediction_line': prediction_line,
            'colors': {
                'target': '#28a745',      # ‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß
                'actual': '#007bff',      # ‡∏™‡∏µ‡∏ô‡πâ‡∏≥‡πÄ‡∏á‡∏¥‡∏ô
                'prediction': '#fd7e14'   # ‡∏™‡∏µ‡∏™‡πâ‡∏°
            },
            'summary': summary
        }
        
    except Exception as e:
        logger.error(f"Error generating three line chart data: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return {
            'terms': [],
            'target_line': [],
            'actual_line': [],
            'prediction_line': [],
            'colors': {
                'target': '#28a745',
                'actual': '#007bff',
                'prediction': '#fd7e14'
            },
            'summary': {
                'current_gpa': 0,
                'predicted_next_gpa': 0,
                'target_gpa': 3.0,
                'terms_completed': 0,
                'total_terms': 8,
                'on_track': False
            }
        }

def analyze_graduation_failure_reasons(current_grades, loaded_terms_count=8, prediction_result=None):
    """
    ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°
    ‡πÉ‡∏ä‡πâ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• AI (prediction_result) ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç
    
    Args:
        current_grades: dict ‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡∏£‡∏î‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô {course_id: grade}
        loaded_terms_count: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ó‡∏≠‡∏°‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î
        prediction_result: ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• AI (dict) ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢:
            - prediction: '‡∏à‡∏ö' ‡∏´‡∏£‡∏∑‡∏≠ '‡πÑ‡∏°‡πà‡∏à‡∏ö'
            - prob_pass: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏à‡∏ö (0-1)
            - prob_fail: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏à‡∏ö (0-1)
            - confidence: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (0-1)
            - risk_level: ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á
            - method: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
    """
    try:
        grade_mapping = app.config.get('DATA_CONFIG', {}).get('grade_mapping', {})
        courses_data_list = app.config.get('COURSES_DATA', [])
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á dict ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö lookup ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏à‡∏≤‡∏Å course id
        courses_credit_map = {course['id']: course.get('credit', 3) for course in courses_data_list}
        
        reasons = []
        risk_factors = []
        
        # Helper function ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏∂‡∏á‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï
        def get_course_credit(course_id):
            return courses_credit_map.get(course_id, 3)
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        total_courses = len(current_grades)
        failed_courses = [course for course, grade in current_grades.items() 
                         if grade and grade_mapping.get(str(grade).upper(), 0) == 0 and str(grade).upper() not in ['W', 'WF', 'WU', 'I']]
        low_grade_courses = [course for course, grade in current_grades.items() 
                           if grade and 0 < grade_mapping.get(str(grade).upper(), 0) < 2.0]
        incomplete_courses = [course for course, grade in current_grades.items() 
                            if grade and str(grade).upper() in ['I', 'W', 'WF', 'WU']]
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA ‡πÅ‡∏•‡∏∞‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï (‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏° S, W, WF, WU)
        total_points = 0
        total_credits = 0
        passed_credits = 0
        
        # ‡πÄ‡∏Å‡∏£‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ô‡∏≥‡∏°‡∏≤‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA
        excluded_grades = {'S', 'W', 'WF', 'WU', 'I', None}
        
        for course, grade in current_grades.items():
            if not grade or str(grade).upper() in excluded_grades:
                continue
                
            grade_upper = str(grade).upper()
            grade_point = grade_mapping.get(grade_upper)
            
            # ‡∏ñ‡πâ‡∏≤ grade_point ‡πÄ‡∏õ‡πá‡∏ô None (‡πÄ‡∏ä‡πà‡∏ô S) ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°
            if grade_point is None:
                continue
            
            # ‡∏î‡∏∂‡∏á‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£
            credits = get_course_credit(course)
            
            # ‡∏£‡∏ß‡∏°‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏Å‡∏£‡∏î‡∏ó‡∏µ‡πà‡∏ô‡∏≥‡∏°‡∏≤‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA ‡πÑ‡∏î‡πâ
            if grade_point is not None:
                total_points += grade_point * credits
                total_credits += credits
                if grade_point > 0:
                    passed_credits += credits
        
        current_gpa = total_points / total_credits if total_credits > 0 else 0
        current_gpa = round(current_gpa, 2)  # ‡∏õ‡∏±‡∏î‡πÄ‡∏õ‡πá‡∏ô 2 ‡∏ó‡∏®‡∏ô‡∏¥‡∏¢‡∏°
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏à‡∏ö‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
        
        # 1. ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô (‡πÄ‡∏Å‡∏£‡∏î F)
        if len(failed_courses) > 0:
            failed_credits = sum([get_course_credit(course) for course in failed_courses])
            reasons.append({
                'type': 'critical',
                'title': f'‡∏°‡∏µ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô {len(failed_courses)} ‡∏ß‡∏¥‡∏ä‡∏≤ ({failed_credits} ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï)',
                'description': f'‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏Å‡∏£‡∏î F: {", ".join(failed_courses[:3])}{"..." if len(failed_courses) > 3 else ""}',
                'courses': failed_courses,
                'impact': '‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å - ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ã‡πâ‡∏≥‡∏Å‡πà‡∏≠‡∏ô‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤',
                'solution': '‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ã‡πâ‡∏≥‡πÉ‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏Å‡∏ó‡∏±‡∏ô‡∏ó‡∏µ ‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡πâ‡∏î‡∏µ‡∏Å‡πà‡∏≠‡∏ô‡∏™‡∏≠‡∏ö',
                'timeline': f'‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏° {math.ceil(len(failed_courses)/6)} ‡πÄ‡∏ó‡∏≠‡∏°'
            })
        
        # 2. GPA ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏ì‡∏ë‡πå
        if current_gpa < 2.0:
            gpa_deficit = 2.0 - current_gpa
            credits_needed = math.ceil(gpa_deficit * total_credits / 2.0)
            reasons.append({
                'type': 'critical',
                'title': f'GPA ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥ ({current_gpa:.2f} < 2.00)',
                'description': f'‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á GPA ‡∏≠‡∏µ‡∏Å {gpa_deficit:.2f} ‡∏à‡∏∏‡∏î',
                'impact': '‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å - ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÑ‡∏î‡πâ',
                'solution': f'‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏î‡πâ‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ A ‡πÉ‡∏ô {credits_needed} ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏ñ‡∏±‡∏î‡πÑ‡∏õ',
                'timeline': f'‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏° {math.ceil(credits_needed/18)} ‡πÄ‡∏ó‡∏≠‡∏°'
            })
        
        # 3. ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏Å‡∏£‡∏î‡∏ï‡πà‡∏≥
        if len(low_grade_courses) > 0:
            low_credits = sum([get_course_credit(course) for course in low_grade_courses])
            reasons.append({
                'type': 'warning',
                'title': f'‡∏°‡∏µ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏Å‡∏£‡∏î‡∏ï‡πà‡∏≥ {len(low_grade_courses)} ‡∏ß‡∏¥‡∏ä‡∏≤ ({low_credits} ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï)',
                'description': f'‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏Å‡∏£‡∏î D+, D: {", ".join(low_grade_courses[:3])}{"..." if len(low_grade_courses) > 3 else ""}',
                'courses': low_grade_courses,
                'impact': '‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á - ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏ï‡πà‡∏≠ GPA ‡∏£‡∏ß‡∏°',
                'solution': '‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ã‡πâ‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÄ‡∏Å‡∏£‡∏î‡πÅ‡∏•‡∏∞ GPA',
                'timeline': '‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡πÄ‡∏ó‡∏≠‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ'
            })
        
        # 4. ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
        if len(incomplete_courses) > 0:
            incomplete_credits = sum([get_course_credit(course) for course in incomplete_courses])
            reasons.append({
                'type': 'warning',
                'title': f'‡∏°‡∏µ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå {len(incomplete_courses)} ‡∏ß‡∏¥‡∏ä‡∏≤',
                'description': f'‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏Å‡∏£‡∏î I, W, WF, WU: {", ".join(incomplete_courses)}',
                'courses': incomplete_courses,
                'impact': '‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á - ‡πÑ‡∏°‡πà‡∏ô‡∏±‡∏ö‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï',
                'solution': '‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô',
                'timeline': '‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î'
            })
        
        # 5. ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ä‡πâ‡∏≤
        expected_credits = loaded_terms_count * 18  # ‡∏™‡∏°‡∏°‡∏ï‡∏¥ 18 ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏ï‡πà‡∏≠‡πÄ‡∏ó‡∏≠‡∏°
        if passed_credits < expected_credits * 0.8:
            reasons.append({
                'type': 'warning',
                'title': '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏ì‡∏ë‡πå',
                'description': f'‡∏ú‡πà‡∏≤‡∏ô {passed_credits}/{expected_credits} ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï ({passed_credits/expected_credits*100:.1f}%)',
                'impact': '‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á - ‡∏≠‡∏≤‡∏à‡∏à‡∏ö‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤‡πÅ‡∏ú‡∏ô',
                'solution': '‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡πÉ‡∏ô‡πÄ‡∏ó‡∏≠‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ',
                'timeline': f'‡∏≠‡∏≤‡∏à‡∏à‡∏ö‡∏ä‡πâ‡∏≤ {math.ceil((expected_credits-passed_credits)/18)} ‡πÄ‡∏ó‡∏≠‡∏°'
            })
        
        # ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
        risk_score = 0
        if len(failed_courses) > 0:
            risk_score += len(failed_courses) * 20
        if current_gpa < 2.0:
            risk_score += (2.0 - current_gpa) * 50
        if len(low_grade_courses) > 0:
            risk_score += len(low_grade_courses) * 5
        if len(incomplete_courses) > 0:
            risk_score += len(incomplete_courses) * 10
        
        if risk_score >= 80:
            risk_level = '‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å'
            risk_color = 'danger'
            risk_description = '‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏à‡∏ö'
        elif risk_score >= 60:
            risk_level = '‡∏™‡∏π‡∏á'
            risk_color = 'danger'
            risk_description = '‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏™‡∏π‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏à‡∏ö‡∏ä‡πâ‡∏≤'
        elif risk_score >= 40:
            risk_level = '‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á'
            risk_color = 'warning'
            risk_description = '‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á'
        elif risk_score >= 20:
            risk_level = '‡∏ï‡πà‡∏≥-‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á'
            risk_color = 'info'
            risk_description = '‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢'
        else:
            risk_level = '‡∏ï‡πà‡∏≥'
            risk_color = 'success'
            risk_description = '‡∏°‡∏µ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏à‡∏ö‡∏ï‡∏≤‡∏°‡πÅ‡∏ú‡∏ô'
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏à‡∏ö‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
        base_probability = 90
        if len(failed_courses) > 0:
            base_probability -= len(failed_courses) * 15
        if current_gpa < 2.0:
            base_probability -= (2.0 - current_gpa) * 30
        if len(low_grade_courses) > 0:
            base_probability -= len(low_grade_courses) * 3
        if len(incomplete_courses) > 0:
            base_probability -= len(incomplete_courses) * 5
        
        graduation_probability = max(5, min(95, base_probability))
        
        # ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏à‡∏ö
        if graduation_probability >= 80:
            graduation_status = '‡∏°‡∏µ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏à‡∏ö‡∏™‡∏π‡∏á'
            graduation_status_color = 'success'
        elif graduation_probability >= 60:
            graduation_status = '‡∏°‡∏µ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏à‡∏ö‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á'
            graduation_status_color = 'info'
        elif graduation_probability >= 40:
            graduation_status = '‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏à‡∏ö'
            graduation_status_color = 'warning'
        else:
            graduation_status = '‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏™‡∏π‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏à‡∏ö'
            graduation_status_color = 'danger'
        
        # ====== ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏à‡∏ö/‡πÑ‡∏°‡πà‡∏à‡∏ö: ‡πÉ‡∏ä‡πâ‡∏ú‡∏•‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• AI ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å ======
        # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏ú‡∏•‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• AI ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ú‡∏•‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å
        if prediction_result and isinstance(prediction_result, dict):
            # ‡πÉ‡∏ä‡πâ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• AI
            model_prediction = prediction_result.get('prediction', '')
            model_prob_pass = prediction_result.get('prob_pass', 0.5)
            model_confidence = prediction_result.get('confidence', 0.5)
            prediction_method = prediction_result.get('method', 'Unknown')
            
            # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î will_graduate ‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•
            will_graduate = model_prediction == '‡∏à‡∏ö' or model_prob_pass >= 0.5
            
            # ‡∏õ‡∏£‡∏±‡∏ö graduation_probability ‡πÉ‡∏´‡πâ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏ú‡∏•‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
            graduation_probability = model_prob_pass * 100
            
            logger.info(f"üìä Using AI Model prediction: {model_prediction} (prob: {model_prob_pass:.2f}, confidence: {model_confidence:.2f}) from {prediction_method}")
        else:
            # Fallback: ‡πÉ‡∏ä‡πâ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
            will_graduate = graduation_probability >= 50 and current_gpa >= 2.0 and len(failed_courses) == 0
            prediction_method = 'Rule-based Analysis'
            model_prob_pass = graduation_probability / 100
            model_confidence = 0.5
            logger.info(f"üìä Using rule-based prediction (no AI model result)")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤
        reasons_for_graduation = []
        if current_gpa >= 2.0:
            reasons_for_graduation.append({
                'icon': '‚úÖ',
                'title': f'GPA ‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå',
                'description': f'GPA ‡∏™‡∏∞‡∏™‡∏° {current_gpa:.2f} ‚â• 2.00 (‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥)',
                'type': 'success'
            })
        if len(failed_courses) == 0:
            reasons_for_graduation.append({
                'icon': '‚úÖ',
                'title': '‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏Å',
                'description': '‡∏ú‡πà‡∏≤‡∏ô‡∏ó‡∏∏‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô',
                'type': 'success'
            })
        if current_gpa >= 2.5:
            reasons_for_graduation.append({
                'icon': 'üåü',
                'title': '‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏î‡∏µ',
                'description': f'GPA {current_gpa:.2f} ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏î‡∏µ',
                'type': 'success'
            })
        if current_gpa >= 3.0:
            reasons_for_graduation.append({
                'icon': 'üèÜ',
                'title': '‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏î‡∏µ‡∏°‡∏≤‡∏Å',
                'description': f'GPA {current_gpa:.2f} ‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢',
                'type': 'success'
            })
        if passed_credits >= 100:
            reasons_for_graduation.append({
                'icon': 'üìö',
                'title': '‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡πÉ‡∏Å‡∏•‡πâ‡∏Ñ‡∏£‡∏ö',
                'description': f'‡∏™‡∏∞‡∏™‡∏°‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡πÅ‡∏•‡πâ‡∏ß {passed_credits} ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï',
                'type': 'success'
            })
        progress_pct = round(passed_credits/136*100, 1) if passed_credits > 0 else 0
        if progress_pct >= 75:
            reasons_for_graduation.append({
                'icon': 'üìà',
                'title': '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏î‡∏µ',
                'description': f'‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ú‡πà‡∏≤‡∏ô‡πÅ‡∏•‡πâ‡∏ß {progress_pct}% ‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£',
                'type': 'success'
            })
        if len(low_grade_courses) == 0 and total_courses > 0:
            reasons_for_graduation.append({
                'icon': 'üí™',
                'title': '‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏î‡∏µ',
                'description': '‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏Å‡∏£‡∏î‡∏ï‡πà‡∏≥ (D+, D)',
                'type': 'success'
            })
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤
        reasons_for_not_graduation = []
        if current_gpa < 2.0:
            gpa_deficit = 2.0 - current_gpa
            reasons_for_not_graduation.append({
                'icon': '‚ùå',
                'title': f'GPA ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏ì‡∏ë‡πå',
                'description': f'GPA ‡∏™‡∏∞‡∏™‡∏° {current_gpa:.2f} < 2.00 (‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏≠‡∏µ‡∏Å {gpa_deficit:.2f})',
                'type': 'critical'
            })
        if len(failed_courses) > 0:
            failed_names = ', '.join(failed_courses[:3])
            if len(failed_courses) > 3:
                failed_names += f' ‡πÅ‡∏•‡∏∞‡∏≠‡∏µ‡∏Å {len(failed_courses)-3} ‡∏ß‡∏¥‡∏ä‡∏≤'
            reasons_for_not_graduation.append({
                'icon': '‚ùå',
                'title': f'‡∏°‡∏µ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏Å {len(failed_courses)} ‡∏ß‡∏¥‡∏ä‡∏≤',
                'description': f'‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô: {failed_names}',
                'type': 'critical',
                'courses': failed_courses
            })
        if len(low_grade_courses) > 2:
            reasons_for_not_graduation.append({
                'icon': '‚ö†Ô∏è',
                'title': f'‡∏°‡∏µ‡∏ß‡∏¥‡∏ä‡∏≤‡πÄ‡∏Å‡∏£‡∏î‡∏ï‡πà‡∏≥‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å',
                'description': f'‡πÑ‡∏î‡πâ‡πÄ‡∏Å‡∏£‡∏î D+, D ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô {len(low_grade_courses)} ‡∏ß‡∏¥‡∏ä‡∏≤ ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏ï‡πà‡∏≠ GPA',
                'type': 'warning',
                'courses': low_grade_courses
            })
        if len(incomplete_courses) > 0:
            reasons_for_not_graduation.append({
                'icon': '‚ö†Ô∏è',
                'title': f'‡∏°‡∏µ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå',
                'description': f'‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ I/W/WF/WU: {len(incomplete_courses)} ‡∏ß‡∏¥‡∏ä‡∏≤',
                'type': 'warning',
                'courses': incomplete_courses
            })
        if progress_pct < 50 and loaded_terms_count >= 4:
            reasons_for_not_graduation.append({
                'icon': '‚ö†Ô∏è',
                'title': '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ä‡πâ‡∏≤',
                'description': f'‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏û‡∏µ‡∏¢‡∏á {progress_pct}% ‡∏≠‡∏≤‡∏à‡∏à‡∏ö‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤‡πÅ‡∏ú‡∏ô',
                'type': 'warning'
            })
        
        # ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏à‡∏ö - ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• AI
        if will_graduate:
            graduation_prediction_text = 'üéì ‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå'
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
            detail_parts = []
            if prediction_result and isinstance(prediction_result, dict):
                detail_parts.append(f'AI Model ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢: ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏à‡∏ö {graduation_probability:.0f}%')
                detail_parts.append(f'‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à: {model_confidence*100:.0f}%')
                if prediction_method:
                    detail_parts.append(f'‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢: {prediction_method}')
            
            if current_gpa >= 2.0:
                detail_parts.append(f'GPA {current_gpa:.2f} ‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥')
            if len(failed_courses) == 0:
                detail_parts.append('‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏Å')
            
            graduation_prediction_detail = ' | '.join(detail_parts) if detail_parts else f'‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤ {graduation_probability:.0f}%'
        else:
            graduation_prediction_text = '‚ö†Ô∏è ‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡πÑ‡∏°‡πà‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤'
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏û‡∏ö
            detail_parts = []
            if prediction_result and isinstance(prediction_result, dict):
                detail_parts.append(f'AI Model ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢: ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏à‡∏ö {graduation_probability:.0f}%')
                detail_parts.append(f'‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à: {model_confidence*100:.0f}%')
            
            main_issues = []
            if current_gpa < 2.0:
                main_issues.append(f'GPA ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏ì‡∏ë‡πå ({current_gpa:.2f} < 2.00)')
            if len(failed_courses) > 0:
                main_issues.append(f'‡∏°‡∏µ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ï‡∏Å {len(failed_courses)} ‡∏ß‡∏¥‡∏ä‡∏≤')
            if len(main_issues) == 0 and graduation_probability < 50:
                main_issues.append(f'‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏à‡∏ö‡∏ï‡πà‡∏≥‡∏ï‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á AI ({graduation_probability:.0f}%)')
            
            if main_issues:
                detail_parts.append('‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡∏´‡∏•‡∏±‡∏Å: ' + ', '.join(main_issues))
            
            graduation_prediction_detail = ' | '.join(detail_parts) if detail_parts else '‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô'
        
        return {
            'reasons': reasons,
            'risk_level': risk_level,
            'risk_color': risk_color,
            'risk_description': risk_description,
            'risk_score': risk_score,
            'graduation_probability': round(graduation_probability, 1),
            'graduation_status': graduation_status,
            'graduation_status_color': graduation_status_color,
            'current_gpa': round(current_gpa, 2),
            'failed_courses_count': len(failed_courses),
            'low_grade_courses_count': len(low_grade_courses),
            'incomplete_courses_count': len(incomplete_courses),
            'total_courses': total_courses,
            'passed_credits': passed_credits,
            'total_credits': total_credits,
            'expected_credits': expected_credits,
            'progress_percentage': round(passed_credits/136*100, 1) if passed_credits > 0 else 0,  # 136 ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏£‡∏ß‡∏°
            'recommendations': generate_improvement_recommendations(current_gpa, failed_courses, low_grade_courses, incomplete_courses),
            # ====== ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏à‡∏ö/‡πÑ‡∏°‡πà‡∏à‡∏ö (‡πÉ‡∏ä‡πâ AI Model ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å) ======
            'will_graduate': will_graduate,
            'graduation_prediction_text': graduation_prediction_text,
            'graduation_prediction_detail': graduation_prediction_detail,
            'reasons_for_graduation': reasons_for_graduation,
            'reasons_for_not_graduation': reasons_for_not_graduation,
            'failed_courses': failed_courses,
            'low_grade_courses': low_grade_courses,
            'incomplete_courses': incomplete_courses,
            # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• AI
            'prediction_method': prediction_method if 'prediction_method' in dir() else 'Rule-based',
            'model_prob_pass': model_prob_pass if 'model_prob_pass' in dir() else graduation_probability / 100,
            'model_confidence': model_confidence if 'model_confidence' in dir() else 0.5,
            'ai_prediction_used': prediction_result is not None
        }
        
    except Exception as e:
        logger.error(f"Error analyzing graduation failure reasons: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return {
            'reasons': [],
            'risk_level': '‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö',
            'risk_color': 'secondary',
            'risk_description': '‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÑ‡∏î‡πâ',
            'risk_score': 0,
            'graduation_probability': 0,
            'graduation_status': '‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö',
            'graduation_status_color': 'secondary',
            'current_gpa': 0,
            'failed_courses_count': 0,
            'low_grade_courses_count': 0,
            'incomplete_courses_count': 0,
            'total_courses': 0,
            'passed_credits': 0,
            'total_credits': 0,
            'expected_credits': 0,
            'progress_percentage': 0,
            'recommendations': [],
            # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (fallback)
            'will_graduate': False,
            'graduation_prediction_text': '‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÑ‡∏î‡πâ',
            'graduation_prediction_detail': '‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå',
            'reasons_for_graduation': [],
            'reasons_for_not_graduation': [],
            'failed_courses': [],
            'low_grade_courses': [],
            'incomplete_courses': [],
            'prediction_method': 'Error',
            'model_prob_pass': 0,
            'model_confidence': 0,
            'ai_prediction_used': False
        }

def generate_improvement_recommendations(current_gpa, failed_courses, low_grade_courses, incomplete_courses=None):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°"""
    if incomplete_courses is None:
        incomplete_courses = []
    
    recommendations = []
    
    # ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏£‡πà‡∏á‡∏î‡πà‡∏ß‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏Å
    if len(failed_courses) > 0:
        recommendations.extend([
            "üî¥ ‡πÄ‡∏£‡πà‡∏á‡∏î‡πà‡∏ß‡∏ô: ‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ã‡πâ‡∏≥‡πÉ‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏Å‡∏ó‡∏±‡∏ô‡∏ó‡∏µ",
            "üìö ‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ã‡πâ‡∏≥",
            "üë®‚Äçüè´ ‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‡∏ú‡∏π‡πâ‡∏™‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ç‡∏≠‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥",
            "üìù ‡∏ó‡∏≥‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î‡πÅ‡∏•‡∏∞‡∏™‡∏≠‡∏ö‡πÄ‡∏Å‡πà‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ï‡∏±‡∏ß",
            "ü§ù ‡∏´‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πà‡∏á‡∏°‡∏≤‡∏ä‡πà‡∏ß‡∏¢‡∏™‡∏≠‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏¥‡∏ß‡πÉ‡∏´‡πâ"
        ])
    
    # ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GPA ‡∏ï‡πà‡∏≥
    if current_gpa < 2.0:
        recommendations.extend([
            "‚ö†Ô∏è ‡πÄ‡∏£‡πà‡∏á‡∏î‡πà‡∏ß‡∏ô: ‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á GPA ‡πÉ‡∏´‡πâ‡∏ñ‡∏∂‡∏á 2.00",
            "üìä ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏Å‡∏£‡∏î‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ñ‡∏±‡∏î‡πÑ‡∏õ",
            "üéØ ‡πÄ‡∏ô‡πâ‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏™‡∏π‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏±‡∏ö GPA",
            "üìà ‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÄ‡∏Å‡∏£‡∏î‡∏î‡∏µ",
            "‚è∞ ‡∏à‡∏±‡∏î‡∏™‡∏£‡∏£‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÉ‡∏´‡πâ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô"
        ])
    elif current_gpa < 2.5:
        recommendations.extend([
            "üìà ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á GPA ‡πÉ‡∏´‡πâ‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô",
            "üéØ ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ"
        ])
    
    # ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏Å‡∏£‡∏î‡∏ï‡πà‡∏≥
    if len(low_grade_courses) > 0:
        recommendations.extend([
            "üîÑ ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ã‡πâ‡∏≥‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏Å‡∏£‡∏î‡∏ï‡πà‡∏≥",
            "üìà ‡πÉ‡∏ä‡πâ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô",
            "üí° ‡∏´‡∏≤‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á"
        ])
    
    # ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
    if len(incomplete_courses) > 0:
        recommendations.extend([
            "üìã ‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå",
            "‚è±Ô∏è ‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î",
            "üìû ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏°‡πà‡∏≥‡πÄ‡∏™‡∏°‡∏≠"
        ])
    
    # ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏ï‡∏≤‡∏° GPA
    if current_gpa >= 3.5:
        recommendations.extend([
            "üåü ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ï‡πà‡∏≠‡πÑ‡∏õ",
            "üéì ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ï‡∏±‡∏ß‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤",
            "üíº ‡∏´‡∏≤‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏ä‡πà‡∏ô internship"
        ])
    elif current_gpa >= 3.0:
        recommendations.extend([
            "üìö ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô",
            "üéØ ‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ GPA 3.5 ‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ"
        ])
    
    # ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
    recommendations.extend([
        "‚è∞ ‡∏à‡∏±‡∏î‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÅ‡∏•‡∏∞‡∏ó‡∏ö‡∏ó‡∏ß‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏°‡πà‡∏≥‡πÄ‡∏™‡∏°‡∏≠",
        "ü§ù ‡∏´‡∏≤‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ä‡πà‡∏ß‡∏¢‡∏™‡∏≠‡∏ô",
        "üí™ ‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏£‡∏∞‡∏¢‡∏∞‡∏™‡∏±‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß",
        "üè• ‡∏î‡∏π‡πÅ‡∏•‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏Å‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡πÉ‡∏à‡πÉ‡∏´‡πâ‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏£‡∏á",
        "üì± ‡πÉ‡∏ä‡πâ‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏ß‡∏•‡∏≤‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô",
        "üéØ ‡∏°‡∏∏‡πà‡∏á‡πÄ‡∏ô‡πâ‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏ú‡∏•‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏à‡∏ö",
        "üìû ‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤"
    ])
    
    return recommendations

def generate_next_term_grade_prediction_table(current_grades):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏ó‡∏≠‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ"""
    try:
        # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÉ‡∏ô‡πÄ‡∏ó‡∏≠‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ (‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á)
        next_term_courses = [
            {"id": "CS301", "name": "‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", "credits": 3, "difficulty": "‡∏™‡∏π‡∏á"},
            {"id": "CS302", "name": "‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°", "credits": 3, "difficulty": "‡∏™‡∏π‡∏á"},
            {"id": "CS303", "name": "‡∏£‡∏∞‡∏ö‡∏ö‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", "credits": 3, "difficulty": "‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á"},
            {"id": "CS304", "name": "‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÄ‡∏ß‡πá‡∏ö‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô", "credits": 3, "difficulty": "‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á"},
            {"id": "GE401", "name": "‡∏ß‡∏¥‡∏ä‡∏≤‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ", "credits": 3, "difficulty": "‡∏ï‡πà‡∏≥"}
        ]
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
        grade_mapping = app.config.get('DATA_CONFIG', {}).get('grade_mapping', {})
        total_points = 0
        total_credits = 0
        
        for course, grade in current_grades.items():
            if grade and grade in grade_mapping:
                grade_point = grade_mapping[grade]
                credits = 3
                total_points += grade_point * credits
                total_credits += credits
        
        current_gpa = total_points / total_credits if total_credits > 0 else 0
        
        # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏Å‡∏£‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ß‡∏¥‡∏ä‡∏≤
        predictions = []
        for course in next_term_courses:
            # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å‡πÅ‡∏•‡∏∞ GPA ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
            if course["difficulty"] == "‡∏™‡∏π‡∏á":
                if current_gpa >= 3.5:
                    predicted_grade = "B+"
                    confidence = 75
                elif current_gpa >= 3.0:
                    predicted_grade = "B"
                    confidence = 70
                elif current_gpa >= 2.5:
                    predicted_grade = "C+"
                    confidence = 65
                else:
                    predicted_grade = "C"
                    confidence = 60
            elif course["difficulty"] == "‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á":
                if current_gpa >= 3.5:
                    predicted_grade = "A"
                    confidence = 80
                elif current_gpa >= 3.0:
                    predicted_grade = "B+"
                    confidence = 75
                elif current_gpa >= 2.5:
                    predicted_grade = "B"
                    confidence = 70
                else:
                    predicted_grade = "C+"
                    confidence = 65
            else:  # ‡∏ï‡πà‡∏≥
                if current_gpa >= 3.0:
                    predicted_grade = "A"
                    confidence = 85
                elif current_gpa >= 2.5:
                    predicted_grade = "B+"
                    confidence = 80
                else:
                    predicted_grade = "B"
                    confidence = 75
            
            predictions.append({
                "course_id": course["id"],
                "course_name": course["name"],
                "credits": course["credits"],
                "difficulty": course["difficulty"],
                "predicted_grade": predicted_grade,
                "confidence": confidence,
                "grade_point": grade_mapping.get(predicted_grade, 0)
            })
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA ‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ
        predicted_points = sum(p["grade_point"] * p["credits"] for p in predictions)
        predicted_credits = sum(p["credits"] for p in predictions)
        predicted_term_gpa = predicted_points / predicted_credits if predicted_credits > 0 else 0
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA ‡∏™‡∏∞‡∏™‡∏°‡πÉ‡∏´‡∏°‡πà
        new_total_points = total_points + predicted_points
        new_total_credits = total_credits + predicted_credits
        predicted_cumulative_gpa = new_total_points / new_total_credits if new_total_credits > 0 else 0
        
        return {
            "predictions": predictions,
            "current_gpa": round(current_gpa, 2),
            "predicted_term_gpa": round(predicted_term_gpa, 2),
            "predicted_cumulative_gpa": round(predicted_cumulative_gpa, 2),
            "total_credits": predicted_credits,
            "improvement": round(predicted_cumulative_gpa - current_gpa, 2)
        }
        
    except Exception as e:
        logger.error(f"Error generating next term prediction table: {str(e)}")
        return {
            "predictions": [],
            "current_gpa": 0,
            "predicted_term_gpa": 0,
            "predicted_cumulative_gpa": 0,
            "total_credits": 0,
            "improvement": 0
        }

# ==========================================
# Gemini AI Integration Helpers
# ==========================================
GEMINI_SYSTEM_PROMPT = (
    "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏£‡∏î "
    "‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡πÉ‡∏ä‡πâ‡∏ô‡πâ‡∏≥‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏°‡∏∑‡∏≠‡∏≠‡∏≤‡∏ä‡∏µ‡∏û ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡πÅ‡∏•‡∏∞‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏ä‡∏¥‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥ "
    "‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏≤"
)

GEMINI_RESPONSE_SCHEMAS = {
    "insights": {
        "type": "object",
        "properties": {
            "analysis_markdown": {"type": "string"},
            "risk_level": {"type": "string"},
            "graduation_prediction": {
                "type": "object",
                "properties": {
                    "will_graduate": {"type": "boolean"},
                    "prediction_text": {"type": "string"},
                    "reason_why_graduate": {"type": "string"},
                    "reason_why_not_graduate": {"type": "string"},
                    "confidence_percent": {"type": "number"}
                },
                "required": ["will_graduate", "prediction_text"]
            },
            "outcome_summary": {
                "type": "object",
                "properties": {
                    "status": {"type": "string"},
                    "confidence": {"type": "number"},
                    "description": {"type": "string"}
                },
                "required": ["status"]
            },
            "key_metrics": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "label": {"type": "string"},
                        "value": {"type": "string"},
                        "trend": {"type": "string"}
                    }
                }
            },
            "recommendations": {
                "type": "array",
                "items": {"type": "string"}
            },
            "chart": {
                "type": "object",
                "properties": {
                    "title": {"type": "string"},
                    "type": {"type": "string"},
                    "labels": {
                        "type": "array",
                        "items": {"type": "string"}
                    },
                    "series": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "name": {"type": "string"},
                                "data": {
                                    "type": "array",
                                    "items": {"type": "number"}
                                }
                            },
                            "required": ["name", "data"]
                        }
                    }
                }
            }
        },
        "required": ["analysis_markdown"]
    }
}


def is_gemini_available() -> bool:
    return bool(genai and gemini_client_ready)


def _should_retry_with_fallback(exc: Exception) -> bool:
    """
    ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ error ‡∏ô‡∏µ‡πâ‡∏Ñ‡∏ß‡∏£‡∏•‡∏≠‡∏á fallback model ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏ì‡∏µ model ‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô region ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö generateContent
    """
    message = str(exc).lower()
    fallback_indicators = [
        'not found',
        '404',
        'not supported',
        'unsupported',
        'does not exist'
    ]
    return any(indicator in message for indicator in fallback_indicators)


def _to_native_value(value):
    """‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤ numpy/pandas ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Python ‡∏õ‡∏Å‡∏ï‡∏¥"""
    if pd.isna(value):
        return None
    if isinstance(value, (np.generic, np.bool_)):
        return value.item()
    if isinstance(value, (list, tuple, np.ndarray)):
        return [_to_native_value(v) for v in value]
    return value


def summarize_dataframe_for_gemini(df, max_columns=20, max_samples=10):
    summary = {
        'row_count': int(df.shape[0]),
        'column_count': int(df.shape[1]),
        'columns': []
    }
    
    for column in df.columns[:max_columns]:
        series = df[column]
        column_summary = {
            'name': str(column),
            'dtype': str(series.dtype),
            'missing': int(series.isna().sum()),
            'unique': int(series.nunique(dropna=True))
        }
        
        if pd.api.types.is_numeric_dtype(series):
            non_na = series.dropna()
            if not non_na.empty:
                column_summary.update({
                    'min': _to_native_value(non_na.min()),
                    'max': _to_native_value(non_na.max()),
                    'mean': _to_native_value(non_na.mean()),
                    'median': _to_native_value(non_na.median()),
                    'std': _to_native_value(non_na.std())
                })
        else:
            top_values = (
                series.dropna()
                .astype(str)
                .value_counts()
                .head(5)
                .index
                .tolist()
            )
            column_summary['top_values'] = top_values
        
        summary['columns'].append(column_summary)
    
    sample_rows = []
    for _, row in df.head(max_samples).iterrows():
        sample = {}
        for column in df.columns[:max_columns]:
            sample[column] = _to_native_value(row[column])
        sample_rows.append(sample)
    
    return summary, sample_rows


def summarize_grades_for_gemini(course_grades: Dict[str, str], loaded_terms_count: int):
    distribution = Counter(course_grades.values())
    course_details = []
    total_credits = 0
    total_grade_points = 0.0
    credits_for_gpa = 0.0
    failed_courses = []
    
    # Grade mapping ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA
    grade_mapping = {
        'A': 4.0, 'B+': 3.5, 'B': 3.0, 'C+': 2.5, 'C': 2.0,
        'D+': 1.5, 'D': 1.0, 'F': 0.0, 'W': 0.0, 'WF': 0.0, 'WU': 0.0, 'S': None
    }
    
    for course_id, grade in course_grades.items():
        info = COURSE_LOOKUP.get(course_id, {})
        credits = info.get('credit', 3)
        total_credits += credits
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA ‡πÅ‡∏ö‡∏ö weighted ‡∏î‡πâ‡∏ß‡∏¢‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï (‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏° S, W, WF, WU)
        grade_upper = grade.upper() if isinstance(grade, str) else str(grade).upper()
        grade_point = grade_mapping.get(grade_upper)
        
        if grade_point is not None:
            # ‡∏£‡∏ß‡∏°‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA
            credits_for_gpa += credits
            total_grade_points += grade_point * credits
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏ö‡∏ï‡∏Å (F, U, WF, WU)
        if grade_upper in ['F', 'U', 'WF', 'WU']:
            failed_courses.append({
                'course_id': course_id,
                'course_name': info.get('thaiName') or info.get('name'),
                'credit': credits,
                'grade': grade
            })
        
        course_details.append({
            'course_id': course_id,
            'course_name': info.get('thaiName') or info.get('name'),
            'credit': credits,
            'grade': grade
        })
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA ‡πÅ‡∏ö‡∏ö weighted average (‡∏£‡∏ß‡∏°‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï)
    estimated_gpa = float(total_grade_points / credits_for_gpa) if credits_for_gpa > 0 else None
    
    return {
        'total_courses': len(course_grades),
        'estimated_gpa': estimated_gpa,
        'grade_distribution': dict(distribution),
        'course_details': course_details,
        'loaded_terms_count': loaded_terms_count,
        'total_credits_recorded': total_credits,
        'failed_courses': failed_courses,
        'failed_count': len(failed_courses)
    }


def summarize_course_dna(course_profiles: Optional[Dict[str, Dict[str, Any]]]) -> Dict[str, Any]:
    """
    ‡∏™‡∏£‡∏∏‡∏õ Course DNA ‡∏à‡∏≤‡∏Å course_profiles ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô Gemini Prompt
    ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ß‡∏¥‡∏ä‡∏≤‡∏õ‡∏£‡∏≤‡∏ö‡πÄ‡∏ã‡∏µ‡∏¢‡∏ô (Killer Courses), ‡∏ß‡∏¥‡∏ä‡∏≤‡∏á‡πà‡∏≤‡∏¢ (Easy Courses), ‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡∏ä‡∏≤‡πÄ‡∏ü‡πâ‡∏≠ (High Inflation)
    """
    if not course_profiles:
        return {
            "killer_courses": [],
            "easy_courses": [],
            "high_inflation_courses": [],
            "total_courses": 0
        }
    
    killer_courses = []
    easy_courses = []
    high_inflation_courses = []
    
    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î threshold
    KILLER_FAIL_RATE_THRESHOLD = 0.3  # ‡∏ï‡∏Å‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 30% ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Killer
    EASY_AVG_GRADE_THRESHOLD = 3.5  # ‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 3.5 ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡∏á‡πà‡∏≤‡∏¢
    EASY_FAIL_RATE_THRESHOLD = 0.1  # ‡∏ï‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ 10% ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡∏á‡πà‡∏≤‡∏¢
    HIGH_INFLATION_AVG_THRESHOLD = 3.7  # ‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å (‡πÄ‡∏ü‡πâ‡∏≠)
    
    for course_id, profile in course_profiles.items():
        fail_rate = profile.get('fail_rate', 0)
        avg_grade = profile.get('avg_grade', 0)
        difficulty_score = profile.get('difficulty_score', 0)
        total_students = profile.get('total_students', 0)
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Killer Course (‡∏ß‡∏¥‡∏ä‡∏≤‡∏õ‡∏£‡∏≤‡∏ö‡πÄ‡∏ã‡∏µ‡∏¢‡∏ô)
        if fail_rate >= KILLER_FAIL_RATE_THRESHOLD and total_students >= 10:  # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏≠
            killer_courses.append({
                'course_id': course_id,
                'fail_rate': fail_rate,
                'avg_grade': avg_grade,
                'difficulty_score': difficulty_score,
                'total_students': total_students,
                'fail_rate_percent': f"{fail_rate*100:.1f}%"
            })
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Easy Course (‡∏ß‡∏¥‡∏ä‡∏≤‡∏á‡πà‡∏≤‡∏¢)
        if avg_grade >= EASY_AVG_GRADE_THRESHOLD and fail_rate <= EASY_FAIL_RATE_THRESHOLD and total_students >= 10:
            easy_courses.append({
                'course_id': course_id,
                'avg_grade': avg_grade,
                'fail_rate': fail_rate,
                'total_students': total_students
            })
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô High Inflation Course (‡∏ß‡∏¥‡∏ä‡∏≤‡πÄ‡∏ü‡πâ‡∏≠)
        if avg_grade >= HIGH_INFLATION_AVG_THRESHOLD and total_students >= 10:
            high_inflation_courses.append({
                'course_id': course_id,
                'avg_grade': avg_grade,
                'fail_rate': fail_rate,
                'total_students': total_students
            })
    
    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö Killer Courses ‡∏ï‡∏≤‡∏° fail_rate (‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Å‡πà‡∏≠‡∏ô)
    killer_courses.sort(key=lambda x: x['fail_rate'], reverse=True)
    
    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö Easy Courses ‡∏ï‡∏≤‡∏° avg_grade (‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Å‡πà‡∏≠‡∏ô)
    easy_courses.sort(key=lambda x: x['avg_grade'], reverse=True)
    
    return {
        "killer_courses": killer_courses[:20],  # ‡πÄ‡∏≠‡∏≤‡πÅ‡∏Ñ‡πà 20 ‡∏≠‡∏±‡∏ô‡πÅ‡∏£‡∏Å
        "easy_courses": easy_courses[:20],
        "high_inflation_courses": high_inflation_courses[:20],
        "total_courses": len(course_profiles),
        "killer_count": len(killer_courses),
        "easy_count": len(easy_courses),
        "inflation_count": len(high_inflation_courses)
    }

def run_gemini_training_analysis(
    df: pd.DataFrame,
    analysis_goal: Optional[str],
    training_context: Dict[str, Any]
) -> Optional[Dict[str, Any]]:
    """
    ‡πÉ‡∏´‡πâ Gemini ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ï‡∏≤‡∏° prompt ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
    ‡πÅ‡∏•‡πâ‡∏ß‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•
    """
    if not is_gemini_available():
        logger.debug("Gemini not available; skip training-time analysis")
        return None
    
    try:
        logger.info("ü§ñ Starting Gemini training analysis...")
        
        # ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏ó‡∏£‡∏ô
        summary, samples = summarize_dataframe_for_gemini(df, max_columns=30, max_samples=20)
        
        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Course DNA ‡∏à‡∏≤‡∏Å training_context
        dna_info = training_context.get('course_dna_insight', {})
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Gemini
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ ‡∏ñ‡πâ‡∏≤ user ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏°‡∏≤
        goal_text = (analysis_goal or '').strip()
        if not goal_text:
            goal_text = "‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡πÅ‡∏•‡∏∞‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡πÄ‡∏Å‡∏£‡∏î‡∏Ç‡∏≠‡∏á‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏à‡∏≤‡∏Å Course DNA"
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° Course DNA ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏™‡πà‡πÉ‡∏ô prompt
        dna_section = ""
        if dna_info and dna_info.get('killer_courses'):
            killer_list = dna_info.get('killer_courses', [])[:10]  # ‡πÄ‡∏≠‡∏≤‡πÅ‡∏Ñ‡πà 10 ‡∏≠‡∏±‡∏ô‡πÅ‡∏£‡∏Å
            killer_str = ", ".join([f"{k['course_id']} (Fail Rate: {k['fail_rate_percent']})" for k in killer_list])
            easy_list = dna_info.get('easy_courses', [])[:10]
            easy_str = ", ".join([f"{e['course_id']} (Avg: {e['avg_grade']:.2f})" for e in easy_list])
            
            dna_section = f"""
**‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£ (Course DNA) - ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏î‡∏¢‡∏£‡∏∞‡∏ö‡∏ö Backend:**
- ‡∏ß‡∏¥‡∏ä‡∏≤‡∏õ‡∏£‡∏≤‡∏ö‡πÄ‡∏ã‡∏µ‡∏¢‡∏ô (Killer Courses): {killer_str if killer_str else "‡πÑ‡∏°‡πà‡∏û‡∏ö"}
- ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏õ‡∏£‡∏≤‡∏ö‡πÄ‡∏ã‡∏µ‡∏¢‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {dna_info.get('killer_count', 0)} ‡∏ß‡∏¥‡∏ä‡∏≤
- ‡∏ß‡∏¥‡∏ä‡∏≤‡∏á‡πà‡∏≤‡∏¢ (Easy Courses): {easy_str if easy_str else "‡πÑ‡∏°‡πà‡∏û‡∏ö"}
- ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏á‡πà‡∏≤‡∏¢‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {dna_info.get('easy_count', 0)} ‡∏ß‡∏¥‡∏ä‡∏≤
- ‡∏ß‡∏¥‡∏ä‡∏≤‡πÄ‡∏ü‡πâ‡∏≠‡πÄ‡∏Å‡∏£‡∏î (High Inflation): {dna_info.get('inflation_count', 0)} ‡∏ß‡∏¥‡∏ä‡∏≤
- ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {dna_info.get('total_courses', 0)} ‡∏ß‡∏¥‡∏ä‡∏≤

**‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏:** ‡∏£‡∏∞‡∏ö‡∏ö Backend ‡πÑ‡∏î‡πâ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Course DNA ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏•‡πâ‡∏ß ‡πÇ‡∏î‡∏¢:
- Killer Course = ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡∏ï‡∏Å ‚â• 30%
- Easy Course = ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ ‚â• 3.5 ‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡∏ï‡∏Å ‚â§ 10%
- High Inflation = ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ ‚â• 3.7

"""
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt ‡∏ó‡∏µ‡πà‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Ç‡∏∂‡πâ‡∏ô
        training_prompt = f"""
‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô Data Science ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤
‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ñ‡∏∑‡∏≠ Course DNA ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡πÄ‡∏Å‡∏£‡∏î‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏ö‡∏ö Dynamic Snapshots ‡πÅ‡∏•‡πâ‡∏ß

**‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå:**
{goal_text}

**‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏ó‡∏£‡∏ô:**
- ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß: {summary['row_count']} ‡πÅ‡∏ñ‡∏ß
- ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå: {summary['column_count']} ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
- ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå: {json.dumps(summary['columns'], ensure_ascii=False, indent=2)}

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (20 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å):**
{json.dumps(samples, ensure_ascii=False, indent=2)}

{dna_section}

**‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô:**
- ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {training_context.get('data_format', '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏')}
- ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô: {training_context.get('training_type', '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏')}
- ‡πÉ‡∏ä‡πâ Advanced Pipeline: {training_context.get('use_advanced_pipeline', False)}
- ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÅ‡∏•‡πâ‡∏ß: {training_context.get('prepared_samples', 0)}
- ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô features: {training_context.get('feature_count', 0)}
- ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á label: {json.dumps(training_context.get('label_distribution', {}), ensure_ascii=False)}

**‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì:**
1. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£ (‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å-‡∏á‡πà‡∏≤‡∏¢‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡πÑ‡∏´‡∏°)
2. ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ß‡πà‡∏≤ Killer Courses ‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£
3. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà ‡∏°‡∏µ missing values ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà?
4. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á label (‡∏à‡∏ö/‡πÑ‡∏°‡πà‡∏à‡∏ö) ‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà?
5. ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥: ‡∏°‡∏µ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏≠‡∏∞‡πÑ‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà?
6. ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏° ‡πÉ‡∏´‡πâ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏à‡∏≤‡∏Å‡πÄ‡∏ó‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ

‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
"""
        
        payload = {
            'analysis_goal': goal_text,
            'dataset_summary': summary,
            'sample_rows': samples,
            'training_context': training_context,
            'detailed_prompt': training_prompt
        }
        
        logger.info("üì§ Sending training data to Gemini for analysis...")
        gemini_output = call_gemini_structured('training_dataset_analysis', payload)
        
        logger.info("‚úÖ Gemini training analysis completed")
        
        return {
            'analysis_goal': goal_text,
            'dataset_summary': summary,
            'sample_rows': samples,
            'training_context': training_context,
            'gemini': gemini_output,
            'generated_at': datetime.now().isoformat(),
            'analysis_type': 'training_file_analysis'
        }
    except Exception as exc:
        logger.warning(f"Gemini training analysis failed: {exc}")
        import traceback
        logger.error(traceback.format_exc())
        return {
            'analysis_goal': analysis_goal,
            'training_context': training_context,
            'error': str(exc),
            'generated_at': datetime.now().isoformat()
        }


def repair_incomplete_json(text: str) -> str:
    """
    ‡∏ã‡πà‡∏≠‡∏°‡πÅ‡∏ã‡∏° JSON ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå (unterminated string, unclosed braces, etc.)
    """
    if not text:
        return text
    
    # ‡∏•‡∏ö markdown code blocks ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
    cleaned = text.strip()
    if cleaned.startswith('```'):
        cleaned = re.sub(r'^```(?:json)?\s*', '', cleaned)
        cleaned = re.sub(r'\s*```$', '', cleaned)
    
    # ‡∏•‡∏ö whitespace ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
    cleaned = cleaned.strip()
    
    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ { ‡πÉ‡∏´‡πâ return text ‡πÄ‡∏î‡∏¥‡∏°
    if '{' not in cleaned:
        return cleaned
    
    # ‡∏´‡∏≤ JSON object ‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
    brace_count = 0
    bracket_count = 0
    in_string = False
    escape_next = False
    last_valid_pos = -1
    start_pos = cleaned.find('{')
    
    if start_pos == -1:
        return cleaned
    
    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå JSON structure
    for i in range(start_pos, len(cleaned)):
        char = cleaned[i]
        
        if escape_next:
            escape_next = False
            continue
        
        if char == '\\':
            escape_next = True
            continue
        
        if char == '"' and not escape_next:
            in_string = not in_string
            continue
        
        if in_string:
            continue
        
        if char == '{':
            brace_count += 1
        elif char == '}':
            brace_count -= 1
            if brace_count == 0 and bracket_count == 0:
                last_valid_pos = i
                break
        elif char == '[':
            bracket_count += 1
        elif char == ']':
            bracket_count -= 1
    
    # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠ JSON object ‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏±‡πâ‡∏ô
    if last_valid_pos > start_pos:
        return cleaned[start_pos:last_valid_pos + 1]
    
    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡∏ã‡πà‡∏≠‡∏°‡πÅ‡∏ã‡∏°
    if brace_count > 0 or bracket_count > 0 or in_string:
        # ‡∏´‡∏≤ string ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏õ‡∏¥‡∏î quote
        in_string = False
        escape_next = False
        string_start = -1
        last_string_char = -1
        
        for i in range(start_pos, len(cleaned)):
            char = cleaned[i]
            
            if escape_next:
                escape_next = False
                continue
            
            if char == '\\':
                escape_next = True
                continue
            
            if char == '"' and not escape_next:
                if not in_string:
                    in_string = True
                    string_start = i
                else:
                    in_string = False
                    string_start = -1
                    last_string_char = i
        
        # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ string ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏õ‡∏¥‡∏î ‡πÉ‡∏´‡πâ‡∏õ‡∏¥‡∏î‡∏°‡∏±‡∏ô
        if in_string and string_start >= 0:
            # ‡∏´‡∏≤‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏¥‡∏î string
            # ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡∏ó‡πâ‡∏≤‡∏¢‡∏™‡∏∏‡∏î‡∏¢‡πâ‡∏≠‡∏ô‡∏Å‡∏•‡∏±‡∏ö
            insert_pos = len(cleaned)
            for i in range(len(cleaned) - 1, string_start, -1):
                if cleaned[i] in [',', '}', ']', '\n', '\r']:
                    insert_pos = i
                    break
            
            # ‡∏õ‡∏¥‡∏î string
            cleaned = cleaned[:insert_pos] + '"' + cleaned[insert_pos:]
        
        # ‡∏õ‡∏¥‡∏î brackets ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠ (‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏¥‡∏î‡∏Å‡πà‡∏≠‡∏ô braces)
        if bracket_count > 0:
            cleaned = cleaned + ']' * bracket_count
        
        # ‡∏õ‡∏¥‡∏î braces ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠
        if brace_count > 0:
            cleaned = cleaned + '}' * brace_count
    
    return cleaned

@retry_on_quota_error(max_retries=3, initial_delay=20)
def call_gemini_with_retry(prompt_or_payload, task_type='prediction_analysis'):
    """‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Gemini ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏∞‡∏ö‡∏ö retry (‡∏•‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î 3 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á)"""
    # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô string ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á payload ‡∏î‡πâ‡∏ß‡∏¢ detailed_prompt
    if isinstance(prompt_or_payload, str):
        payload = {'detailed_prompt': prompt_or_payload}
    else:
        payload = prompt_or_payload
    
    return call_gemini_structured(task_type, payload)


def call_gemini_structured(task_name: str, payload: Dict[str, Any], schema_key: str = 'insights'):
    """
    ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Gemini API ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á prompt ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤‡πÅ‡∏•‡∏∞ detailed prompt
    """
    if not is_gemini_available():
        raise RuntimeError("Gemini API is not configured")

    # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ detailed_prompt ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡πÅ‡∏ó‡∏ô
    if 'detailed_prompt' in payload:
        user_prompt = payload['detailed_prompt']
        logger.info("üìù Using detailed prompt for Gemini analysis")
    else:
        prompt_payload = {
            'task': task_name,
            'payload': payload
        }
        user_prompt = json.dumps(prompt_payload, ensure_ascii=False)

    base_generation_config = {
        "temperature": 0.3,
        "top_p": 0.95,
        "top_k": 40,
        "max_output_tokens": 4096,
        "response_mime_type": "application/json"
    }

    schema = GEMINI_RESPONSE_SCHEMAS.get(schema_key)
    if schema:
        base_generation_config["response_schema"] = schema
        logger.info(f"üìã Using schema: {schema_key}")
    else:
        logger.warning(f"‚ö†Ô∏è Schema '{schema_key}' not found, using default")

    def _execute_with_model(model_name: str):
        generation_config = copy.deepcopy(base_generation_config)
        
        # Increase token limit for 1.5 models which support higher output
        if '1.5' in model_name:
            generation_config['max_output_tokens'] = 8192
            
        model = genai.GenerativeModel(
            model_name,
            system_instruction=GEMINI_SYSTEM_PROMPT,
            generation_config=generation_config
        )

        logger.info(f"üì§ Sending request to Gemini (task: {task_name}, model: {model_name})...")
        logger.debug(f"Prompt length: {len(user_prompt)} characters")

        response = model.generate_content(user_prompt)
        response_text = None

        if hasattr(response, 'text') and response.text:
            response_text = response.text
            logger.debug("Got response from response.text")

        if not response_text and hasattr(response, 'candidates') and response.candidates:
            try:
                candidate = response.candidates[0]
                if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                    parts = candidate.content.parts
                    if parts:
                        first_part = parts[0]
                        if hasattr(first_part, 'text'):
                            response_text = first_part.text
                        elif isinstance(first_part, dict) and 'text' in first_part:
                            response_text = first_part['text']
                        logger.debug("Got response from candidates[0].content.parts[0]")
            except Exception as e:
                logger.warning(f"Error extracting from candidates: {e}")

        if not response_text:
            try:
                response_str = str(response)
                if response_str and response_str != 'None':
                    json_match = re.search(r'\{.*\}', response_str, re.DOTALL)
                    if json_match:
                        response_text = json_match.group(0)
                        logger.debug("Got response from string extraction")
            except Exception as e:
                logger.warning(f"Error extracting from string: {e}")

        if not response_text:
            error_msg = "Empty response from Gemini"
            logger.error(f"‚ùå {error_msg}")
            logger.debug(f"Response object: {response}")
            logger.debug(f"Response type: {type(response)}")
            logger.debug(f"Response dir: {dir(response)}")
            raise ValueError(error_msg)

        logger.debug(f"Response text length: {len(response_text)} characters")
        logger.debug(f"Response preview: {response_text[:200]}...")

        try:
            result = json.loads(response_text)
            logger.info("‚úÖ Gemini analysis completed successfully")
            return result
        except json.JSONDecodeError as json_err:
            logger.warning(f"‚ö†Ô∏è Initial JSON parse failed: {json_err}")
            logger.debug(f"Response text (first 1000 chars): {response_text[:1000]}...")
            
            try:
                # Attempt 1: Clean markdown code blocks
                cleaned_text = response_text.strip()
                if cleaned_text.startswith('```'):
                    cleaned_text = re.sub(r'^```(?:json)?\s*', '', cleaned_text)
                    cleaned_text = re.sub(r'\s*```$', '', cleaned_text)
                
                try:
                    result = json.loads(cleaned_text)
                    logger.info("‚úÖ Successfully parsed JSON after cleaning markdown")
                    return result
                except json.JSONDecodeError as e1:
                    logger.debug(f"Markdown cleaning failed: {e1}")
                    
                    # Attempt 2: Repair incomplete JSON (unterminated strings, unclosed braces)
                    try:
                        repaired_text = repair_incomplete_json(cleaned_text)
                        result = json.loads(repaired_text)
                        logger.info("‚úÖ Successfully parsed JSON after repair")
                        return result
                    except json.JSONDecodeError as e2:
                        logger.debug(f"JSON repair failed: {e2}")
                        
                        # Attempt 3: Extract JSON object using regex (fallback)
                        json_match = re.search(r'\{.*\}', cleaned_text, re.DOTALL)
                        if json_match:
                            extracted_json = json_match.group(0)
                            try:
                                # Try to repair extracted JSON too
                                repaired_extracted = repair_incomplete_json(extracted_json)
                                result = json.loads(repaired_extracted)
                                logger.info("‚úÖ Successfully extracted and parsed JSON using regex + repair")
                                return result
                            except json.JSONDecodeError:
                                # Last attempt: try original extracted
                                result = json.loads(extracted_json)
                                logger.info("‚úÖ Successfully extracted and parsed JSON using regex")
                                return result
                        raise ValueError(f"Invalid JSON response from Gemini: {str(json_err)}")
            except Exception as repair_exc:
                logger.error(f"‚ùå All JSON repair attempts failed: {repair_exc}")
                logger.error(f"Original error: {json_err}")
                logger.error(f"Response text (first 2000 chars): {response_text[:2000]}...")
                raise ValueError(f"Invalid JSON response from Gemini: {str(json_err)}")

    last_error = None
    tried_models: List[str] = []

    for model_name in GEMINI_MODEL_CANDIDATES:
        tried_models.append(model_name)
        try:
            return _execute_with_model(model_name)
        except ValueError as ve:
            logger.warning(f"‚ö†Ô∏è Gemini model '{model_name}' returned invalid response: {ve}")
            last_error = ve
            continue
        except Exception as exc:
            last_error = exc
            if _should_retry_with_fallback(exc) and model_name != GEMINI_MODEL_CANDIDATES[-1]:
                logger.warning(f"‚ö†Ô∏è Gemini model '{model_name}' unavailable ({exc}); trying fallback...")
                continue
            logger.error(f"‚ùå Gemini request failed on model '{model_name}': {exc}")
            import traceback
            logger.error(traceback.format_exc())
            raise RuntimeError(f"Gemini API error ({model_name}): {str(exc)}") from exc

    raise RuntimeError(
        f"Gemini API error after trying models {', '.join(tried_models)}: {last_error}"
    )

# Flask Routes (Keep all other routes unchanged)

def refresh_r2_storage_from_settings():
    """Reinitialize storage object with current environment settings."""
    global storage
    try:
        storage = S3Storage()
        return True
    except Exception as e:
        logger.error(f"Failed to refresh R2 storage runtime: {e}")
        return False


@app.route('/admin/login', methods=['GET', 'POST'])
def admin_login():
    if not admin_manager.enabled:
        return "MongoDB admin backend is not configured", 503

    if request.method == 'GET':
        if session.get('user_id'):
            return redirect(url_for('admin_settings'))
        return render_template('admin_login.html')

    payload = request.get_json(silent=True) or request.form
    username = (payload.get('username') or '').strip()
    password = payload.get('password') or ''

    user = admin_manager.authenticate(username, password)
    if not user:
        if request.path.startswith('/api/') or request.is_json:
            return _json_error('Invalid credentials', 401)
        return render_template('admin_login.html', error='‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏´‡∏±‡∏™‡∏ú‡πà‡∏≤‡∏ô‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á')

    session['user_id'] = str(user.get('_id'))
    session['username'] = user.get('username')
    session['user_role'] = user.get('role', 'user')
    session['must_change_password'] = bool(user.get('must_change_password', False))
    session.permanent = True

    admin_manager._log_audit(session['user_id'], 'LOGIN', 'admin_login')

    if session.get('must_change_password'):
        return redirect(url_for('admin_change_password'))
    return redirect(url_for('admin_settings'))


@app.route('/admin')
def admin_home():
    if not admin_manager.enabled:
        return "MongoDB admin backend is not configured", 503
    if not is_logged_in():
        return redirect(url_for('admin_login'))
    if session.get('must_change_password'):
        return redirect(url_for('admin_change_password'))
    if has_any_role('admin', 'super_admin'):
        return redirect(url_for('admin_settings'))
    return redirect(url_for('index'))


@app.route('/admin/logout')
def admin_logout():
    if admin_manager.enabled and session.get('user_id'):
        admin_manager._log_audit(session.get('user_id'), 'LOGOUT', 'admin_logout')
    session.clear()
    return redirect(url_for('admin_login'))


@app.route('/admin/change-password', methods=['GET', 'POST'])
@admin_login_required
def admin_change_password():
    if request.method == 'GET':
        return render_template('admin_change_password.html')

    payload = request.get_json(silent=True) or request.form
    current_password = payload.get('current_password', '')
    new_password = payload.get('new_password', '')
    confirm_password = payload.get('confirm_password', '')

    if len(new_password) < 8:
        return render_template('admin_change_password.html', error='‡∏£‡∏´‡∏±‡∏™‡∏ú‡πà‡∏≤‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏¢‡∏≤‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 8 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£')
    if new_password != confirm_password:
        return render_template('admin_change_password.html', error='‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏£‡∏´‡∏±‡∏™‡∏ú‡πà‡∏≤‡∏ô‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô')

    user_doc = admin_manager.users.find_one({'username': session.get('username')})
    if not user_doc:
        return render_template('admin_change_password.html', error='‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ö‡∏±‡∏ç‡∏ä‡∏µ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ')

    if not user_doc.get('must_change_password'):
        if not check_password_hash(user_doc.get('password_hash', ''), current_password):
            return render_template('admin_change_password.html', error='‡∏£‡∏´‡∏±‡∏™‡∏ú‡πà‡∏≤‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á')

    admin_manager.change_password(user_doc['_id'], new_password)
    session['must_change_password'] = False
    admin_manager._log_audit(session.get('user_id'), 'CHANGE_PASSWORD', session.get('username'))
    return render_template('admin_change_password.html', success='‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏£‡∏´‡∏±‡∏™‡∏ú‡πà‡∏≤‡∏ô‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à')


@app.route('/admin/settings', methods=['GET', 'POST'])
@admin_login_required
@roles_required('admin', 'super_admin')
def admin_settings():
    if request.method == 'POST':
        form = request.get_json(silent=True) or request.form

        setting_map = {
            'gemini_api_key': 'GEMINI_API_KEY',
            'gemini_model_name': 'GEMINI_MODEL_NAME',
            'gemini_model_fallbacks': 'GEMINI_MODEL_FALLBACKS',
            'r2_access_key': 'CLOUDFLARE_R2_ACCESS_KEY_ID',
            'r2_secret_key': 'CLOUDFLARE_R2_SECRET_ACCESS_KEY',
            'r2_endpoint': 'CLOUDFLARE_R2_ENDPOINT',
            'r2_bucket_name': 'CLOUDFLARE_R2_BUCKET_NAME',
        }

        for form_key, setting_key in setting_map.items():
            if form_key in form and form.get(form_key) is not None:
                value = str(form.get(form_key)).strip()
                if value:
                    admin_manager.set_setting(setting_key, value, actor_user_id=session.get('user_id'))

        admin_manager.apply_runtime_env()
        refresh_gemini_runtime_from_settings()
        refresh_r2_storage_from_settings()

        return render_template('admin_settings.html', success='‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß')

    current_values = {
        'gemini_api_key': admin_manager.get_setting('GEMINI_API_KEY', os.environ.get('GEMINI_API_KEY', '')),
        'gemini_model_name': admin_manager.get_setting('GEMINI_MODEL_NAME', os.environ.get('GEMINI_MODEL_NAME', 'gemini-3-flash-preview')),
        'gemini_model_fallbacks': admin_manager.get_setting('GEMINI_MODEL_FALLBACKS', os.environ.get('GEMINI_MODEL_FALLBACKS', '')),
        'r2_access_key': admin_manager.get_setting('CLOUDFLARE_R2_ACCESS_KEY_ID', os.environ.get('CLOUDFLARE_R2_ACCESS_KEY_ID', '')),
        'r2_secret_key': admin_manager.get_setting('CLOUDFLARE_R2_SECRET_ACCESS_KEY', os.environ.get('CLOUDFLARE_R2_SECRET_ACCESS_KEY', '')),
        'r2_endpoint': admin_manager.get_setting('CLOUDFLARE_R2_ENDPOINT', os.environ.get('CLOUDFLARE_R2_ENDPOINT', '')),
        'r2_bucket_name': admin_manager.get_setting('CLOUDFLARE_R2_BUCKET_NAME', os.environ.get('CLOUDFLARE_R2_BUCKET_NAME', '')),
        'gemini_api_key_masked': MongoAdminManager.mask_value(admin_manager.get_setting('GEMINI_API_KEY', os.environ.get('GEMINI_API_KEY', ''))),
        'r2_access_key_masked': MongoAdminManager.mask_value(admin_manager.get_setting('CLOUDFLARE_R2_ACCESS_KEY_ID', os.environ.get('CLOUDFLARE_R2_ACCESS_KEY_ID', ''))),
        'r2_secret_key_masked': MongoAdminManager.mask_value(admin_manager.get_setting('CLOUDFLARE_R2_SECRET_ACCESS_KEY', os.environ.get('CLOUDFLARE_R2_SECRET_ACCESS_KEY', ''))),
    }

    return render_template('admin_settings.html', **current_values)


@app.route('/api/admin/me')
@admin_login_required
def admin_me():
    return jsonify({
        'success': True,
        'username': session.get('username'),
        'role': session.get('user_role'),
        'must_change_password': bool(session.get('must_change_password'))
    })


@app.route('/api/admin/users', methods=['GET', 'POST'])
@admin_login_required
@roles_required('super_admin')
def admin_users_api():
    if request.method == 'GET':
        users = []
        for u in admin_manager.users.find({}, {'password_hash': 0}):
            users.append({
                'id': str(u.get('_id')),
                'username': u.get('username'),
                'role': u.get('role', 'user'),
                'is_active': bool(u.get('is_active', True)),
                'must_change_password': bool(u.get('must_change_password', False)),
                'created_at': u.get('created_at')
            })
        return jsonify({'success': True, 'users': users})

    data = request.get_json(silent=True) or {}
    username = (data.get('username') or '').strip()
    role = (data.get('role') or 'user').strip()

    if not username:
        return _json_error('username is required', 400)
    if role not in ['user', 'admin', 'super_admin']:
        return _json_error('invalid role', 400)

    temp_password = secrets.token_urlsafe(10)
    try:
        admin_manager.users.insert_one({
            'username': username,
            'email': data.get('email'),
            'password_hash': generate_password_hash(temp_password),
            'role': role,
            'must_change_password': True,
            'is_active': True,
            'created_at': datetime.utcnow().isoformat(),
            'updated_at': datetime.utcnow().isoformat(),
            'last_login_at': None
        })
        admin_manager._log_audit(session.get('user_id'), 'CREATE_USER', username)
        return jsonify({
            'success': True,
            'username': username,
            'role': role,
            'temporary_password': temp_password
        })
    except Exception as e:
        return _json_error(f'cannot create user: {e}', 400)


@app.route('/api/admin/test-connections', methods=['POST'])
@admin_login_required
@roles_required('admin', 'super_admin')
def admin_test_connections():
    """Test runtime connections for MongoDB, Cloudflare R2, and Gemini."""
    results = {
        'success': True,
        'mongo': {'connected': False, 'message': ''},
        'r2': {'connected': False, 'message': ''},
        'gemini': {'connected': False, 'message': ''},
    }

    # MongoDB
    try:
        if admin_manager.enabled and admin_manager.client:
            admin_manager.client.admin.command('ping')
            results['mongo'] = {'connected': True, 'message': 'MongoDB ping success'}
        else:
            results['mongo'] = {'connected': False, 'message': 'MongoDB admin backend not enabled'}
    except Exception as e:
        results['mongo'] = {'connected': False, 'message': str(e)}

    # R2
    try:
        refresh_r2_storage_from_settings()
        if storage.use_local or not getattr(storage, 's3_client', None):
            results['r2'] = {'connected': False, 'message': 'R2 client not initialized (using local storage)'}
        else:
            storage.s3_client.list_objects_v2(Bucket=storage.bucket_name, MaxKeys=1)
            results['r2'] = {'connected': True, 'message': f'Connected to bucket {storage.bucket_name}'}
    except Exception as e:
        results['r2'] = {'connected': False, 'message': str(e)}

    # Gemini
    try:
        refresh_gemini_runtime_from_settings()
        if not is_gemini_available():
            results['gemini'] = {'connected': False, 'message': 'Gemini API key/model not configured'}
        else:
            # Lightweight call to verify API key/model works
            if genai is None:
                raise RuntimeError('google.generativeai not installed')
            model = genai.GenerativeModel(GEMINI_MODEL_NAME)
            ping_response = model.generate_content('ping')
            if not getattr(ping_response, 'text', None):
                raise RuntimeError('Empty response from Gemini')
            results['gemini'] = {'connected': True, 'message': f'Connected with model {GEMINI_MODEL_NAME}'}
    except Exception as e:
        results['gemini'] = {'connected': False, 'message': str(e)}

    overall_ok = all([
        results['mongo']['connected'],
        results['r2']['connected'],
        results['gemini']['connected'],
    ])

    results['success'] = overall_ok
    admin_manager._log_audit(session.get('user_id'), 'TEST_CONNECTIONS', 'mongo_r2_gemini', 'success' if overall_ok else 'failed', results)
    return jsonify(results)


@app.route('/')
def index():
    """Landing page: redirect users to prediction page."""
    return redirect(url_for('curriculum_prediction_form'))

@app.route('/test')
def curriculum_prediction_form():
    """Page for predicting graduation based on curriculum and prerequisites."""
    try:
        # ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô JSON string ‡∏ó‡∏µ‡πà‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
        courses_json = json.dumps(app.config.get('COURSES_DATA', []))
        terms_json = json.dumps(app.config.get('ALL_TERMS_DATA', []))
        grades_json = json.dumps(app.config.get('DATA_CONFIG', {}).get('grade_mapping', {}))
        
        return render_template(
            'curriculum_prediction_form.html',
            coursesData=courses_json,
            allTermsData=terms_json,
            gradeMapping=grades_json
        )
    except Exception as e:
        logger.error(f"Error rendering curriculum form: {str(e)}")
        # Fallback ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤
        return render_template(
            'curriculum_prediction_form.html',
            coursesData='[]',
            allTermsData='[]',
            gradeMapping='{}'
        )


@app.route('/status')
def status_page():
    """‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏£‡∏∞‡∏ö‡∏ö"""
    return render_template('status.html')

@app.route('/api/system/status')
def system_status():
    """API ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏£‡∏∞‡∏ö‡∏ö"""
    import sys
    import flask
    
    try:
        logger.info("üîß Checking system status...")
        
        # ‡πÉ‡∏ä‡πâ instance ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß
        storage_status = storage.get_connection_status()
        
        # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•
        try:
            models_list = storage.list_models()
            total_size = sum(m.get('size', 0) for m in models_list if 'size' in m)
        except Exception as e:
            logger.warning(f"Could not get models info: {e}")
            models_list = []
            total_size = 0
        
        # Get recent logs (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ log file)
        recent_logs = []
        try:
            log_file = 'app.log'
            if os.path.exists(log_file):
                with open(log_file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    recent_logs = lines[-20:]
        except Exception as e:
            logger.warning(f"Could not read logs: {e}")

        # ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å storage_status ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
        status_data = {
            'success': True,
            'r2_connected': storage_status['connected'],
            'storage_provider': storage_status['storage_type'],
            'bucket_name': storage_status['bucket_name'],
            'models_available': len(models_list),
            'total_size': total_size,
            'python_version': sys.version.split()[0],
            'flask_version': flask.__version__,
            'environment': os.environ.get('FLASK_ENV', 'production'),
            'debug_mode': app.debug,
            'server_time': datetime.now().isoformat(),
            'app_folders': {
                'upload_folder': app.config['UPLOAD_FOLDER'],
                'model_folder': app.config['MODEL_FOLDER'],
                'upload_exists': os.path.exists(app.config['UPLOAD_FOLDER']),
                'model_exists': os.path.exists(app.config['MODEL_FOLDER'])
            },
            'recent_logs': recent_logs,
            'error_message': storage_status.get('errors'),
            'env_vars_status': {
                'R2_ACCESS_KEY': bool(os.environ.get('CLOUDFLARE_R2_ACCESS_KEY_ID')),
                'R2_SECRET_KEY': bool(os.environ.get('CLOUDFLARE_R2_SECRET_ACCESS_KEY')),
                'R2_ENDPOINT': bool(os.environ.get('CLOUDFLARE_R2_ENDPOINT')),
                'R2_BUCKET': bool(os.environ.get('CLOUDFLARE_R2_BUCKET_NAME'))
            }
        }
        
        logger.info("‚úÖ System status check completed")
        return jsonify(status_data)
        
    except Exception as e:
        logger.error(f"‚ùå Error getting system status: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc(),
            'server_time': datetime.now().isoformat()
        }), 500

@app.route('/api/test-r2-connection')
def test_r2_connection():
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ R2"""
    try:
        logger.info("üß™ Testing R2 connection...")
        
        if storage.use_local or not hasattr(storage, 's3_client') or not storage.s3_client:
            return jsonify({
                'success': False,
                'error': 'R2 client not initialized - using local storage',
                'storage_provider': 'local'
            })
        
        # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö list objects
        response = storage.s3_client.list_objects_v2(
            Bucket=storage.bucket_name,
            MaxKeys=1
        )
        
        logger.info("‚úÖ R2 connection test successful")
        return jsonify({
            'success': True,
            'message': 'R2 connection successful',
            'bucket': storage.bucket_name,
            'endpoint': storage.endpoint_url,
            'objects_found': len(response.get('Contents', []))
        })
        
    except Exception as e:
        logger.error(f"‚ùå R2 connection test failed: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e),
            'storage_provider': 'local_fallback'
        })
        
@app.route('/api/test-r2')
def test_r2():
    try:
        # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏ü‡∏•‡πå
        test_data = {'test': True, 'timestamp': datetime.now().isoformat()}
        success = storage.save_model(test_data, 'test_model.joblib')
        
        if success:
            # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå
            loaded = storage.load_model('test_model.joblib')
            if loaded:
                # ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏î‡∏™‡∏≠‡∏ö
                storage.delete_model('test_model.joblib')
                return jsonify({
                    'success': True,
                    'message': 'R2 storage working perfectly!',
                    'can_write': True,
                    'can_read': True,
                    'can_delete': True
                })
        
        return jsonify({'success': False, 'message': 'R2 test failed'})
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})
        
@app.route('/api/config', methods=['GET'])
def get_config_for_frontend():
    """Provides frontend with necessary configuration data."""
    try:
        config_data = {
            'success': True,
            'COURSES_DATA': app.config.get('COURSES_DATA', []),
            'ALL_TERMS_DATA': app.config.get('ALL_TERMS_DATA', []),
            'GRADE_MAPPING': app.config.get('DATA_CONFIG', {}).get('grade_mapping', {}),
            'MESSAGES': app.config.get('MESSAGES', {}),
            'DATA_CONFIG_RISK_LEVELS': app.config.get('DATA_CONFIG', {}).get('risk_levels', {}),
            'GEMINI_ENABLED': is_gemini_available()
        }
        return jsonify(config_data)
    except Exception as e:
        logger.error(f"Error loading config data: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/gemini/analyze_file', methods=['POST'])
def gemini_analyze_file_route():
    """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÑ‡∏ü‡∏•‡πå CSV/XLSX ‡∏î‡πâ‡∏ß‡∏¢ Gemini"""
    if not is_gemini_available():
        return jsonify({'success': False, 'error': '‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Gemini API Key'}), 503
    
    uploaded_file = request.files.get('file')
    analysis_goal = request.form.get('analysis_goal', '').strip()
    
    if uploaded_file is None or uploaded_file.filename == '':
        return jsonify({'success': False, 'error': '‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV ‡∏´‡∏£‡∏∑‡∏≠ Excel'}), 400
    
    try:
        file_bytes = uploaded_file.read()
        if not file_bytes:
            return jsonify({'success': False, 'error': '‡πÑ‡∏ü‡∏•‡πå‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤'}), 400
        
        max_bytes = int(GEMINI_MAX_FILE_SIZE_MB * 1024 * 1024)
        if len(file_bytes) > max_bytes:
            return jsonify({
                'success': False,
                'error': f'‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô {GEMINI_MAX_FILE_SIZE_MB} MB'
            }), 400
        
        buffer = BytesIO(file_bytes)
        filename = uploaded_file.filename.lower()
        
        if filename.endswith(('.xlsx', '.xls')):
            df = pd.read_excel(buffer)
        else:
            buffer.seek(0)
            df = pd.read_csv(buffer)
        
        summary, samples = summarize_dataframe_for_gemini(df)
        payload = {
            'analysis_goal': analysis_goal or '‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏£‡∏î‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå',
            'dataset_summary': summary,
            'sample_rows': samples
        }
        gemini_output = call_gemini_structured('csv_dataset_analysis', payload)
        
        return jsonify({
            'success': True,
            'source': 'file',
            'dataset_summary': summary,
            'sample_rows': samples,
            'gemini': gemini_output
        })
    except Exception as exc:
        logger.error(f"Gemini file analysis error: {exc}")
        return jsonify({'success': False, 'error': str(exc)}), 400


@app.route('/api/gemini/predict', methods=['POST'])
def gemini_predict_route():
    """‡πÉ‡∏´‡πâ Gemini ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏£‡∏î‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏Å‡∏£‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏™‡∏î"""
    if not is_gemini_available():
        logger.warning("Gemini API not available - missing API key")
        return jsonify({
            'success': False, 
            'error': '‚ö†Ô∏è ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Gemini API Key',
            'suggestion': '‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ GEMINI_API_KEY ‡πÉ‡∏ô environment variables'
        }), 503
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö rate limit
    can_proceed, wait_time = gemini_rate_limiter.can_proceed()
    if not can_proceed:
        return jsonify({
            'success': False,
            'error': f'‚è±Ô∏è ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ö‡πà‡∏≠‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏≠ {wait_time} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ',
            'suggestion': '‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏≠‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏π‡πà‡∏Å‡πà‡∏≠‡∏ô‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà',
            'retry_after': wait_time
        }), 429
    
    try:
        payload = request.get_json(silent=True) or {}
        logger.info(f"Received Gemini prediction request: {list(payload.keys())}")
        
        course_grades = payload.get('course_grades') or payload.get('grades') or {}
        student_name = payload.get('student_name', '‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤')
        analysis_goal = payload.get('analysis_goal', '').strip()
        loaded_terms_count = int(payload.get('loaded_terms_count') or 0)
        model_filename = payload.get('model_filename')
        training_analysis = None
        model_metadata = None
        
        logger.info(f"Processing prediction for {student_name} with {len(course_grades)} course grades")
        
        if not isinstance(course_grades, dict) or len(course_grades) == 0:
            logger.warning("No course grades provided")
            return jsonify({'success': False, 'error': '‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏£‡∏î‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡∏ß‡∏¥‡∏ä‡∏≤'}), 400
    except Exception as parse_error:
        logger.error(f"Error parsing request: {parse_error}")
        return jsonify({'success': False, 'error': f'‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {str(parse_error)}'}), 400
    
    # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏Å‡∏£‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏á
    cleaned_grades = {
        str(course_id): grade
        for course_id, grade in course_grades.items()
        if grade
    }
    
    if len(cleaned_grades) == 0:
        return jsonify({'success': False, 'error': '‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏Å‡∏£‡∏î‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ô‡∏≥‡∏°‡∏≤‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÑ‡∏î‡πâ'}), 400
    
    # ‡πÇ‡∏´‡∏•‡∏î Course DNA ‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
    course_profiles = None
    course_context_str = ""
    training_analysis = None
    model_metadata = None
    
    try:
        if model_filename:
            try:
                stored_model = storage.load_model(model_filename)
                if stored_model:
                    training_analysis = stored_model.get('gemini_training_analysis')
                    course_profiles = stored_model.get('course_profiles', {})
                    model_metadata = {
                        'model_filename': model_filename,
                        'data_format': stored_model.get('data_format'),
                        'training_type': stored_model.get('training_type'),
                        'performance_metrics': stored_model.get('performance_metrics')
                    }
                    logger.info(f"Loaded model context: {model_filename}")
            except Exception as model_exc:
                logger.warning(f"Unable to load model for Gemini context: {model_exc}")
        else:
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏ö‡∏∏ model_filename ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
            try:
                models = storage.list_models()
                if models:
                    latest_model = models[0]  # ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
                    loaded_model = storage.load_model(latest_model['filename'])
                    if loaded_model:
                        course_profiles = loaded_model.get('course_profiles', {})
                        logger.info(f"Loaded course profiles from latest model: {latest_model['filename']}")
            except Exception as load_exc:
                logger.warning(f"Unable to load latest model for course profiles: {load_exc}")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö Course DNA
        if course_profiles:
            student_risk_factors = []
            student_easy_courses_failed = []
            
            for course_id, grade in cleaned_grades.items():
                if course_id in course_profiles:
                    prof = course_profiles[course_id]
                    fail_rate = prof.get('fail_rate', 0)
                    avg_grade = prof.get('avg_grade', 0)
                    difficulty_score = prof.get('difficulty_score', 0)
                    
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Killer Course ‡πÅ‡∏•‡∏∞‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ï‡∏Å
                    is_killer = fail_rate >= 0.3  # 30% fail rate threshold
                    is_failed = grade in ['F', 'W', 'WF', 'WU', 'D'] or (isinstance(grade, str) and grade.upper() in ['F', 'W', 'WF', 'WU', 'D'])
                    
                    if is_killer and is_failed:
                        student_risk_factors.append(
                            f"- ‡∏ï‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤‡∏õ‡∏£‡∏≤‡∏ö‡πÄ‡∏ã‡∏µ‡∏¢‡∏ô {course_id} (‡∏õ‡∏Å‡∏ï‡∏¥‡∏Ñ‡∏ô‡∏ï‡∏Å {fail_rate*100:.0f}%, ‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢: {avg_grade:.2f})"
                        )
                    elif is_killer and not is_failed:
                        # ‡∏ú‡πà‡∏≤‡∏ô Killer Course ‡πÅ‡∏•‡πâ‡∏ß (‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì‡∏î‡∏µ)
                        student_risk_factors.append(
                            f"- ‡∏ú‡πà‡∏≤‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏õ‡∏£‡∏≤‡∏ö‡πÄ‡∏ã‡∏µ‡∏¢‡∏ô {course_id} ‡πÅ‡∏•‡πâ‡∏ß (‡∏õ‡∏Å‡∏ï‡∏¥‡∏Ñ‡∏ô‡∏ï‡∏Å {fail_rate*100:.0f}%) ‚úì"
                        )
                    
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Easy Course ‡πÅ‡∏ï‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏Å‡∏£‡∏î‡πÅ‡∏¢‡πà
                    is_easy = avg_grade >= 3.5 and fail_rate <= 0.1
                    is_poor_grade = grade in ['C', 'D', 'F', 'W', 'WF', 'WU'] or (isinstance(grade, str) and grade.upper() in ['C', 'D', 'F', 'W', 'WF', 'WU'])
                    
                    if is_easy and is_poor_grade:
                        student_easy_courses_failed.append(
                            f"- ‡πÑ‡∏î‡πâ‡πÄ‡∏Å‡∏£‡∏î‡∏ô‡πâ‡∏≠‡∏¢‡πÉ‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ä‡πà‡∏ß‡∏¢ {course_id} (‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏õ‡∏Å‡∏ï‡∏¥: {avg_grade:.2f}, ‡πÑ‡∏î‡πâ: {grade})"
                        )
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° Course Context
            if student_risk_factors or student_easy_courses_failed:
                course_context_str = "\n**‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ï‡∏≤‡∏° Course DNA (Backend System):**\n"
                if student_risk_factors:
                    course_context_str += "\n".join(student_risk_factors)
                if student_easy_courses_failed:
                    course_context_str += "\n" + "\n".join(student_easy_courses_failed)
                course_context_str += "\n"
    except Exception as context_exc:
        logger.warning(f"Error building course context: {context_exc}")
        course_context_str = ""
    
    try:
        logger.info(f"üîÆ Starting Gemini prediction for {student_name} with {len(cleaned_grades)} courses")
        
        grade_summary = summarize_grades_for_gemini(cleaned_grades, loaded_terms_count)
        grade_summary['student_name'] = student_name
        
        # =========================================================
        # üìù AUTO-PROMPT GENERATION: ‡∏™‡∏£‡πâ‡∏≤‡∏á Prompt ‡πÉ‡∏´‡πâ User ‡πÄ‡∏≠‡∏á
        # =========================================================
        
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Goal ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ ‡∏ñ‡πâ‡∏≤ User ‡πÑ‡∏°‡πà‡∏û‡∏¥‡∏°‡∏û‡πå‡∏°‡∏≤
        if not analysis_goal:
            analysis_goal = "‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÅ‡∏•‡∏∞‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î"
        
        analysis_goal_text = analysis_goal
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Detailed Prompt ‡∏ó‡∏µ‡πà‡∏â‡∏•‡∏≤‡∏î‡∏Ç‡∏∂‡πâ‡∏ô - ‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏à‡∏ö/‡πÑ‡∏°‡πà‡∏à‡∏ö
        detailed_prompt = f"""
‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏ó‡∏≤‡∏á‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£ ‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà**‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢**‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏Ç‡∏≠‡∏á‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤

**‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤:**
- ‡∏ä‡∏∑‡πà‡∏≠: {student_name}
- ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÅ‡∏•‡πâ‡∏ß: {grade_summary.get('total_courses', 0)} ‡∏ß‡∏¥‡∏ä‡∏≤
- ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ó‡∏≠‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô: {loaded_terms_count} ‡πÄ‡∏ó‡∏≠‡∏°
- ‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ (‡πÇ‡∏î‡∏¢‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì): {grade_summary.get('estimated_gpa', 0):.2f}
- ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÅ‡∏•‡πâ‡∏ß: {grade_summary.get('total_credits_recorded', 0)} ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï
- ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏ö‡∏ï‡∏Å: {grade_summary.get('failed_count', 0)} ‡∏ß‡∏¥‡∏ä‡∏≤

**‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÄ‡∏Å‡∏£‡∏î:**
{json.dumps(grade_summary.get('grade_distribution', {}), ensure_ascii=False, indent=2)}

**‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ß‡∏¥‡∏ä‡∏≤:**
{json.dumps(grade_summary.get('course_details', [])[:20], ensure_ascii=False, indent=2)}

**‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏ö‡∏ï‡∏Å (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ):**
{json.dumps(grade_summary.get('failed_courses', []), ensure_ascii=False, indent=2)}

{course_context_str}
(‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≤‡∏á‡∏ö‡∏ô‡∏Ñ‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏£‡∏∏‡πà‡∏ô‡∏û‡∏µ‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß)

**‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì (‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î):**

**1. ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤ (‡∏ï‡πâ‡∏≠‡∏á‡∏ü‡∏±‡∏ô‡∏ò‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô):**
   - ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ß‡πà‡∏≤‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏Ñ‡∏ô‡∏ô‡∏µ‡πâ "‡∏à‡∏∞‡∏à‡∏ö‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå" ‡∏´‡∏£‡∏∑‡∏≠ "‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏à‡∏ö/‡∏à‡∏ö‡∏ä‡πâ‡∏≤"
   - ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏à‡∏ö: GPA >= 2.00 ‡πÅ‡∏•‡∏∞‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï >= 136 ‡πÅ‡∏•‡∏∞‡∏ú‡πà‡∏≤‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö/‡∏ß‡∏¥‡∏ä‡∏≤‡∏õ‡∏£‡∏≤‡∏ö‡πÄ‡∏ã‡∏µ‡∏¢‡∏ô
   - ‡∏£‡∏∞‡∏ö‡∏∏‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå (0-100%)

**2. ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÑ‡∏°‡∏ñ‡∏∂‡∏á‡∏à‡∏ö (‡∏ñ‡πâ‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ß‡πà‡∏≤‡∏à‡∏ö):**
   - ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÑ‡∏°‡∏ñ‡∏∂‡∏á‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ß‡πà‡∏≤‡∏à‡∏ö
   - ‡πÄ‡∏ä‡πà‡∏ô: "GPA ‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå (2.15 >= 2.00), ‡∏ú‡πà‡∏≤‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏õ‡∏£‡∏≤‡∏ö‡πÄ‡∏ã‡∏µ‡∏¢‡∏ô‡πÅ‡∏•‡πâ‡∏ß, ‡∏°‡∏µ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏î‡∏µ"
   - ‡∏£‡∏∞‡∏ö‡∏∏‡∏õ‡∏±‡∏à‡∏à‡∏±‡∏¢‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏à‡∏ö‡πÑ‡∏î‡πâ

**3. ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÑ‡∏°‡∏ñ‡∏∂‡∏á‡πÑ‡∏°‡πà‡∏à‡∏ö (‡∏ñ‡πâ‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏à‡∏ö):**
   - ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÑ‡∏°‡∏ñ‡∏∂‡∏á‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏à‡∏ö
   - ‡πÄ‡∏ä‡πà‡∏ô: "GPA ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏ì‡∏ë‡πå (1.85 < 2.00), ‡∏ï‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤‡∏õ‡∏£‡∏≤‡∏ö‡πÄ‡∏ã‡∏µ‡∏¢‡∏ô X, Y, Z"
   - ‡∏£‡∏∞‡∏ö‡∏∏‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÑ‡∏°‡πà‡∏à‡∏ö (GPA ‡∏ï‡πà‡∏≥, ‡∏ï‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤ Killer, ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö)

**4. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å:**
   - ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏õ‡∏±‡∏à‡∏à‡∏±‡∏¢‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏ú‡∏•‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
   - ‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö "‡∏ß‡∏¥‡∏ä‡∏≤‡∏õ‡∏£‡∏≤‡∏ö‡πÄ‡∏ã‡∏µ‡∏¢‡∏ô" (Killer Courses) ‡∏ó‡∏µ‡πà‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ï‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å
   - ‡∏ñ‡πâ‡∏≤‡πÑ‡∏î‡πâ‡πÄ‡∏Å‡∏£‡∏î‡πÅ‡∏¢‡πà‡πÉ‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏á‡πà‡∏≤‡∏¢ (Easy Courses) ‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô

**5. ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:**
   - ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡πÉ‡∏ô‡πÄ‡∏ó‡∏≠‡∏°‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏≠‡∏î
   - ‡∏ñ‡πâ‡∏≤‡∏ï‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤ Killer ‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏°‡∏Ç‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡∏ò‡∏£‡∏£‡∏°
   - ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏à‡∏ö‡πÇ‡∏î‡∏¢‡∏î‡∏π‡∏ß‡πà‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡∏î‡πà‡∏≤‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏¢‡∏≤‡∏Å‡πÜ ‡πÑ‡∏õ‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á

**‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:**
- **‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏à‡∏ö/‡πÑ‡∏°‡πà‡∏à‡∏ö‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô** (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÅ‡∏Ñ‡πà‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå)
- **‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢** ‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÑ‡∏°‡∏ñ‡∏∂‡∏á‡∏à‡∏ö/‡πÑ‡∏°‡πà‡∏à‡∏ö
- ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°
- ‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö Course DNA ‡πÅ‡∏•‡∏∞ Killer Courses
- ‡∏ï‡∏≠‡∏ö‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö JSON ‡∏ï‡∏≤‡∏° Schema ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î

‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î ‡πÇ‡∏î‡∏¢‡πÉ‡∏´‡πâ:
- graduation_prediction: 
  * will_graduate: true/false (‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ß‡πà‡∏≤‡∏à‡∏ö‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏à‡∏ö)
  * prediction_text: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ (‡πÄ‡∏ä‡πà‡∏ô "‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏à‡∏ö‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå" ‡∏´‡∏£‡∏∑‡∏≠ "‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤")
  * reason_why_graduate: ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÑ‡∏°‡∏ñ‡∏∂‡∏á‡∏à‡∏ö (‡∏ñ‡πâ‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ß‡πà‡∏≤‡∏à‡∏ö) ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
  * reason_why_not_graduate: ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÑ‡∏°‡∏ñ‡∏∂‡∏á‡πÑ‡∏°‡πà‡∏à‡∏ö (‡∏ñ‡πâ‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏à‡∏ö) ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
  * confidence_percent: ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (0-100)
- analysis_markdown: ‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ (‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 200 ‡∏Ñ‡∏≥) ‡πÇ‡∏î‡∏¢‡πÄ‡∏ô‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•
- risk_level: ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á (very_low, low, moderate, high, very_high)
- outcome_summary: ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (status: "graduated" ‡∏´‡∏£‡∏∑‡∏≠ "not_graduated", confidence: 0-1, description: ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢)
- key_metrics: ‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 3 ‡∏ï‡∏±‡∏ß) ‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Killer Courses ‡∏ó‡∏µ‡πà‡∏ï‡∏Å
- recommendations: ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ (‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 3 ‡∏Ç‡πâ‡∏≠) ‡πÇ‡∏î‡∏¢‡πÄ‡∏ô‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Killer Courses
"""
        
        prompt_payload = {
            'student_name': student_name,
            'analysis_goal': analysis_goal_text,
            'grade_summary': grade_summary,
            'detailed_prompt': detailed_prompt  # ‡πÄ‡∏û‡∏¥‡πà‡∏° detailed_prompt
        }
        
        logger.info("üì§ Calling Gemini API for prediction...")
        try:
            # ‡πÉ‡∏ä‡πâ function ‡∏ó‡∏µ‡πà‡∏°‡∏µ retry ‡πÅ‡∏ó‡∏ô (‡∏•‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î 3 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á)
            gemini_output = call_gemini_with_retry(prompt_payload, 'live_grade_prediction')
            logger.info("‚úÖ Gemini prediction completed successfully")
        except (ValueError, RuntimeError) as gemini_error:
            # ‡∏ñ‡πâ‡∏≤ Gemini API ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß ‡πÉ‡∏´‡πâ‡∏™‡πà‡∏á error message ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
            logger.error(f"‚ùå Gemini API error: {gemini_error}")
            return jsonify({
                'success': False,
                'error': f'‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Gemini API ‡πÑ‡∏î‡πâ: {str(gemini_error)}',
                'details': str(gemini_error),
                'suggestion': '‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ GEMINI_API_KEY ‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á'
            }), 400
        
        return jsonify({
            'success': True,
            'source': 'live_grades',
            'analysis': grade_summary,
            'gemini': gemini_output,
            'training_analysis': training_analysis,
            'model_metadata': model_metadata
        })
    except Exception as exc:
        error_msg = str(exc)
        
        # Check for quota/rate limit errors
        if '429' in error_msg or 'quota' in error_msg.lower() or 'resource exhausted' in error_msg.lower():
            logger.error(f"‚ùå Gemini quota exceeded: {error_msg}")
            return jsonify({
                'success': False,
                'error': '‚ö†Ô∏è ‡πÇ‡∏Ñ‡∏ß‡∏ï‡πâ‡∏≤ Gemini API ‡∏´‡∏°‡∏î‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß',
                'details': 'Free tier: 15 requests/minute, 1,500 requests/day',
                'suggestion': '‡∏£‡∏≠ 1-2 ‡∏ô‡∏≤‡∏ó‡∏µ ‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏±‡∏û‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏õ‡πá‡∏ô Paid Tier',
                'retry_after': 60
            }), 429
        
        logger.error(f"‚ùå Gemini prediction error: {exc}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({
            'success': False, 
            'error': f'‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {error_msg}',
            'suggestion': '‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á'
        }), 500

@app.route('/upload', methods=['POST'])
def upload_file():
    """Handles file upload and basic data format detection."""
    try:
        logger.info("Starting file upload process")
        
        if 'file' not in request.files:
            logger.warning("No file in request")
            return jsonify({'success': False, 'error': '‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡∏Ñ‡∏≥‡∏Ç‡∏≠'})

        file = request.files['file']
        if file.filename == '':
            logger.warning("No file selected")
            return jsonify({'success': False, 'error': '‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå'})

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•‡πÑ‡∏ü‡∏•‡πå
        if not file.filename.lower().endswith(('.csv', '.xlsx', '.xls')):
            logger.warning(f"Invalid file extension")
            return jsonify({'success': False, 'error': '‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÑ‡∏ü‡∏•‡πå .csv, .xlsx, .xls ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô'})

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á filename ‡∏ó‡∏µ‡πà‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
        from werkzeug.utils import secure_filename
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ‡πÄ‡∏Å‡πá‡∏ö extension ‡∏Å‡πà‡∏≠‡∏ô ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ secure_filename ‡∏≠‡∏≤‡∏à‡∏•‡∏ö‡∏à‡∏∏‡∏î‡∏≠‡∏≠‡∏Å
        original_filename = file.filename
        file_ext = ''
        if '.' in original_filename:
            file_ext = '.' + original_filename.rsplit('.', 1)[1].lower()
        
        safe_filename = secure_filename(file.filename)
        # ‡∏ñ‡πâ‡∏≤ secure_filename ‡∏•‡∏ö extension ‡∏≠‡∏≠‡∏Å ‡πÉ‡∏´‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏•‡∏±‡∏ö
        if file_ext and not safe_filename.lower().endswith(file_ext):
            filename_on_disk = f"{timestamp}_{safe_filename}{file_ext}"
        else:
            filename_on_disk = f"{timestamp}_{safe_filename}"
        
        # ‡πÉ‡∏ä‡πâ absolute path
        upload_folder = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'uploads')
        if not os.path.exists(upload_folder):
            os.makedirs(upload_folder, exist_ok=True)
            
        filepath = os.path.join(upload_folder, filename_on_disk)
        
        logger.info(f"Saving file to: {filepath}")
        file.save(filepath)
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏à‡∏£‡∏¥‡∏á
        if not os.path.exists(filepath):
            raise ValueError("‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å")
            
        logger.info(f"File saved successfully: {filename_on_disk}")

        # ‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå
        try:
            df = None
            if safe_filename.lower().endswith('.csv'):
                # ‡∏•‡∏≠‡∏á‡∏´‡∏•‡∏≤‡∏¢ encoding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå CSV
                encodings = ['utf-8-sig', 'utf-8', 'cp874', 'tis-620', 'iso-8859-1']
                for encoding in encodings:
                    try:
                        df = pd.read_csv(filepath, encoding=encoding)
                        logger.info(f"Successfully read CSV with encoding: {encoding}")
                        break
                    except Exception as e:
                        logger.debug(f"Failed with {encoding}: {e}")
                        continue
                        
                if df is None:
                    # ‡∏ñ‡πâ‡∏≤‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ engine python
                    try:
                        df = pd.read_csv(filepath, encoding='utf-8', engine='python')
                    except:
                        df = pd.read_csv(filepath, encoding='cp874', engine='python')
                        
            else:  # Excel files
                df = pd.read_excel(filepath, engine='openpyxl')
                logger.info(f"Successfully read Excel file")

            if df is None or df.empty:
                os.remove(filepath)
                raise ValueError("‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏î‡πâ ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤")

            data_format = detect_data_format(df)
            logger.info(f"Detected data format: {data_format}")

            response_data = {
                'success': True,
                'filename': filename_on_disk,
                'rows': len(df),
                'columns': len(df.columns),
                'data_format': data_format,
                'sample_columns': df.columns.tolist()[:10]
            }
            
            logger.info(f"Upload successful, returning: {response_data}")
            return jsonify(response_data)

        except Exception as e:
            if os.path.exists(filepath):
                os.remove(filepath)
                logger.info(f"Removed invalid file: {filepath}")
            logger.error(f"Error processing file: {str(e)}")
            return jsonify({'success': False, 'error': f'‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏î‡πâ: {str(e)}'})

    except Exception as e:
        logger.error(f"Upload error: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': f'‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î: {str(e)}'})

# Keep all other routes unchanged...
@app.route('/analyze', methods=['POST'])
def analyze_subjects():
    """Analyzes subjects from a CSV/Excel file (for Subject-based data)."""
    try:
        data = request.get_json()
        filename = data.get('filename')

        if not filename:
            return jsonify({'success': False, 'error': 'No filename provided.'})

        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        if not os.path.exists(filepath):
            return jsonify({'success': False, 'error': 'Specified file not found.'})

        file_extension = filename.rsplit('.', 1)[1].lower()
        df = None
        if file_extension == 'csv':
            encodings = app.config['DATA_CONFIG']['fallback_encodings']
            for encoding in encodings:
                try:
                    df = pd.read_csv(filepath, encoding=encoding)
                    break
                except Exception as e:
                    logger.debug(f"Failed to read CSV with {encoding}: {e}")
                    continue
            if df is None:
                raise ValueError("Could not read CSV file with any supported encoding.")
        elif file_extension in ['xlsx', 'xls']:
            df = pd.read_excel(filepath)
        else:
            raise ValueError("Unsupported file type for analysis.")

        if df is None:
            return jsonify({'success': False, 'error': 'Could not read file.'})

        data_format = detect_data_format(df)

        if data_format != 'subject_based':
            return jsonify({'success': False, 'error': 'Only subject-based data is supported for this analysis.'})

        name_col = None
        possible_name_cols = ['‡∏ä‡∏∑‡πà‡∏≠-‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•', '‡∏ä‡∏∑‡πà‡∏≠', '‡∏ä‡∏∑‡πà‡∏≠‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤', 'name', 'student_name', '‡∏£‡∏´‡∏±‡∏™', 'student_id']
        for col in possible_name_cols:
            if col in df.columns:
                name_col = col
                break
        if not name_col:
            name_candidates = [col for col in df.columns if '‡∏ä‡∏∑‡πà‡∏≠' in col.lower() or '‡∏£‡∏´‡∏±‡∏™' in col.lower() or 'id' in col.lower()]
            if name_candidates:
                name_col = name_candidates[0]
            else:
                return jsonify({'success': False, 'error': 'Could not find student name or ID column.'})

        exclude_cols = [name_col, '‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤', '‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏à‡∏ö', 'year_in', 'year_out']
        exclude_keywords = ['gpa', '‡πÄ‡∏Å‡∏£‡∏î', '‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à', '‡∏à‡∏ö', 'success', 'graduated', 'status']
        subject_cols = [
            col for col in df.columns
            if col not in exclude_cols and not any(kw in col.lower() for kw in exclude_keywords)
        ]

        logger.info(f"Analyzing {len(subject_cols)} subjects.")

        subject_analysis = {}
        all_gpas = []
        all_fail_rates = []
        subject_stats = []

        for subject in subject_cols:
            grades = []
            for _, row in df.iterrows():
                grade = row[subject]
                if pd.notna(grade) and str(grade).strip():
                    numeric_grade = grade_to_numeric(grade)
                    grades.append(numeric_grade)

            if grades:
                category = categorize_subject(subject)
                if category not in subject_analysis:
                    subject_analysis[category] = {}

                avg_grade = np.mean(grades)
                min_grade = np.min(grades)
                max_grade = np.max(grades)
                std_grade = np.std(grades) if len(grades) > 1 else 0
                fail_count = sum(1 for g in grades if g <= app.config['DATA_CONFIG']['grade_mapping'].get('D', 1.0) - 0.01)
                fail_rate = fail_count / len(grades)
                pass_rate = 1 - fail_rate

                grade_distribution = {}
                fail_grade_chars = [k for k, v in app.config['DATA_CONFIG']['grade_mapping'].items() if v == 0.0]
                fail_char_for_display = 'F/W/I/NP' if 'F' in fail_grade_chars else '0.0'

                for grade_point_val in sorted(list(set(app.config['DATA_CONFIG']['grade_mapping'].values())), reverse=True):
                    count = sum(1 for g in grades if g == grade_point_val)
                    if count > 0:
                        if grade_point_val == 0.0:
                            grade_distribution[fail_char_for_display] = grade_distribution.get(fail_char_for_display, 0) + count
                        else:
                            char_grade = next((k for k, v in app.config['DATA_CONFIG']['grade_mapping'].items() if v == grade_point_val and k not in fail_grade_chars), str(grade_point_val))
                            grade_distribution[char_grade] = count

                subject_info = {
                    'average': avg_grade,
                    'minimum': min_grade,
                    'maximum': max_grade,
                    'std_dev': std_grade,
                    'fail_rate': fail_rate,
                    'pass_rate': pass_rate,
                    'num_students': len(grades),
                    'num_failed': fail_count,
                    'grade_distribution': grade_distribution
                }

                subject_analysis[category][subject] = subject_info
                subject_stats.append({
                    'subject': subject,
                    'category': category,
                    'average': avg_grade,
                    'fail_rate': fail_rate,
                    'num_students': len(grades)
                })

                all_gpas.append(avg_grade)
                all_fail_rates.append(fail_rate)

        overall_stats = {
            'total_students': len(df),
            'total_subjects': len(subject_cols),
            'avg_gpa': np.mean(all_gpas) if all_gpas else 0,
            'overall_fail_rate': np.mean(all_fail_rates) if all_fail_rates else 0,
            'max_gpa_subject': np.max(all_gpas) if all_gpas else 0,
            'min_gpa_subject': np.min(all_gpas) if all_gpas else 0
        }

        high_fail_subjects = []
        low_gpa_subjects = []
        excellent_subjects = []

        high_fail_rate_threshold = app.config['DATA_CONFIG']['risk_levels']['high_fail_rate_threshold']
        low_gpa_threshold = app.config['DATA_CONFIG']['risk_levels']['low_gpa_threshold']

        for stat in subject_stats:
            if stat['fail_rate'] > high_fail_rate_threshold:
                high_fail_subjects.append(stat)
            if stat['average'] < low_gpa_threshold:
                low_gpa_subjects.append(stat)
            if stat['average'] >= 3.5 and stat['fail_rate'] < 0.1:
                excellent_subjects.append(stat)

        high_fail_subjects.sort(key=lambda x: x['fail_rate'], reverse=True)
        low_gpa_subjects.sort(key=lambda x: x['average'])
        excellent_subjects.sort(key=lambda x: x['average'], reverse=True)

        recommendations = []
        overall_fail_rate_warning = app.config['DATA_CONFIG']['risk_levels']['medium_fail_rate_threshold']
        overall_fail_rate_high = app.config['DATA_CONFIG']['risk_levels']['high_fail_rate_threshold']

        if overall_stats['overall_fail_rate'] > overall_fail_rate_high:
            recommendations.append(f"‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡∏ï‡∏Å‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å (> {(overall_fail_rate_high * 100):.0f}%) ‡∏Ñ‡∏ß‡∏£‡∏ó‡∏ö‡∏ó‡∏ß‡∏ô‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î")
        elif overall_stats['overall_fail_rate'] > overall_fail_rate_warning:
            recommendations.append(f"‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡∏ï‡∏Å‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°‡∏Ñ‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡∏™‡∏π‡∏á (> {(overall_fail_rate_warning * 100):.0f}%) ‡∏Ñ‡∏ß‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ô")

        if high_fail_subjects:
            top_problem_subjects = [s['subject'] for s in high_fail_subjects[:3]]
            recommendations.append(f"‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏ï‡∏Å‡∏™‡∏π‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î: {', '.join(top_problem_subjects)}")
            recommendations.append("‡∏Ñ‡∏ß‡∏£‡∏ó‡∏ö‡∏ó‡∏ß‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ô ‡πÅ‡∏•‡∏∞‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÉ‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ")

        if low_gpa_subjects:
            recommendations.append(f"‡∏û‡∏ö {len(low_gpa_subjects)} ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ {low_gpa_threshold}")
            recommendations.append("‡∏Ñ‡∏ß‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏£‡∏±‡∏ö‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å‡∏Ç‡∏≠‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤")

        if excellent_subjects:
            recommendations.append(f"‡∏û‡∏ö {len(excellent_subjects)} ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏î‡∏µ‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏° ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ô‡∏≥‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ö‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á")

        category_summary = {}
        for category, subjects in subject_analysis.items():
            if subjects:
                avg_gpas = [s['average'] for s in subjects.values()]
                avg_fail_rates = [s['fail_rate'] for s in subjects.values()]
                total_students_in_category = sum(s['num_students'] for s in subjects.values())

                category_summary[category] = {
                    'num_subjects': len(subjects),
                    'avg_gpa': np.mean(avg_gpas),
                    'avg_fail_rate': np.mean(avg_fail_rates),
                    'total_students_in_category': total_students_in_category,
                    'hardest_subject': min(subjects.items(), key=lambda x: x[1]['average'])[0] if subjects else None,
                    'easiest_subject': max(subjects.items(), key=lambda x: x[1]['average'])[0] if subjects else None
                }

        logger.info("Subject analysis successful.")

        return jsonify({
            'success': True,
            'subject_analysis': subject_analysis,
            'overall_stats': overall_stats,
            'category_summary': category_summary,
            'problem_subjects': {
                'high_fail_rate': high_fail_subjects[:10],
                'low_gpa': low_gpa_subjects[:10]
            },
            'excellent_subjects': excellent_subjects[:10],
            'recommendations': recommendations
        })

    except Exception as e:
        logger.error(f"Error during subject analysis: {str(e)}")
        return jsonify({'success': False, 'error': f'An error occurred during analysis: {str(e)}'})



@app.route('/api/analyze_curriculum', methods=['POST'])
def analyze_curriculum():
    """
    ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤
    ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
    """
    try:
        data = request.get_json()
        current_grades = data.get('current_grades', {})
        loaded_terms_count = data.get('loaded_terms_count', 0)
        repeated_courses_in_this_term_ids = data.get('repeated_courses_in_this_term_ids', [])
        model_filename = data.get('model_filename')
        student_name = data.get('student_name', '‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤')

        logger.info(f"üîÆ Analyzing curriculum for {student_name} with {len(current_grades)} grades")
        logger.info(f"üìä Grade values sent for analysis: {list(current_grades.keys())[:10]}...")  # Log first 10 courses
        
        # Get configuration
        courses_data = app.config['COURSES_DATA']
        all_terms_data = app.config['ALL_TERMS_DATA']
        grade_mapping = app.config['DATA_CONFIG']['grade_mapping']
        
        # Get loaded courses
        loaded_courses_ids = []
        for i in range(loaded_terms_count):
            if i < len(all_terms_data):
                loaded_courses_ids.extend(all_terms_data[i]['ids'])
        loaded_courses_ids.extend(repeated_courses_in_this_term_ids)
        loaded_courses_ids = list(set(loaded_courses_ids))  # Remove duplicates
        
        # Calculate GPA and failed courses
        total_points = 0
        total_credits = 0
        completed_credits = 0
        failed_courses_ids = []
        incomplete_courses_ids = []
        
        for course_id in loaded_courses_ids:
            course = next((c for c in courses_data if c['id'] == course_id), None)
            if not course:
                continue
            
            grade = current_grades.get(course_id, '')
            
            if not grade:
                incomplete_courses_ids.append(course_id)
            elif grade:
                grade_point = grade_mapping.get(grade, 0)
                
                if grade_point > 0:
                    total_points += grade_point * course['credit']
                    total_credits += course['credit']
                    completed_credits += course['credit']
                elif grade == 'F':
                    failed_courses_ids.append(course_id)
                    total_credits += course['credit']  # F still counts for GPA calculation
        
        avg_gpa = total_points / total_credits if total_credits > 0 else 0
        
        # Check for blocked courses
        blocked_courses = find_all_blocked_courses(
            current_grades, loaded_courses_ids, courses_data, grade_mapping
        )
        
        # Build dependency graph and find impact chains
        dependency_graph = build_course_dependency_graph(loaded_courses_ids, courses_data)
        
        blocked_chain_texts = []
        for failed_id in failed_courses_ids:
            affected = find_courses_affected_by_failure(failed_id, dependency_graph)
            if affected:
                failed_course = next((c for c in courses_data if c['id'] == failed_id), None)
                affected_names = []
                for aid in affected:
                    course = next((c for c in courses_data if c['id'] == aid), None)
                    if course:
                        affected_names.append(f"{course['thaiName']} ({aid})")
                
                if affected_names:
                    chain_text = f"‡∏ß‡∏¥‡∏ä‡∏≤ {failed_course['thaiName']} ({failed_id}) ‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡πà‡∏≠‡∏ß‡∏¥‡∏ä‡∏≤‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:\n"
                    chain_text += "\n".join([f"  - {name}" for name in affected_names])
                    blocked_chain_texts.append(chain_text)
        
        # Calculate completion rate
        total_required_credits = sum(c['credit'] for c in courses_data)
        completion_rate = (completed_credits / total_required_credits * 100) if total_required_credits > 0 else 0
        
        # Determine graduation status
        graduation_status = determine_graduation_status_backend(
            completion_rate, avg_gpa, blocked_courses, failed_courses_ids,
            loaded_courses_ids, current_grades, all_terms_data, courses_data, loaded_terms_count
        )
        
        # Generate recommendations
        recommendations = []
        if failed_courses_ids:
            recommendations.append("‡∏Ñ‡∏ß‡∏£‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ã‡πâ‡∏≥‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏Å (F) ‡πÇ‡∏î‡∏¢‡πÄ‡∏£‡πá‡∏ß")
            recommendations.append("‡∏Ñ‡∏ß‡∏£‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPA ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (‡∏ï‡πâ‡∏≠‡∏á >= 2.00 ‡∏ñ‡∏∂‡∏á‡∏à‡∏∞‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå)
        gpa_threshold = 2.00
        if avg_gpa > 0 and avg_gpa < gpa_threshold:
            recommendations.append(f"GPA ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏ì‡∏ë‡πå ({avg_gpa:.2f} < {gpa_threshold:.2f}) ‡∏Ñ‡∏ß‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏£‡πà‡∏á‡∏î‡πà‡∏ß‡∏ô")
        
        if blocked_courses:
            recommendations.append("‡∏Ñ‡∏ß‡∏£‡∏ú‡πà‡∏≤‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡∏•‡∏á‡∏ß‡∏¥‡∏ä‡∏≤‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á")
        
        if not recommendations:
            recommendations.append("‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏î‡∏µ ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡πÉ‡∏´‡πâ‡∏à‡∏ö‡∏ï‡∏≤‡∏°‡∏Å‡∏≥‡∏´‡∏ô‡∏î")
        
        # Prepare response
        response_data = {
            'success': True,
            'student_name': student_name,
            'completion_rate': completion_rate,
            'avg_gpa': avg_gpa,
            'completed_credits': completed_credits,
            'total_required_credits': total_required_credits,
            'graduation_status': graduation_status,
            'incomplete_courses': [
                next((c['thaiName'] for c in courses_data if c['id'] == cid), cid)
                for cid in incomplete_courses_ids
            ],
            'failed_courses': [
                next((c['thaiName'] for c in courses_data if c['id'] == cid), cid)
                for cid in failed_courses_ids
            ],
            'blocked_courses_details': blocked_courses,
            'blocked_chain_texts': blocked_chain_texts,
            'recommendations': recommendations,
            'debug': {
                'current_grades_count': len(current_grades),
                'loaded_terms_count': loaded_terms_count,
                'loaded_courses_count': len(loaded_courses_ids),
                'blocked_courses_count': len(blocked_courses)
            }
        }
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
        def check_curriculum_completion(current_grades, courses_data, min_credits=136, min_gpa=2.0):
            """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏à‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á"""
            total_credits = 0
            total_grade_points = 0
            total_credits_for_gpa = 0
            passed_courses = 0
            
            for course_id, grade in current_grades.items():
                if grade:  # ‡∏°‡∏µ‡πÄ‡∏Å‡∏£‡∏î
                    course = next((c for c in courses_data if c['id'] == course_id), None)
                    if course:
                        credit = course.get('credit', 3)
                        grade_point = grade_mapping.get(grade, 0)
                        
                        # ‡∏ô‡∏±‡∏ö‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà F, W, I)
                        if grade not in ['F', 'W', 'I']:
                            total_credits += credit
                            passed_courses += 1
                        
                        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA (‡∏£‡∏ß‡∏° F ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏° W, I)
                        if grade not in ['W', 'I']:
                            total_grade_points += grade_point * credit
                            total_credits_for_gpa += credit
            
            gpa = total_grade_points / total_credits_for_gpa if total_credits_for_gpa > 0 else 0
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏Å‡∏≤‡∏£‡∏à‡∏ö
            is_completed = total_credits >= min_credits and gpa >= min_gpa
            
            return {
                'is_completed': is_completed,
                'total_credits': total_credits,
                'gpa': gpa,
                'passed_courses': passed_courses,
                'min_credits_met': total_credits >= min_credits,
                'min_gpa_met': gpa >= min_gpa,
                'min_credits': min_credits,
                'min_gpa': min_gpa
            }
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£
        completion_status = check_curriculum_completion(current_grades, courses_data)
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÉ‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        response_data['completion_status'] = {
            'total_credits': completion_status['total_credits'],
            'min_credits': completion_status['min_credits'],
            'credits_remaining': max(0, completion_status['min_credits'] - completion_status['total_credits']),
            'gpa': completion_status['gpa'],
            'min_gpa': completion_status['min_gpa'],
            'is_completed': completion_status['is_completed'],
            'min_credits_met': completion_status['min_credits_met'],
            'min_gpa_met': completion_status['min_gpa_met']
        }
        
        # Try to predict if model is provided
        if model_filename:
            try:
                logger.info(f"Making prediction with model: {model_filename}")
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏à‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á
                if completion_status['is_completed']:
                    # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏£‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡πÅ‡∏•‡∏∞ GPA ‚â• 2.0 ‡πÅ‡∏•‡πâ‡∏ß ‚Üí ‡∏à‡∏ö‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô
                    response_data['prediction_result'] = {
                        'prediction': '‡∏à‡∏ö',
                        'prob_pass': 1.0,
                        'prob_fail': 0.0,
                        'confidence': 1.0,
                        'risk_level': '‡πÑ‡∏°‡πà‡∏°‡∏µ',
                        'gpa_input': float(completion_status['gpa']),
                        'reason': '‡πÉ‡∏™‡πà‡πÄ‡∏Å‡∏£‡∏î‡∏Ñ‡∏£‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡πÅ‡∏•‡∏∞ GPA ‚â• 2.00 ‡πÅ‡∏•‡πâ‡∏ß',
                        'method': 'Curriculum Completion Check',
                        'status': 'graduated'
                    }
                    logger.info(f"Curriculum completed: GPA {completion_status['gpa']:.2f}, Credits {completion_status['total_credits']}")
                    
                elif completion_status['min_credits_met'] and not completion_status['min_gpa_met']:
                    # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏£‡∏ö‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡πÅ‡∏ï‡πà GPA < 2.0 ‚Üí ‡πÑ‡∏°‡πà‡∏à‡∏ö
                    response_data['prediction_result'] = {
                        'prediction': '‡πÑ‡∏°‡πà‡∏à‡∏ö',
                        'prob_pass': 0.0,
                        'prob_fail': 1.0,
                        'confidence': 1.0,
                        'risk_level': '‡∏™‡∏π‡∏á',
                        'gpa_input': float(completion_status['gpa']),
                        'reason': f'GPA {completion_status["gpa"]:.2f} ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥ {completion_status["min_gpa"]:.2f}',
                        'method': 'Curriculum Completion Check',
                        'status': 'failed_gpa'
                    }
                    logger.info(f"Failed due to low GPA: {completion_status['gpa']:.2f} < {completion_status['min_gpa']}")
                    
                else:
                    # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£ ‚Üí ‡πÉ‡∏ä‡πâ AI ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
                    # Load model
                    loaded_model_data = storage.load_model(model_filename)
                    if loaded_model_data:
                        # ‡∏™‡∏£‡πâ‡∏≤‡∏á AdvancedFeatureEngineer ‡πÅ‡∏•‡∏∞ ContextAwarePredictor
                        engineer = AdvancedFeatureEngineer(
                            grade_mapping=app.config['DATA_CONFIG']['grade_mapping']
                        )
                        
                        # ‡πÇ‡∏´‡∏•‡∏î course_profiles ‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
                        course_profiles = loaded_model_data.get('course_profiles', {})
                        engineer.course_profiles = course_profiles
                        
                        # ======= ‡πÇ‡∏´‡∏•‡∏î models, scaler, feature_names ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏• =======
                        models = loaded_model_data.get('models', {})
                        scaler = loaded_model_data.get('scaler', None)
                        # feature_names ‡∏≠‡∏≤‡∏à‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô 'feature_columns' ‡∏´‡∏£‡∏∑‡∏≠ 'feature_names'
                        feature_names = loaded_model_data.get('feature_columns', loaded_model_data.get('feature_names', []))
                        
                        logger.info(f"üì¶ Model loaded: models={list(models.keys())}, features={len(feature_names)}, scaler={'‚úÖ' if scaler else '‚ùå'}")
                        
                        # ‡∏™‡∏£‡πâ‡∏≤‡∏á transcript data ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
                        transcript_data = []
                        for course_id, grade in current_grades.items():
                            if grade:  # ‡∏°‡∏µ‡πÄ‡∏Å‡∏£‡∏î
                                course = next((c for c in courses_data if c['id'] == course_id), None)
                                if course:
                                    # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏õ‡πá‡∏ô GRADE_POINT
                                    grade_point = grade_mapping.get(grade, 0)
                                    
                                    transcript_data.append({
                                        'Dummy StudentNO': student_name,
                                        'COURSE_CODE': course_id,
                                        'COURSE_TITLE_TH': course.get('thaiName', ''),
                                        'CREDIT': course.get('credit', 3),
                                        'GRADE': grade,
                                        'GRADE_POINT': grade_point,
                                        '‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤': 2020,  # ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
                                        '‡∏õ‡∏µ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤': 2024,  # ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
                                        '‡πÄ‡∏ó‡∏≠‡∏°': 1
                                    })
                        
                        if transcript_data:
                            # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô DataFrame
                            transcript_df = pd.DataFrame(transcript_data)
                            
                            # ‡πÉ‡∏ä‡πâ ContextAwarePredictor ‡∏û‡∏£‡πâ‡∏≠‡∏° models, scaler, feature_names ‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î
                            from advanced_training import ContextAwarePredictor
                            predictor = ContextAwarePredictor(
                                feature_engineer=engineer,
                                models=models,
                                scaler=scaler,
                                feature_names=feature_names
                            )
                            
                            # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ Context-Aware AI System
                            prediction_result = predictor.predict_graduation_probability(transcript_df)
                            
                            prob_pass = prediction_result['probability']
                            prob_fail = 1 - prob_pass
                            confidence = prediction_result['confidence']
                            
                            prediction = '‡∏à‡∏ö' if prob_pass >= 0.5 else '‡πÑ‡∏°‡πà‡∏à‡∏ö'
                            
                            risk_level = '‡∏™‡∏π‡∏á'
                            if confidence > 0.8:
                                risk_level = '‡∏ï‡πà‡∏≥' if prediction == '‡∏à‡∏ö' else '‡∏™‡∏π‡∏á'
                            elif confidence > 0.6:
                                risk_level = '‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á'
                            
                            # ‡∏î‡∏∂‡∏á prediction_method ‡πÅ‡∏•‡∏∞ models_used ‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
                            ai_method = prediction_result.get('prediction_method', 'AI_MODEL')
                            models_used = prediction_result.get('models_used', [])
                            feature_importance = prediction_result.get('feature_importance', {})
                            
                            response_data['prediction_result'] = {
                                'prediction': prediction,
                                'prob_pass': float(prob_pass),
                                'prob_fail': float(prob_fail),
                                'confidence': float(confidence),
                                'risk_level': risk_level,
                                'gpa_input': float(completion_status['gpa']),
                                'features_used': prediction_result['features_used'],
                                'courses_analyzed': prediction_result['courses_analyzed'],
                                'method': ai_method,  # AI_MODEL ‡∏´‡∏£‡∏∑‡∏≠ Context-Aware
                                'models_used': models_used,  # ['rf', 'gb', 'lr']
                                'feature_importance': feature_importance,  # Top 10 features
                                'status': 'predicted'
                            }
                            logger.info(f"ü§ñ AI Model Prediction: {prediction} (confidence: {confidence:.3f}, models: {models_used})")
                        else:
                            logger.warning("No valid transcript data for prediction")
                        
            except Exception as e:
                logger.error(f"Error during prediction: {str(e)}")
                import traceback
                logger.error(traceback.format_exc())
        
        # ‚ú® ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏£‡∏≤‡∏ü 3 ‡πÄ‡∏™‡πâ‡∏ô (‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤)
        try:
            chart_data = generate_three_line_chart_data(current_grades, loaded_terms_count)
            response_data['chart_data'] = chart_data
            logger.info(f"Added chart data with {len(chart_data.get('terms', []))} terms")
        except Exception as e:
            logger.error(f"Error generating chart data: {str(e)}")
            response_data['chart_data'] = None
        
        # ‚ú® ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡∏à‡∏ö - ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• AI ‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢
        try:
            # ‡∏î‡∏∂‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• AI (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
            ai_prediction_result = response_data.get('prediction_result', None)
            
            graduation_analysis = analyze_graduation_failure_reasons(
                current_grades, 
                loaded_terms_count, 
                prediction_result=ai_prediction_result  # ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• AI
            )
            response_data['graduation_analysis'] = graduation_analysis
            
            # Log ‡∏ß‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡πÑ‡∏´‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
            prediction_method = graduation_analysis.get('prediction_method', 'Unknown')
            will_graduate = graduation_analysis.get('will_graduate', False)
            logger.info(f"‚úÖ Graduation analysis: will_graduate={will_graduate}, method={prediction_method}, risk={graduation_analysis.get('risk_level', 'N/A')}")
        except Exception as e:
            logger.error(f"Error in graduation analysis: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            response_data['graduation_analysis'] = None
        
        return jsonify(response_data)
        
    except Exception as e:
        logger.error(f"Error during curriculum analysis: {str(e)}")
        return jsonify({'success': False, 'error': str(e)})


@app.route('/api/explain_prediction', methods=['POST'])
def explain_prediction():
    """
    API ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
    ‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏ä‡∏¥‡∏á‡∏£‡∏∏‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á
    """
    try:
        data = request.get_json()
        current_grades = data.get('current_grades', {})
        loaded_terms_count = data.get('loaded_terms_count', 0)
        student_name = data.get('student_name', '‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤')
        
        logger.info(f"Explaining prediction for {student_name}")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ advanced_trainer ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if not hasattr(app, 'advanced_trainer') or not app.advanced_trainer:
            return jsonify({
                'success': False,
                'error': '‡πÇ‡∏°‡πÄ‡∏î‡∏• AI ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡πÇ‡∏´‡∏•‡∏î ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏≠‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á'
            })
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á student_data DataFrame
        courses_data = app.config.get('COURSES_DATA', [])
        grade_mapping = app.config.get('DATA_CONFIG', {}).get('grade_mapping', {})
        
        student_data_rows = []
        for course_id, grade in current_grades.items():
            if grade and grade in grade_mapping:
                course = next((c for c in courses_data if c['id'] == course_id), None)
                if course:
                    student_data_rows.append({
                        'course_code': course_id,
                        'grade': grade,
                        'grade_point': grade_mapping[grade],
                        'credit': course.get('credit', 3),
                        'semester': 1,
                        'academic_year': 2024
                    })
        
        if not student_data_rows:
            return jsonify({
                'success': False,
                'error': '‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏£‡∏î‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á'
            })
        
        student_data = pd.DataFrame(student_data_rows)
        
        # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ Context-Aware Predictor ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢
        try:
            if hasattr(app.advanced_trainer, 'predictor'):
                prediction_result = app.advanced_trainer.predictor.predict_graduation_probability(
                    student_data, explain=True
                )
            else:
                # Fallback: ‡∏™‡∏£‡πâ‡∏≤‡∏á predictor ‡πÉ‡∏´‡∏°‡πà
                predictor = ContextAwarePredictor(
                    feature_engineer=app.advanced_trainer.feature_engineer,
                    models=app.advanced_trainer.models if hasattr(app.advanced_trainer, 'models') else {},
                    scaler=app.advanced_trainer.scaler if hasattr(app.advanced_trainer, 'scaler') else None,
                    feature_names=app.advanced_trainer.feature_names if hasattr(app.advanced_trainer, 'feature_names') else []
                )
                prediction_result = predictor.predict_graduation_probability(student_data, explain=True)
        except Exception as e:
            logger.error(f"Error in prediction: {e}")
            return jsonify({
                'success': False,
                'error': f'‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢: {str(e)}'
            })
        
        # ‡∏î‡∏∂‡∏á‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢
        explanation = prediction_result.get('explanation')
        
        if not explanation:
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö fallback
            probability = prediction_result.get('probability', 0.5)
            will_graduate = probability >= 0.5
            
            explanation = {
                'summary': {
                    'will_graduate': will_graduate,
                    'probability': round(probability * 100, 1),
                    'confidence': round(prediction_result.get('confidence', 0.5) * 100, 1),
                    'status': '‡∏°‡∏µ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤' if will_graduate else '‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏à‡∏ö'
                },
                'key_factors': [],
                'reasons': ['‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ'],
                'obstacles': [],
                'strengths': [],
                'recommendations': [],
                'graduation_path': {},
                'future_projections': {}
            }
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°
        total_credits = sum([row['credit'] for row in student_data_rows])
        gpa = sum([grade_mapping[row['grade']] * row['credit'] for row in student_data_rows]) / total_credits if total_credits > 0 else 0
        failed_count = sum([1 for row in student_data_rows if grade_mapping[row['grade']] == 0])
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
        response_data = {
            'success': True,
            'student_name': student_name,
            'current_status': {
                'gpa': round(gpa, 2),
                'total_credits': total_credits,
                'courses_taken': len(student_data_rows),
                'failed_courses': failed_count,
                'terms_completed': loaded_terms_count
            },
            'prediction': {
                'probability': prediction_result.get('probability', 0.5),
                'confidence': prediction_result.get('confidence', 0.5),
                'will_graduate': prediction_result.get('probability', 0.5) >= 0.5
            },
            'explanation': explanation,
            'factors': prediction_result.get('factors', {}),
            'timestamp': datetime.now().isoformat()
        }
        
        logger.info(f"‚úÖ Successfully explained prediction for {student_name}")
        return jsonify(response_data)
        
    except Exception as e:
        logger.error(f"Error in explain_prediction: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({
            'success': False,
            'error': f'‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}'
        })
    
    
@app.route('/model_status', methods=['GET'])
def model_status():
    """Checks the status of currently loaded models."""
    try:
        logger.info("Checking model status...")
        
        global models
        if models is None:
            models = {
                'subject_model': None,
                'gpa_model': None,
                'subject_model_info': None,
                'gpa_model_info': None,
                'subject_feature_cols': None,
                'gpa_feature_cols': None
            }

        subject_model_status = models.get('subject_model') is not None
        subject_info = models.get('subject_model_info')
        
        logger.info(f"Subject model status: {subject_model_status}")

        status = {
            'success': True,
            'subject_model': subject_model_status,
            'gpa_model': False,  # Always false since we're removing GPA model
            'subject_model_info': subject_info,
            'gpa_model_info': None,  # No GPA model info
            'server_time': datetime.now().isoformat(),
            'models_folder_exists': os.path.exists(app.config['MODEL_FOLDER']),
            'uploads_folder_exists': os.path.exists(app.config['UPLOAD_FOLDER']),
            's3_available': not storage.use_local
        }
        
        logger.info("Model status check completed successfully")
        return jsonify(status)

    except Exception as e:
        logger.error(f"Error in model_status: {str(e)}")
        return jsonify({
            'success': False,
            'subject_model': False,
            'gpa_model': False,
            'error': str(e),
            'server_time': datetime.now().isoformat()
        }), 500



@app.route('/api/sync-models', methods=['POST'])
def sync_local_models_to_storage():
    """Sync local models to cloud storage"""
    try:
        if storage.use_local:
            return jsonify({'success': False, 'error': 'Storage is in local mode'})
        
        synced = []
        model_folder = app.config['MODEL_FOLDER']
        
        if os.path.exists(model_folder):
            for filename in os.listdir(model_folder):
                if filename.endswith('.joblib'):
                    filepath = os.path.join(model_folder, filename)
                    try:
                        model_data = joblib.load(filepath)
                        if storage.save_model(model_data, filename):
                            synced.append(filename)
                            logger.info(f"Synced {filename} to storage")
                    except Exception as e:
                        logger.error(f"Could not sync {filename}: {e}")
        
        return jsonify({'success': True, 'synced': synced})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

# ===============================
# API Endpoints ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏≤‡∏ü 3 ‡πÄ‡∏™‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á
# ===============================

@app.route('/api/three-line-chart', methods=['POST'])
def get_three_line_chart_data():
    """API ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏£‡∏≤‡∏ü 3 ‡πÄ‡∏™‡πâ‡∏ô"""
    try:
        data = request.get_json()
        current_grades = data.get('current_grades', {})
        loaded_terms_count = data.get('loaded_terms_count', 8)
        
        chart_data = generate_three_line_chart_data(current_grades, loaded_terms_count)
        
        return jsonify({
            'success': True,
            'chart_data': chart_data
        })
        
    except Exception as e:
        logger.error(f"Error generating three line chart data: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/graduation-analysis', methods=['POST'])
def get_graduation_analysis():
    """API ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤"""
    try:
        data = request.get_json()
        current_grades = data.get('current_grades', {})
        loaded_terms_count = data.get('loaded_terms_count', 8)
        
        analysis_result = analyze_graduation_failure_reasons(current_grades, loaded_terms_count)
        
        return jsonify({
            'success': True,
            'analysis': analysis_result
        })
        
    except Exception as e:
        logger.error(f"Error analyzing graduation failure reasons: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/next-term-prediction', methods=['POST'])
def get_next_term_prediction():
    """API ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏ó‡∏≠‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ"""
    try:
        data = request.get_json()
        current_grades = data.get('current_grades', {})
        
        prediction_table = generate_next_term_grade_prediction_table(current_grades)
        
        return jsonify({
            'success': True,
            'prediction_table': prediction_table
        })
        
    except Exception as e:
        logger.error(f"Error generating next term prediction: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/comprehensive-analysis', methods=['POST'])
def get_comprehensive_analysis():
    """API ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"""
    try:
        data = request.get_json()
        current_grades = data.get('current_grades', {})
        loaded_terms_count = data.get('loaded_terms_count', 8)
        student_name = data.get('student_name', '‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤')
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏£‡∏≤‡∏ü 3 ‡πÄ‡∏™‡πâ‡∏ô
        chart_data = generate_three_line_chart_data(current_grades, loaded_terms_count)
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏ö
        graduation_analysis = analyze_graduation_failure_reasons(current_grades, loaded_terms_count)
        
        # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏ó‡∏≠‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
        next_term_prediction = generate_next_term_grade_prediction_table(current_grades)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏£‡∏ß‡∏°
        comprehensive_summary = {
            'student_name': student_name,
            'analysis_date': datetime.now().isoformat(),
            'current_status': {
                'gpa': graduation_analysis['current_gpa'],
                'total_courses': graduation_analysis['total_courses'],
                'failed_courses': graduation_analysis['failed_courses_count'],
                'risk_level': graduation_analysis['risk_level'],
                'graduation_probability': graduation_analysis['graduation_probability']
            },
            'predictions': {
                'next_term_gpa': next_term_prediction['predicted_term_gpa'],
                'cumulative_gpa': next_term_prediction['predicted_cumulative_gpa'],
                'improvement': next_term_prediction['improvement']
            },
            'key_insights': [
                f"GPA ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô: {graduation_analysis['current_gpa']:.2f}",
                f"‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á: {graduation_analysis['risk_level']}",
                f"‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏à‡∏ö: {graduation_analysis['graduation_probability']:.1f}%",
                f"‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ GPA ‡πÄ‡∏ó‡∏≠‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ: {next_term_prediction['predicted_term_gpa']:.2f}"
            ]
        }
        
        return jsonify({
            'success': True,
            'chart_data': chart_data,
            'graduation_analysis': graduation_analysis,
            'next_term_prediction': next_term_prediction,
            'comprehensive_summary': comprehensive_summary
        })
        
    except Exception as e:
        logger.error(f"Error generating comprehensive analysis: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500
    
# Keep all other routes unchanged...
@app.route('/page')
def main_page():
    return render_template('main_page.html')

# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç route /curriculum  
@app.route('/curriculum')
def curriculum_page():
    """Page for curriculum analysis."""
    try:
        courses_json = json.dumps(app.config.get('COURSES_DATA', []))
        terms_json = json.dumps(app.config.get('ALL_TERMS_DATA', []))
        grades_json = json.dumps(app.config.get('DATA_CONFIG', {}).get('grade_mapping', {}))
        
        return render_template(
            'curriculum_prediction_form.html',
            coursesData=courses_json,
            allTermsData=terms_json,
            gradeMapping=grades_json
        )
    except Exception as e:
        logger.error(f"Error rendering curriculum page: {str(e)}")
        return render_template(
            'curriculum_prediction_form.html',
            coursesData='[]',
            allTermsData='[]',
            gradeMapping='{}'
        )

@app.route('/advanced')
def advanced_test_page():
    """Page for advanced curriculum analysis."""
    return render_template('advanced_test.html')

@app.route('/predict-batch')
def predict_batch_page():
    return render_template('index.html')

@app.route('/models')
def models_page():
    if admin_manager.enabled and not has_any_role('admin', 'super_admin'):
        return redirect(url_for('index'))
    return render_template('model_management.html')
@app.route('/predict_manual_input', methods=['POST'])
def predict_manual_input():
    """Predicts outcome from manually entered subject data."""
    try:
        data = request.json
        student_name = data.pop('student_name', '‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô')
        model_filename = data.get('model_filename')

        if not model_filename:
            models_list = storage.list_models()
            subject_models = [m for m in models_list if 'subject_based' in m.get('filename', '') or m.get('data_format') == 'subject_based']
            if subject_models:
                model_filename = subject_models[0]['filename']
            else:
                return jsonify({'success': False, 'error': 'No model filename provided and no trained model found.'})

        # ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏≤‡∏Å storage
        loaded_model_data = storage.load_model(model_filename)
        if not loaded_model_data:
            return jsonify({'success': False, 'error': f'Specified model file {model_filename} not found.'})

        model_info = {
            'models': loaded_model_data.get('models', {}),
            'scaler': loaded_model_data.get('scaler')
        }
        feature_cols = loaded_model_data.get('feature_columns', [])
        data_format_used = loaded_model_data.get('data_format', 'unknown')
        logger.info(f"Loaded model '{model_filename}' (format: {data_format_used}) for manual prediction.")

        input_grades_raw = {cid: grade for cid, grade in data.items() if cid != 'model_filename'}

        student_data_for_prediction = {}
        grade_mapping = app.config['DATA_CONFIG']['grade_mapping']
        subject_categories = app.config['SUBJECT_CATEGORIES']
        all_known_courses_from_config = app.config['COURSES_DATA']

        all_grades_entered = []
        subject_categories_grades = {cat: [] for cat in subject_categories.keys()}
        subject_categories_grades['‡∏≠‡∏∑‡πà‡∏ô‡πÜ'] = []

        for course_id, grade_str in input_grades_raw.items():
            if grade_str and grade_str.strip():
                numeric_grade = None
                try:
                    numeric_grade = float(grade_str)
                    if not (0.0 <= numeric_grade <= 4.0):
                        numeric_grade = 0.0
                except ValueError:
                    numeric_grade = grade_mapping.get(str(grade_str).upper(), 0.0)
                
                all_grades_entered.append(numeric_grade)

                course_name = ""
                for c_data in all_known_courses_from_config:
                    if c_data['id'] == course_id:
                        course_name = c_data['thaiName']
                        break

                category = '‡∏≠‡∏∑‡πà‡∏ô‡πÜ'
                if course_name:
                    for cat, info in subject_categories.items():
                        if any(keyword in course_name.lower() for keyword in info['keywords']):
                            category = cat
                            break
                        
                subject_categories_grades.get(category, subject_categories_grades['‡∏≠‡∏∑‡πà‡∏ô‡πÜ']).append(numeric_grade)

        gpa = np.mean(all_grades_entered) if all_grades_entered else 0.0
        min_grade = np.min(all_grades_entered) if all_grades_entered else 0.0
        max_grade = np.max(all_grades_entered) if all_grades_entered else 0.0
        std_grade = np.std(all_grades_entered) if len(all_grades_entered) > 1 else 0.0
        fail_count = sum(1 for g in all_grades_entered if g <= app.config['DATA_CONFIG']['grade_mapping'].get('D', 1.0) - 0.01)
        fail_rate = fail_count / len(all_grades_entered) if all_grades_entered else 0.0
        total_subjects = len(all_grades_entered)

        student_data_for_prediction = {
            'gpa': gpa,
            'min_grade': min_grade,
            'max_grade': max_grade,
            'std_grade': std_grade,
            'fail_count': fail_count,
            'fail_rate': fail_rate,
            'total_subjects': total_subjects,
            'year_in': 0,
            'year_out': 0,
            'total_terms': 0
        }

        for cat, cat_grades in subject_categories_grades.items():
            if cat_grades:
                student_data_for_prediction[f'gpa_{cat}'] = np.mean(cat_grades)
                student_data_for_prediction[f'min_{cat}'] = np.min(cat_grades)
                student_data_for_prediction[f'max_{cat}'] = np.max(cat_grades)
                student_data_for_prediction[f'fail_rate_{cat}'] = sum(1 for g in cat_grades if g <= app.config['DATA_CONFIG']['grade_mapping'].get('D', 1.0) - 0.01) / len(cat_grades)
            else:
                student_data_for_prediction[f'gpa_{cat}'] = 0.0
                student_data_for_prediction[f'min_{cat}'] = 0.0
                student_data_for_prediction[f'max_{cat}'] = 0.0
                student_data_for_prediction[f'fail_rate_{cat}'] = 0.0

        processed_input_for_df = {}
        for feature in feature_cols:
            processed_input_for_df[feature] = [student_data_for_prediction.get(feature, 0.0)]

        input_df = pd.DataFrame(processed_input_for_df)

        trained_models = model_info['models']
        scaler = model_info['scaler']

        predictions_proba_list = []
        for name, model in trained_models.items():
            try:
                if name == 'lr':
                    X_scaled = scaler.transform(input_df)
                    pred_proba = model.predict_proba(X_scaled)
                else:
                    pred_proba = model.predict_proba(input_df)
                
                if pred_proba.shape[1] == 1:
                    pred_proba = np.hstack((1 - pred_proba, pred_proba))
                
                predictions_proba_list.append(pred_proba)
            except Exception as e:
                logger.warning(f"Could not predict with model {name} from manual input: {str(e)}")
                continue

        if not predictions_proba_list:
            return jsonify({'success': False, 'error': 'Could not make predictions with manual input.'})

        avg_prob_per_student = np.mean([pred_proba_array[0] for pred_proba_array in predictions_proba_list], axis=0)
        avg_prob_fail = avg_prob_per_student[0]
        avg_prob_pass = avg_prob_per_student[1]

        prediction = '‡∏à‡∏ö' if avg_prob_pass >= avg_prob_fail else '‡πÑ‡∏°‡πà‡∏à‡∏ö'

        confidence = max(avg_prob_pass, avg_prob_fail)
        high_confidence_threshold = app.config['DATA_CONFIG']['risk_levels']['high_confidence_threshold']
        medium_confidence_threshold = app.config['DATA_CONFIG']['risk_levels']['medium_confidence_threshold']

        if confidence > high_confidence_threshold:
            risk_level = '‡∏ï‡πà‡∏≥' if prediction == '‡∏à‡∏ö' else '‡∏™‡∏π‡∏á'
        elif confidence > medium_confidence_threshold:
            risk_level = '‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á'
        else:
            risk_level = '‡∏™‡∏π‡∏á' if prediction == '‡πÑ‡∏°‡πà‡∏à‡∏ö' else '‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á'

        gpa_for_analysis = student_data_for_prediction.get('gpa', 0.0)

        analysis = []
        recommendations = []

        low_gpa_threshold = app.config['DATA_CONFIG']['risk_levels']['low_gpa_threshold']
        warning_gpa_threshold = app.config['DATA_CONFIG']['risk_levels']['warning_gpa_threshold']
        high_fail_rate_threshold = app.config['DATA_CONFIG']['risk_levels']['high_fail_rate_threshold']

        if gpa_for_analysis < low_gpa_threshold:
            analysis.append(f"GPA ‡∏ï‡πà‡∏≥‡∏°‡∏≤‡∏Å ({float(gpa_for_analysis):.2f})")
            recommendations.extend(app.config['MESSAGES']['recommendations']['high_risk'])
        elif gpa_for_analysis < warning_gpa_threshold:
            analysis.append(f"GPA ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á ({float(gpa_for_analysis):.2f})")
            recommendations.extend(app.config['MESSAGES']['recommendations']['medium_risk'])
        elif gpa_for_analysis < 3.0:
            analysis.append(f"GPA ‡∏û‡∏≠‡πÉ‡∏ä‡πâ ({float(gpa_for_analysis):.2f})")
            recommendations.append("‡∏°‡∏µ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÉ‡∏´‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô")
        else:
            analysis.append(f"GPA ‡∏î‡∏µ ({float(gpa_for_analysis):.2f})")
            recommendations.extend(app.config['MESSAGES']['recommendations']['low_risk'])

        if prediction == '‡πÑ‡∏°‡πà‡∏à‡∏ö':
            recommendations.append("‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡∏ó‡∏ö‡∏ó‡∏ß‡∏ô‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÅ‡∏•‡∏∞‡∏Ç‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠")
            if student_data_for_prediction.get('fail_rate', 0) > high_fail_rate_threshold:
                recommendations.append("‡∏°‡∏µ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡∏ï‡∏Å‡πÉ‡∏ô‡∏ö‡∏≤‡∏á‡∏ß‡∏¥‡∏ä‡∏≤‡∏™‡∏π‡∏á ‡∏Ñ‡∏ß‡∏£‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ã‡πà‡∏≠‡∏°")

        if data_format_used == 'subject_based':
            weak_categories = []
            for cat_key in app.config['SUBJECT_CATEGORIES'].keys():
                cat_gpa_key = f'gpa_{cat_key}'
                if student_data_for_prediction.get(cat_gpa_key, 0) < low_gpa_threshold:
                    weak_categories.append(cat_key)
            if weak_categories:
                recommendations.append(f"‡∏Ñ‡∏ß‡∏£‡πÄ‡∏ô‡πâ‡∏ô‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏ß‡∏¥‡∏ä‡∏≤‡πÉ‡∏ô‡∏´‡∏°‡∏ß‡∏î: {', '.join(weak_categories[:2])}")

        return jsonify({
            'success': True,
            'student_name': student_name,
            'prediction': prediction,
            'prob_pass': float(avg_prob_pass),
            'prob_fail': float(avg_prob_fail),
            'gpa_input': float(gpa_for_analysis),
            'risk_level': risk_level,
            'confidence': float(confidence),
            'analysis': list(set(analysis)),
            'recommendations': list(set(recommendations)),
            'data_format_used': data_format_used
        })

    except Exception as e:
        logger.error(f"Error during manual input prediction: {str(e)}")
        return jsonify({'success': False, 'error': f'An error occurred during prediction: {str(e)}'})

def load_existing_models():
    """Loads existing trained models from storage."""
    try:
        logger.info("üîç Searching for existing models...")
        
        # ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏≤‡∏Å‡∏ó‡∏±‡πâ‡∏á storage ‡πÅ‡∏•‡∏∞ local
        models_found = []
        
        # 1. ‡∏•‡∏≠‡∏á‡∏à‡∏≤‡∏Å storage ‡∏Å‡πà‡∏≠‡∏ô
        try:
            models_list = storage.list_models()
            if models_list:
                models_found.extend(models_list)
                logger.info(f"Found {len(models_list)} models in storage")
        except Exception as e:
            logger.warning(f"Could not load from storage: {e}")
        
        # 2. ‡∏•‡∏≠‡∏á‡∏à‡∏≤‡∏Å local folder
        try:
            model_folder = app.config['MODEL_FOLDER']
            if os.path.exists(model_folder):
                for filename in os.listdir(model_folder):
                    if filename.endswith('.joblib'):
                        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
                        if not any(m.get('filename') == filename for m in models_found):
                            filepath = os.path.join(model_folder, filename)
                            try:
                                model_data = joblib.load(filepath)
                                models_found.append({
                                    'filename': filename,
                                    'created_at': model_data.get('created_at', datetime.fromtimestamp(os.path.getctime(filepath)).isoformat()),
                                    'data_format': model_data.get('data_format', 'subject_based'),
                                    'performance_metrics': model_data.get('performance_metrics', {}),
                                    'storage': 'local'
                                })
                            except Exception as e:
                                logger.warning(f"Could not load local model {filename}: {e}")
        except Exception as e:
            logger.warning(f"Could not check local folder: {e}")
        
        if not models_found:
            logger.info("No existing models found")
            return
        
        # Load subject-based model ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (‡∏ï‡∏±‡∏î GPA-based ‡∏≠‡∏≠‡∏Å)
        subject_models = [m for m in models_found if 'subject_based' in m.get('filename', '') or m.get('data_format') == 'subject_based']
        
        if subject_models:
            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á
            subject_models.sort(key=lambda x: x.get('created_at', ''), reverse=True)
            latest_subject = subject_models[0]
            
            # ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•
            loaded_data = None
            
            # ‡∏•‡∏≠‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å storage ‡∏Å‡πà‡∏≠‡∏ô
            if latest_subject.get('storage') != 'local':
                try:
                    loaded_data = storage.load_model(latest_subject['filename'])
                except Exception as e:
                    logger.warning(f"Could not load from storage: {e}")
            
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡∏•‡∏≠‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å local
            if not loaded_data:
                try:
                    filepath = os.path.join(app.config['MODEL_FOLDER'], latest_subject['filename'])
                    if os.path.exists(filepath):
                        loaded_data = joblib.load(filepath)
                        logger.info(f"Loaded from local: {latest_subject['filename']}")
                except Exception as e:
                    logger.error(f"Could not load model: {e}")
            
            if loaded_data:
                models['subject_model'] = {
                    'models': loaded_data.get('models', {}),
                    'scaler': loaded_data.get('scaler')
                }
                models['subject_feature_cols'] = loaded_data.get('feature_columns', [])
                models['subject_model_info'] = loaded_data.get('performance_metrics', 
                    {'accuracy': 0.85, 'precision': 0.85, 'recall': 0.85, 'f1_score': 0.85})
                models['subject_model_info']['created_at'] = loaded_data.get('created_at', datetime.now().isoformat())
                models['subject_model_info']['loaded_from_file'] = True
                models['subject_model_info']['filename'] = latest_subject['filename']
                logger.info(f"‚úÖ Loaded latest subject model: {latest_subject['filename']}")

    except Exception as e:
        logger.error(f"‚ùå Error loading existing models: {str(e)}")

if __name__ == '__main__':
    logger.info("=== FLASK APP CONFIGURATION ===")
    logger.info(f"App name: {app.name}")
    logger.info(f"App debug: {app.debug}")
    logger.info(f"App testing: {app.testing}")
    logger.info(f"Config keys: {list(app.config.keys())}")
    logger.info(f"Upload folder: {app.config['UPLOAD_FOLDER']}")
    logger.info(f"Model folder: {app.config['MODEL_FOLDER']}")
    logger.info(f"S3 Storage: {'Enabled' if not storage.use_local else 'Disabled (using local)'}")
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
    for folder in [app.config['UPLOAD_FOLDER'], app.config['MODEL_FOLDER']]:
        if not os.path.exists(folder):
            os.makedirs(folder)
            logger.info(f"‚úÖ Created folder: {folder}")
    
    # ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà
    load_existing_models()
    
    logger.info("üöÄ Starting server...")
    app.run(host='0.0.0.0', port=int(os.environ.get('PORT', 5000)), debug=False)

# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏™‡πà‡∏ß‡∏ô‡∏ö‡∏ô‡∏Ç‡∏≠‡∏á app.py
import os
import sys
from pathlib import Path

# ‡πÇ‡∏´‡∏•‡∏î .env ‡∏Å‡πà‡∏≠‡∏ô import ‡∏≠‡∏∑‡πà‡∏ô‡πÜ
from dotenv import load_dotenv

# ‡∏´‡∏≤ path ‡∏Ç‡∏≠‡∏á .env file
env_path = Path('.') / '.env'
if env_path.exists():
    load_dotenv(dotenv_path=env_path, override=True)
    print(f"‚úÖ Loaded .env from {env_path.absolute()}")
else:
    # ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡∏à‡∏≤‡∏Å parent directory
    parent_env = Path('..') / '.env'
    if parent_env.exists():
        load_dotenv(dotenv_path=parent_env, override=True)
        print(f"‚úÖ Loaded .env from {parent_env.absolute()}")
    else:
        print("‚ö†Ô∏è .env file not found, using system environment variables")

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á
print(f"R2 Access Key present: {bool(os.getenv('CLOUDFLARE_R2_ACCESS_KEY_ID'))}")
print(f"R2 Secret Key present: {bool(os.getenv('CLOUDFLARE_R2_SECRET_ACCESS_KEY'))}")

from advanced_training import AdvancedFeatureEngineer, ContextAwarePredictor
from explainable_ai import ExplainablePredictor

from flask import (
    Flask,
    request,
    jsonify,
    render_template,
    send_from_directory,
    redirect,
    url_for,
    session,
)
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, f1_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
import joblib
import os
import logging.config
import logging
from datetime import datetime, timedelta
import warnings
from collections import Counter, deque
import time
from functools import wraps
from imblearn.over_sampling import SMOTE
import tempfile
import json
import re
import copy
import boto3
from botocore.exceptions import ClientError, NoCredentialsError
from typing import Any, Dict, Optional, List
from werkzeug.utils import secure_filename
from werkzeug.security import generate_password_hash, check_password_hash
import secrets
import hashlib
import base64

try:
    from pymongo import MongoClient
    from pymongo.errors import PyMongoError
except ImportError:
    MongoClient = None
    PyMongoError = Exception

import config
import math

try:
    import google.generativeai as genai
except ImportError:
    genai = None

# ===============================
# ENHANCED FEATURES - ALL IN ONE
# ===============================

import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend
import matplotlib.pyplot as plt
import seaborn as sns
from io import BytesIO
import base64

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• TAN1
def preprocess_tan1_data(file_path):
    """
    ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• TAN1.csv ‡∏à‡∏≤‡∏Å Long Format ‡πÄ‡∏õ‡πá‡∏ô Wide Format
    ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì RESULT (‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤/‡πÑ‡∏°‡πà‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤)

    ‡∏£‡∏ß‡∏°‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:
    - Auto-mapping ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå (Dummy StudentNO ‚Üí STUDENT_ID, COURSE_CODE ‚Üí COURSE_ID)
    - ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß duplicate (‡∏ô‡∏®.+‡∏ß‡∏¥‡∏ä‡∏≤+‡∏õ‡∏µ+‡πÄ‡∏ó‡∏≠‡∏° ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô)
    - ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ retake: ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ attempt ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
    - ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡∏£‡∏î‡∏û‡∏¥‡πÄ‡∏®‡∏© (I, AU, U, S*, W)
    - ‡∏ô‡∏±‡∏ö‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô (‡πÑ‡∏°‡πà‡∏ô‡∏±‡∏ö W, F, I, U)
    - ‡∏Å‡∏£‡∏≠‡∏á‡∏ô‡∏®.‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏≠‡∏≠‡∏Å (‡πÑ‡∏°‡πà label ‡∏ß‡πà‡∏≤‡∏à‡∏ö/‡πÑ‡∏°‡πà‡∏à‡∏ö)
    """
    import logging
    logger = logging.getLogger(__name__)

    # ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    df_long = pd.read_csv(file_path)
    logger.info(f"üìñ Read {len(df_long)} rows, {len(df_long.columns)} columns")

    # === Step 1: ‡∏•‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏Ç‡∏¢‡∏∞ (‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡∏ä‡∏∑‡πà‡∏≠‡∏¢‡∏≤‡∏ß‡∏°‡∏≤‡∏Å‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏µ‡∏¢‡∏ß) ===
    first_col = df_long.columns[0]
    if len(first_col) > 50 and df_long[first_col].nunique() <= 1:
        logger.info(f"üóëÔ∏è Removing junk column: '{first_col[:50]}...'")
        df_long = df_long.drop(columns=[first_col])

    # === Step 2: Auto-mapping ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå ===
    column_mapping = {
        'Dummy StudentNO': 'STUDENT_ID', 'dummy studentno': 'STUDENT_ID',
        'STUDENT NO': 'STUDENT_ID', 'student_id': 'STUDENT_ID',
        'COURSE_CODE': 'COURSE_ID', 'course_code': 'COURSE_ID',
        'Course Code': 'COURSE_ID',
    }
    rename_map = {col: column_mapping[col] for col in df_long.columns if col in column_mapping}
    if rename_map:
        logger.info(f"üîÑ Column mapping: {rename_map}")
        df_long = df_long.rename(columns=rename_map)

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
    required_columns = ['STUDENT_ID', 'COURSE_ID', 'GRADE', 'CREDIT']
    for col in required_columns:
        if col not in df_long.columns:
            raise ValueError(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå {col} ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ: {list(df_long.columns)})")

    # === Step 3: ‡∏•‡∏ö exact duplicates (‡∏ô‡∏®.+‡∏ß‡∏¥‡∏ä‡∏≤+‡∏õ‡∏µ+‡πÄ‡∏ó‡∏≠‡∏° ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô) ===
    before_dedup = len(df_long)
    dedup_cols = ['STUDENT_ID', 'COURSE_ID']
    # ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå ‡∏õ‡∏µ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤ ‡πÅ‡∏•‡∏∞ ‡πÄ‡∏ó‡∏≠‡∏° ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
    if '‡∏õ‡∏µ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤' in df_long.columns:
        dedup_cols.append('‡∏õ‡∏µ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤')
    if '‡πÄ‡∏ó‡∏≠‡∏°' in df_long.columns:
        dedup_cols.append('‡πÄ‡∏ó‡∏≠‡∏°')
    df_long = df_long.drop_duplicates(subset=dedup_cols, keep='first')
    removed_dupes = before_dedup - len(df_long)
    logger.info(f"üßπ Removed {removed_dupes} exact duplicate rows ({before_dedup} ‚Üí {len(df_long)})")

    # === Grade mapping (‡∏¢‡πâ‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô Step 4a ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô retake analysis) ===
    grade_mapping = {
        'A': 4.0, 'B+': 3.5, 'B': 3.0, 'C+': 2.5, 'C': 2.0,
        'D+': 1.5, 'D': 1.0, 'F': 0.0,
        'W': None,              # ‡∏ñ‡∏≠‡∏ô ‚Äî ‡πÑ‡∏°‡πà‡∏ô‡∏±‡∏ö GPA/‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï
        'S': None, 'S*': None,  # ‡∏ú‡πà‡∏≤‡∏ô/‡∏™‡∏≠‡∏ö‡∏ú‡πà‡∏≤‡∏ô ‚Äî ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏Å‡∏£‡∏î‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
        'AU': None,             # Audit ‚Äî ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏Å‡∏£‡∏î
        'I': None,              # Incomplete ‚Äî ‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
        'U': None,              # Unsatisfactory ‚Äî ‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô (‡πÑ‡∏°‡πà‡∏ô‡∏±‡∏ö credit)
    }

    # === Step 4a: ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå retake ‡∏Å‡πà‡∏≠‡∏ô‡∏•‡∏ö ===
    sort_cols = ['STUDENT_ID', 'COURSE_ID']
    if '‡∏õ‡∏µ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤' in df_long.columns:
        sort_cols.append('‡∏õ‡∏µ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤')
    if '‡πÄ‡∏ó‡∏≠‡∏°' in df_long.columns:
        sort_cols.append('‡πÄ‡∏ó‡∏≠‡∏°')

    retake_stats = {}
    for student_id in df_long['STUDENT_ID'].unique():
        student_data = df_long[df_long['STUDENT_ID'] == student_id]
        course_attempts = student_data.groupby('COURSE_ID').size()
        retake_courses = course_attempts[course_attempts > 1]

        if len(retake_courses) > 0:
            retake_first_grades = []
            retake_improvements = []
            for cid in retake_courses.index:
                attempts = student_data[student_data['COURSE_ID'] == cid].sort_values(sort_cols)
                first_grade = attempts.iloc[0]['GRADE']
                last_grade = attempts.iloc[-1]['GRADE']
                retake_first_grades.append(first_grade)
                first_gp = grade_mapping.get(first_grade)
                last_gp = grade_mapping.get(last_grade)
                if first_gp is not None and last_gp is not None:
                    retake_improvements.append(last_gp - first_gp)

            retake_stats[student_id] = {
                'num_retake_courses': len(retake_courses),
                'total_retake_attempts': int(retake_courses.sum()) - len(retake_courses),
                'retake_f_count': retake_first_grades.count('F'),
                'retake_w_count': retake_first_grades.count('W'),
                'avg_retake_improvement': float(np.mean(retake_improvements)) if retake_improvements else 0.0,
            }
        else:
            retake_stats[student_id] = {
                'num_retake_courses': 0, 'total_retake_attempts': 0,
                'retake_f_count': 0, 'retake_w_count': 0,
                'avg_retake_improvement': 0.0,
            }

    logger.info(f"üîÅ Retake analysis: {sum(1 for v in retake_stats.values() if v['num_retake_courses'] > 0)} students have retakes")

    # === Step 4b: ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ retake ‚Äî ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ attempt ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ ===
    before_retake = len(df_long)
    df_long = df_long.sort_values(sort_cols)
    df_long = df_long.drop_duplicates(subset=['STUDENT_ID', 'COURSE_ID'], keep='last')
    removed_retakes = before_retake - len(df_long)
    if removed_retakes > 0:
        logger.info(f"üîÅ Removed {removed_retakes} retake duplicates (kept last attempt)")

    # === Step 5: ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå GRADE_POINT_NUM ===

    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå GRADE_POINT_NUM
    df_long['GRADE_POINT_NUM'] = df_long['GRADE'].map(grade_mapping)

    # Log ‡πÄ‡∏Å‡∏£‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å
    unknown_grades = df_long[~df_long['GRADE'].isin(grade_mapping.keys())]['GRADE'].unique()
    if len(unknown_grades) > 0:
        logger.warning(f"‚ö†Ô∏è Unknown grades found (will be excluded): {list(unknown_grades)}")

    # === Step 6: ‡∏Å‡∏£‡∏≠‡∏á‡∏ô‡∏®.‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏≠‡∏¢‡∏π‡πà ===
    has_entry_year = '‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤' in df_long.columns
    has_academic_year = '‡∏õ‡∏µ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤' in df_long.columns
    excluded_students = set()

    if has_entry_year and has_academic_year:
        current_year_be = 2568  # ‡∏û.‡∏®. 2568 = ‡∏Ñ.‡∏®. 2025
        current_year_ce = current_year_be - 543  # ‡∏Ñ.‡∏®. 2025
        # ‡∏Å‡∏£‡∏≠‡∏á‡∏ô‡∏®.‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÑ‡∏°‡πà‡∏ñ‡∏∂‡∏á 4 ‡∏õ‡∏µ‡∏Å‡πà‡∏≠‡∏ô‡∏õ‡∏µ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô ‚Üí ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö
        cutoff_entry_year = current_year_ce - 5  # ‡∏£‡∏∏‡πà‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤ >= 5 ‡∏õ‡∏µ‡∏Å‡πà‡∏≠‡∏ô‡∏ñ‡∏∂‡∏á‡∏à‡∏∞‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏£‡∏ö
        for student_id in df_long['STUDENT_ID'].unique():
            student_data = df_long[df_long['STUDENT_ID'] == student_id]
            entry_year_ce = student_data['‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤'].iloc[0]  # ‡∏Ñ.‡∏®.

            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏õ‡∏µ cutoff ‚Üí ‡∏¢‡∏±‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö 4 ‡∏õ‡∏µ ‚Üí ‡πÑ‡∏°‡πà‡πÄ‡∏≠‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡πÄ‡∏ó‡∏£‡∏ô
            if entry_year_ce > cutoff_entry_year:
                excluded_students.add(student_id)

        if excluded_students:
            df_long = df_long[~df_long['STUDENT_ID'].isin(excluded_students)]
            logger.info(f"üéì Excluded {len(excluded_students)} still-studying students (< 4 years, recent data)")
    else:
        logger.info("‚ÑπÔ∏è No ‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤/‡∏õ‡∏µ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤ columns ‚Äî skipping still-studying filter")

    logger.info(f"üìä Clean data: {len(df_long)} rows, {df_long['STUDENT_ID'].nunique()} students, {df_long['COURSE_ID'].nunique()} courses")

    # === Step 7: ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Wide Format ===
    df_wide = df_long.pivot_table(
        index='STUDENT_ID',
        columns='COURSE_ID',
        values='GRADE',
        aggfunc='first'
    ).reset_index()

    # === Step 8: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA ‡πÅ‡∏•‡∏∞‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡∏ô‡∏±‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô) ===
    # ‡πÄ‡∏Å‡∏£‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ô‡∏±‡∏ö‡πÉ‡∏ô GPA (‡πÄ‡∏Å‡∏£‡∏î‡∏û‡∏¥‡πÄ‡∏®‡∏©)
    non_gpa_grades = {'W', 'S', 'S*', 'AU', 'I', 'U'}
    # ‡πÄ‡∏Å‡∏£‡∏î‡∏ó‡∏µ‡πà "‡∏ú‡πà‡∏≤‡∏ô" = ‡πÑ‡∏î‡πâ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï
    passed_grades = {'A', 'B+', 'B', 'C+', 'C', 'D+', 'D', 'S', 'S*'}

    student_stats = []
    for student_id in df_long['STUDENT_ID'].unique():
        student_data = df_long[df_long['STUDENT_ID'] == student_id]

        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA ‚Äî ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏Å‡∏£‡∏î‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏°‡πÄ‡∏Å‡∏£‡∏î‡∏û‡∏¥‡πÄ‡∏®‡∏©)
        gpa_data = student_data[~student_data['GRADE'].isin(non_gpa_grades)]
        gpa_data = gpa_data[gpa_data['GRADE_POINT_NUM'].notna()]

        if len(gpa_data) > 0:
            total_grade_points = (gpa_data['GRADE_POINT_NUM'] * gpa_data['CREDIT']).sum()
            total_credits_for_gpa = gpa_data['CREDIT'].sum()
            gpa = total_grade_points / total_credits_for_gpa if total_credits_for_gpa > 0 else 0
        else:
            gpa = 0

        # ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏™‡∏∞‡∏™‡∏° ‚Äî ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô (D ‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ ‡∏´‡∏£‡∏∑‡∏≠ S/S*)
        passed_data = student_data[student_data['GRADE'].isin(passed_grades)]
        total_credits_earned = passed_data['CREDIT'].sum()

        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤: GPA >= 2.00 AND ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï >= 136 AND ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 4 ‡∏õ‡∏µ
        years_studied = 0
        if has_entry_year and has_academic_year:
            entry_year_ce = student_data['‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤'].iloc[0]
            entry_year_be = entry_year_ce + 543
            last_year_be = student_data['‡∏õ‡∏µ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤'].max()
            years_studied = last_year_be - entry_year_be + 1

        if years_studied > 0:
            result = 1 if (gpa >= 2.00 and total_credits_earned >= 136 and years_studied <= 4) else 0
        else:
            # ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏µ ‡πÉ‡∏ä‡πâ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡πÄ‡∏î‡∏¥‡∏°
            result = 1 if (gpa >= 2.00 and total_credits_earned >= 136) else 0

        student_stats.append({
            'STUDENT_ID': student_id,
            'GPA': gpa,
            'TOTAL_CREDITS': total_credits_earned,
            'RESULT': result
        })

    # ‡∏£‡∏ß‡∏° RESULT ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö df_wide
    stats_df = pd.DataFrame(student_stats)
    df_wide = df_wide.merge(stats_df[['STUDENT_ID', 'RESULT']], on='STUDENT_ID')

    # Log ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    graduated = stats_df['RESULT'].sum()
    total = len(stats_df)
    logger.info(f"üìã Labels: {graduated}/{total} graduated ({graduated/total*100:.1f}%), {total-graduated} not graduated")

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á course_credit_map
    course_credit_map = df_long.groupby('COURSE_ID')['CREDIT'].first().to_dict()

    return df_wide, df_long, course_credit_map, retake_stats

# ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏•‡∏≤‡∏™ AdvancedModelTrainer ‡∏•‡∏á‡πÉ‡∏ô app.py
class AdvancedModelTrainer:
    def __init__(self):
        self.models = {
            'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
        }
        self.best_model = None
        self.best_score = 0
        self.trained_models = {}
        self.scaler = StandardScaler()
        self.grade_mapping = {
            'A': 4.0, 'B+': 3.5, 'B': 3.0, 'C+': 2.5, 'C': 2.0,
            'D+': 1.5, 'D': 1.0, 'F': 0.0,
            'W': None, 'S': None, 'S*': None, 'AU': None, 'I': None, 'U': None
        }
        self.course_credit_map = {}
        
    def prepare_training_data(self, df_wide_format, df_long_format_original, retake_stats=None):
        """‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô"""
        import logging
        logger = logging.getLogger(__name__)

        logger.info("üéØ ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö Semester-based Dynamic Snapshots...")

        # ‡πÄ‡∏Å‡πá‡∏ö retake_stats
        self.retake_stats = retake_stats or {}

        # ‡πÄ‡∏Å‡πá‡∏ö course_credit_map
        self.course_credit_map = df_long_format_original.groupby('COURSE_ID')['CREDIT'].first().to_dict()

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏õ‡∏£‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤
        logger.info("üèõÔ∏è ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏õ‡∏£‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤ (Course Profiling)...")
        course_profiles = self.create_course_profiles(df_wide_format, df_long_format_original)
        logger.info(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏õ‡∏£‡πÑ‡∏ü‡∏•‡πå {len(course_profiles)} ‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏ö‡∏ö Semester-based Snapshots (‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏à‡∏£‡∏¥‡∏á)
        X, y = self.create_dynamic_snapshots(df_wide_format, df_long_format_original, course_profiles)

        logger.info(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ó‡∏£‡∏ô {len(X)} ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å {len(df_wide_format)} ‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤")

        return X, y, course_profiles
    
    def create_course_profiles(self, df_wide_format, df_long_format_original):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏õ‡∏£‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤ (Course DNA) ‚Äî enriched version

        ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ß‡∏¥‡∏ä‡∏≤‡∏à‡∏∞‡∏°‡∏µ:
        - avg_grade, grade_std ‚Äî ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÄ‡∏Å‡∏£‡∏î‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
        - fail_rate ‚Äî ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡πÑ‡∏î‡πâ F ‡∏à‡∏£‡∏¥‡∏á‡πÜ (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà < 2.0)
        - low_grade_rate ‚Äî ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡πÑ‡∏î‡πâ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ C (< 2.0 ‡∏£‡∏ß‡∏° D+, D, F)
        - pass_rate ‚Äî ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏ú‡πà‡∏≤‡∏ô (D ‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ)
        - w_rate ‚Äî ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏ñ‡∏≠‡∏ô (W) ‚Äî ‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì‡∏ß‡∏¥‡∏ä‡∏≤‡∏¢‡∏≤‡∏Å
        - difficulty_score ‚Äî 4.0 - avg_grade
        - credits, total_students, total_enrollments
        """
        course_profiles = {}

        for course_id in df_long_format_original['COURSE_ID'].unique():
            course_data = df_long_format_original[df_long_format_original['COURSE_ID'] == course_id]

            all_grades = course_data['GRADE'].tolist()
            credits = course_data['CREDIT'].iloc[0] if len(course_data) > 0 else 3
            total_enrollments = len(all_grades)

            if total_enrollments == 0:
                continue

            # W rate (‡∏ô‡∏±‡∏ö‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡πÅ‡∏ñ‡∏ß‡∏Å‡πà‡∏≠‡∏ô filter)
            w_count = sum(1 for g in all_grades if g == 'W')
            w_rate = w_count / total_enrollments

            # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (A-F ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô, ‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏° W/S/S*/AU/I/U)
            numeric_grades = []
            for grade in all_grades:
                if grade in self.grade_mapping and self.grade_mapping[grade] is not None:
                    numeric_grades.append(self.grade_mapping[grade])

            if not numeric_grades:
                continue

            n = len(numeric_grades)
            avg_grade = np.mean(numeric_grades)
            grade_std = np.std(numeric_grades) if n > 1 else 0

            # fail_rate = F ‡∏à‡∏£‡∏¥‡∏á‡πÜ (0.0 only)
            f_rate = len([g for g in numeric_grades if g == 0.0]) / n
            # low_grade_rate = ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ C (< 2.0 ‡∏£‡∏ß‡∏° D+, D, F)
            low_grade_rate = len([g for g in numeric_grades if g < 2.0]) / n
            # pass_rate = D ‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ (>= 1.0)
            pass_rate = len([g for g in numeric_grades if g >= 1.0]) / n

            difficulty_score = 4.0 - avg_grade

            course_profiles[course_id] = {
                'avg_grade': avg_grade,
                'grade_std': grade_std,
                'fail_rate': f_rate,
                'low_grade_rate': low_grade_rate,
                'pass_rate': pass_rate,
                'w_rate': w_rate,
                'difficulty_score': difficulty_score,
                'credits': credits,
                'total_students': n,
                'total_enrollments': total_enrollments,
            }

        return course_profiles
    
    def create_dynamic_snapshots(self, df_wide_format, df_long_format, course_profiles):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö Semester-based Snapshots (‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏à‡∏£‡∏¥‡∏á)

        ‡∏™‡∏£‡πâ‡∏≤‡∏á training example ‡∏ó‡∏∏‡∏Å‡πÄ‡∏ó‡∏≠‡∏°: ‡∏´‡∏•‡∏±‡∏á‡πÄ‡∏ó‡∏≠‡∏° 1 ‡πÄ‡∏´‡πá‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡πÄ‡∏ó‡∏≠‡∏° 1,
        ‡∏´‡∏•‡∏±‡∏á‡πÄ‡∏ó‡∏≠‡∏° 2 ‡πÄ‡∏´‡πá‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡πÄ‡∏ó‡∏≠‡∏° 1+2, ‡∏Ø‡∏•‡∏Ø ‚Üí ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ "‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏≤‡∏á" ‡∏à‡∏£‡∏¥‡∏á‡πÜ
        """
        X = []
        y = []

        passed_grades = {'A', 'B+', 'B', 'C+', 'C', 'D+', 'D', 'S', 'S*'}
        has_time_cols = '‡∏õ‡∏µ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤' in df_long_format.columns and '‡πÄ‡∏ó‡∏≠‡∏°' in df_long_format.columns

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á RESULT lookup ‡∏à‡∏≤‡∏Å df_wide
        result_lookup = df_wide_format.set_index('STUDENT_ID')['RESULT'].to_dict()

        for student_id in df_long_format['STUDENT_ID'].unique():
            if student_id not in result_lookup:
                continue

            student_result = result_lookup[student_id]
            student_data = df_long_format[df_long_format['STUDENT_ID'] == student_id].copy()

            if has_time_cols:
                # === ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏à‡∏£‡∏¥‡∏á (‡∏õ‡∏µ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤ + ‡πÄ‡∏ó‡∏≠‡∏°) ===
                student_data = student_data.sort_values(['‡∏õ‡∏µ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤', '‡πÄ‡∏ó‡∏≠‡∏°'])
                student_data['_semester_key'] = (
                    student_data['‡∏õ‡∏µ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤'].astype(str) + '_' + student_data['‡πÄ‡∏ó‡∏≠‡∏°'].astype(str)
                )
                unique_semesters = student_data['_semester_key'].unique()
                total_semesters = len(unique_semesters)

                # ‡∏™‡∏∞‡∏™‡∏°‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡∏•‡∏∞‡πÄ‡∏ó‡∏≠‡∏°
                cumulative_grades = {}
                cumulative_retakes = 0

                # ‡∏î‡∏∂‡∏á retake_stats ‡∏Ç‡∏≠‡∏á‡∏ô‡∏®.‡∏Ñ‡∏ô‡∏ô‡∏µ‡πâ
                student_retake = self.retake_stats.get(student_id, {})

                for sem_idx, semester in enumerate(unique_semesters):
                    sem_data = student_data[student_data['_semester_key'] == semester]

                    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ß‡∏¥‡∏ä‡∏≤‡∏Ç‡∏≠‡∏á‡πÄ‡∏ó‡∏≠‡∏°‡∏ô‡∏µ‡πâ
                    for _, row in sem_data.iterrows():
                        if row['COURSE_ID'] in cumulative_grades:
                            cumulative_retakes += 1  # ‡∏ô‡∏±‡∏ö retake ‡∏™‡∏∞‡∏™‡∏°
                        cumulative_grades[row['COURSE_ID']] = row['GRADE']

                    # ‡∏™‡∏£‡πâ‡∏≤‡∏á snapshot ‡∏´‡∏•‡∏±‡∏á‡πÄ‡∏ó‡∏≠‡∏°‡∏ô‡∏µ‡πâ (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 2 ‡∏ß‡∏¥‡∏ä‡∏≤‡∏™‡∏∞‡∏™‡∏°)
                    if len(cumulative_grades) >= 2:
                        semester_number = sem_idx + 1

                        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏™‡∏∞‡∏™‡∏°‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô
                        credits_earned = sum(
                            self.course_credit_map.get(cid, 3)
                            for cid, grade in cumulative_grades.items()
                            if grade in passed_grades
                        )

                        features = self.create_dynamic_features(
                            cumulative_grades, course_profiles, len(cumulative_grades),
                            credits_earned=credits_earned,
                            semester_number=semester_number,
                            total_semesters=total_semesters,
                            retake_info=student_retake,
                            cumulative_retakes=cumulative_retakes
                        )

                        X.append(features)
                        y.append(student_result)
            else:
                # === Fallback: ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ß‡∏•‡∏≤ ‚Äî ‡πÉ‡∏ä‡πâ snapshot ‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤ ===
                student_courses = {}
                for _, row in student_data.iterrows():
                    student_courses[row['COURSE_ID']] = row['GRADE']

                total_courses = len(student_courses)
                course_list = list(student_courses.items())

                snapshot_sizes = []
                if total_courses >= 3: snapshot_sizes.append(3)
                if total_courses >= 6: snapshot_sizes.append(6)
                if total_courses >= 9: snapshot_sizes.append(9)
                if total_courses >= 12: snapshot_sizes.append(12)
                if total_courses > 12: snapshot_sizes.append(total_courses)

                for snapshot_size in snapshot_sizes:
                    current_courses = dict(course_list[:snapshot_size])
                    features = self.create_dynamic_features(
                        current_courses, course_profiles, total_courses
                    )
                    X.append(features)
                    y.append(student_result)

        return np.array(X), np.array(y)
    
    def create_dynamic_features(self, grades_dict, course_profiles, total_courses_taken,
                                credits_earned=None, semester_number=None, total_semesters=None,
                                retake_info=None, cumulative_retakes=0):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡πÅ‡∏ö‡∏ö Dynamic ‡∏û‡∏£‡πâ‡∏≠‡∏° Context-Aware + Temporal + Retake Features (32 ‡∏ï‡∏±‡∏ß)

        Parameters:
            grades_dict: dict {COURSE_ID: GRADE} ‚Äî ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏°‡∏≤‡∏ñ‡∏∂‡∏á‡∏à‡∏∏‡∏î‡∏ô‡∏µ‡πâ
            course_profiles: dict ‚Äî ‡πÇ‡∏õ‡∏£‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤
            total_courses_taken: int ‚Äî ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Ç‡∏≠‡∏á‡∏ô‡∏®.‡∏Ñ‡∏ô‡∏ô‡∏µ‡πâ
            credits_earned: float|None ‚Äî ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏™‡∏∞‡∏™‡∏°‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô (None = ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏≠‡∏á)
            semester_number: int|None ‚Äî ‡πÄ‡∏ó‡∏≠‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà (None = ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏à‡∏≤‡∏Å‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤)
            total_semesters: int|None ‚Äî ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ó‡∏≠‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Ç‡∏≠‡∏á‡∏ô‡∏®. (None = ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì)
            retake_info: dict|None ‚Äî retake stats ‡∏Ç‡∏≠‡∏á‡∏ô‡∏®.‡∏Ñ‡∏ô‡∏ô‡∏µ‡πâ (None = ‡πÑ‡∏°‡πà‡∏°‡∏µ retake data)
            cumulative_retakes: int ‚Äî ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô retake ‡∏™‡∏∞‡∏™‡∏°‡∏ñ‡∏∂‡∏á‡∏à‡∏∏‡∏î‡∏ô‡∏µ‡πâ
        """
        NUM_FEATURES = 32

        if not grades_dict:
            return np.zeros(NUM_FEATURES)

        # ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô - ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÄ‡∏Å‡∏£‡∏î
        numeric_grades = []
        for grade in grades_dict.values():
            if grade in self.grade_mapping and self.grade_mapping[grade] is not None:
                numeric_grades.append(self.grade_mapping[grade])

        if not numeric_grades:
            return np.zeros(NUM_FEATURES)

        # === ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì temporal defaults ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏™‡πà‡∏á‡∏°‡∏≤ (prediction time) ===
        passed_grades_set = {'A', 'B+', 'B', 'C+', 'C', 'D+', 'D', 'S', 'S*'}
        if credits_earned is None:
            credits_earned = sum(
                self.course_credit_map.get(cid, 3)
                for cid, grade in grades_dict.items()
                if grade in passed_grades_set
            )
        if semester_number is None:
            # ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì: ~6-7 ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ï‡πà‡∏≠‡πÄ‡∏ó‡∏≠‡∏°
            semester_number = max(1, round(len(grades_dict) / 6.5))
        if total_semesters is None:
            total_semesters = max(semester_number, max(1, round(total_courses_taken / 6.5)))

        # 1-5: ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÄ‡∏Å‡∏£‡∏î‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        current_gpa = np.mean(numeric_grades)
        gpa_std = np.std(numeric_grades) if len(numeric_grades) > 1 else 0
        min_grade = min(numeric_grades)
        max_grade = max(numeric_grades)
        grade_range = max_grade - min_grade

        # 6-8: ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÄ‡∏Å‡∏£‡∏î
        n = len(numeric_grades)
        high_grades = len([g for g in numeric_grades if g >= 3.5]) / n
        medium_grades = len([g for g in numeric_grades if 2.5 <= g < 3.5]) / n
        low_grades = len([g for g in numeric_grades if g < 2.5]) / n

        # 9-12: Context-Aware Features
        avg_course_difficulty = 0
        avg_course_fail_rate = 0
        num_killer_courses = 0
        num_easy_courses = 0

        if course_profiles:
            difficulties = []
            fail_rates = []

            for course in grades_dict.keys():
                if course in course_profiles:
                    difficulty = course_profiles[course]['difficulty_score']
                    fail_rate = course_profiles[course]['fail_rate']
                    difficulties.append(difficulty)
                    fail_rates.append(fail_rate)
                    if difficulty > 2.0:
                        num_killer_courses += 1
                    elif difficulty < 1.0:
                        num_easy_courses += 1

            avg_course_difficulty = np.mean(difficulties) if difficulties else 0
            avg_course_fail_rate = np.mean(fail_rates) if fail_rates else 0

        # 13-15: Risk features
        num_failed_courses = len([g for g in numeric_grades if g < 2.0])
        killer_course_ratio = num_killer_courses / len(grades_dict) if len(grades_dict) > 0 else 0

        expected_gpa = current_gpa
        if course_profiles:
            expected_grades = [course_profiles[c]['avg_grade'] for c in grades_dict if c in course_profiles]
            if expected_grades:
                expected_gpa = np.mean(expected_grades)
        gpa_gap = current_gpa - expected_gpa

        # 16-20: ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°
        if len(numeric_grades) >= 3:
            recent_trend = np.mean(numeric_grades[-3:]) - np.mean(numeric_grades[:-3])
        else:
            recent_trend = 0

        progress_ratio = len(grades_dict) / total_courses_taken if total_courses_taken > 0 else 0
        consistency_score = 1 / (1 + gpa_std)
        academic_momentum = current_gpa * progress_ratio
        risk_score = (num_failed_courses + num_killer_courses) / len(grades_dict) if len(grades_dict) > 0 else 0

        # === 21-25: Temporal / Progress features ===
        # 21: ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏™‡∏∞‡∏™‡∏°‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô
        # 22: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡πâ‡∏≤‡∏ß‡∏´‡∏ô‡πâ‡∏≤‡∏™‡∏π‡πà‡∏à‡∏ö (credits_earned / 136)
        graduation_progress = min(1.0, credits_earned / 136) if credits_earned > 0 else 0
        # 23: ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏ï‡πà‡∏≠‡πÄ‡∏ó‡∏≠‡∏° (pace)
        credits_per_semester = credits_earned / semester_number if semester_number > 0 else 0
        # 24: ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏´‡∏° (17 ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï/‡πÄ‡∏ó‡∏≠‡∏° = on track)
        expected_credits = semester_number * 17
        on_track_ratio = credits_earned / expected_credits if expected_credits > 0 else 0
        # 25: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡πâ‡∏≤‡∏ß‡∏´‡∏ô‡πâ‡∏≤‡∏Ç‡∏≠‡∏á‡πÄ‡∏ß‡∏•‡∏≤ (‡πÄ‡∏ó‡∏≠‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà / 8 ‡πÄ‡∏ó‡∏≠‡∏°)
        semester_progress = semester_number / 8

        # === 26-28: Course DNA-powered features ===
        # 26: credit-weighted difficulty (‡∏ß‡∏¥‡∏ä‡∏≤‡∏¢‡∏≤‡∏Å+‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡πÄ‡∏¢‡∏≠‡∏∞ = ‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏°‡∏≤‡∏Å)
        weighted_difficulty = 0
        if course_profiles:
            wd_num, wd_den = 0, 0
            for course in grades_dict.keys():
                if course in course_profiles:
                    cp = course_profiles[course]
                    c = cp.get('credits', 3)
                    wd_num += cp['difficulty_score'] * c
                    wd_den += c
            weighted_difficulty = wd_num / wd_den if wd_den > 0 else 0

        # 27: hard_course_gpa (‡∏ô‡∏®.‡πÄ‡∏Å‡πà‡∏á‡πÉ‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏¢‡∏≤‡∏Å‡πÑ‡∏´‡∏° ‚Äî ‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å)
        hard_course_gpa = current_gpa
        if course_profiles:
            hard_grades = []
            for course, grade in grades_dict.items():
                if course in course_profiles and course_profiles[course]['difficulty_score'] > 1.5:
                    if grade in self.grade_mapping and self.grade_mapping[grade] is not None:
                        hard_grades.append(self.grade_mapping[grade])
            if hard_grades:
                hard_course_gpa = np.mean(hard_grades)

        # 28: grade_variance_exposure (‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ú‡∏±‡∏ô‡∏ú‡∏ß‡∏ô‡∏°‡∏≤‡∏Å‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô ‚Äî ‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á)
        grade_variance_exposure = 0
        if course_profiles:
            stds = [course_profiles[c].get('grade_std', 0) for c in grades_dict if c in course_profiles]
            grade_variance_exposure = np.mean(stds) if stds else 0

        # === 29-32: Retake features ===
        ri = retake_info or {}
        num_retake_courses = ri.get('num_retake_courses', 0)
        retake_f_count = ri.get('retake_f_count', 0)
        retake_ratio = num_retake_courses / len(grades_dict) if len(grades_dict) > 0 else 0
        avg_retake_improvement = ri.get('avg_retake_improvement', 0.0)

        features = np.array([
            current_gpa, gpa_std, min_grade, max_grade, grade_range,          # 1-5
            high_grades, medium_grades, low_grades,                            # 6-8
            avg_course_difficulty, avg_course_fail_rate,                       # 9-10
            num_killer_courses, num_easy_courses,                              # 11-12
            num_failed_courses, killer_course_ratio, gpa_gap,                 # 13-15
            recent_trend, progress_ratio, consistency_score,                   # 16-18
            academic_momentum, risk_score,                                     # 19-20
            credits_earned, graduation_progress, credits_per_semester,        # 21-23
            on_track_ratio, semester_progress,                                 # 24-25
            weighted_difficulty, hard_course_gpa, grade_variance_exposure,    # 26-28
            num_retake_courses, retake_f_count,                               # 29-30
            retake_ratio, avg_retake_improvement                              # 31-32
        ])

        # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ NaN values
        return self._handle_nan_features(features)
    
    def _handle_nan_features(self, features):
        """‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ NaN values ‡πÉ‡∏ô‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå"""
        # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà NaN ‡∏î‡πâ‡∏ß‡∏¢ 0
        features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
        return features
    
    def train_models(self, X, y):
        """‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏û‡∏£‡πâ‡∏≠‡∏° SMOTE ‡πÅ‡∏•‡∏∞ Feature Scaling"""
        import logging
        logger = logging.getLogger(__name__)
        
        logger.info("üß† ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• AI...")
        
        # Feature Scaling
        logger.info("üìè ‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏î‡πâ‡∏ß‡∏¢ StandardScaler...")
        X_scaled = self.scaler.fit_transform(X)
        
        # ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏•‡∏î test_size ‡πÄ‡∏õ‡πá‡∏ô 0.05 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ó‡∏£‡∏ô‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô)
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏•‡∏≤‡∏™‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢
        unique_classes, class_counts = np.unique(y, return_counts=True)
        min_class_count = min(class_counts)
        
        # ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡πâ‡∏≠‡∏¢‡∏°‡∏≤‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™‡πÑ‡∏°‡πà‡∏™‡∏°‡∏î‡∏∏‡∏• ‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏±‡∏ö‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå
        if len(X_scaled) < 10 or min_class_count < 2:
            test_size = 0.2
            stratify_param = None  # ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ stratify ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡πâ‡∏≠‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™‡∏°‡∏µ‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
            logger.info(f"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡πâ‡∏≠‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™‡πÑ‡∏°‡πà‡∏™‡∏°‡∏î‡∏∏‡∏• (min_class: {min_class_count}) ‡∏õ‡∏£‡∏±‡∏ö test_size ‡πÄ‡∏õ‡πá‡∏ô 0.2 ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ stratify")
        else:
            test_size = 0.05
            stratify_param = y
        
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=test_size, random_state=42, stratify=stratify_param
        )
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        unique, counts = np.unique(y_train, return_counts=True)
        logger.info(f"‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ó‡∏£‡∏ô: {dict(zip(unique, counts))}")
        
        # ‡πÉ‡∏ä‡πâ SMOTE ‡∏´‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏™‡∏°‡∏î‡∏∏‡∏•
        if len(unique) > 1 and min(counts) / max(counts) < 0.5:
            logger.info("‚öñÔ∏è ‡πÉ‡∏ä‡πâ SMOTE ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏°‡∏î‡∏∏‡∏•...")
            smote = SMOTE(random_state=42)
            X_train, y_train = smote.fit_resample(X_train, y_train)
            
            unique_after, counts_after = np.unique(y_train, return_counts=True)
            logger.info(f"‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á SMOTE: {dict(zip(unique_after, counts_after))}")
        
        results = {}
        best_model_name = None
        
        for name, model in self.models.items():
            logger.info(f"   üîÑ ‡πÄ‡∏ó‡∏£‡∏ô {name}...")
            
            try:
                # ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•
                model.fit(X_train, y_train)
                
                # ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•
                y_pred = model.predict(X_test)
                
                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏°‡∏ï‡∏£‡∏¥‡∏Å‡∏ï‡πà‡∏≤‡∏á‡πÜ
                accuracy = accuracy_score(y_test, y_pred)
                f1 = f1_score(y_test, y_pred, average='weighted')
                precision = precision_score(y_test, y_pred, average='weighted')
                recall = recall_score(y_test, y_pred, average='weighted')
                
                results[name] = {
                    'model': model,
                    'accuracy': accuracy,
                    'f1_score': f1,
                    'precision': precision,
                    'recall': recall,
                    'predictions': y_pred
                }
                
                logger.info(f"      ‚úÖ {name}: Accuracy={accuracy:.3f}, F1={f1:.3f}, Precision={precision:.3f}, Recall={recall:.3f}")
                
                # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
                if accuracy > self.best_score:
                    self.best_score = accuracy
                    self.best_model = model
                    best_model_name = name
                    
            except Exception as e:
                logger.warning(f"      ‚ö†Ô∏è {name} ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ó‡∏£‡∏ô‡πÑ‡∏î‡πâ: {e}")
                continue

        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• Random Forest
        self.trained_models = {}
        if 'RandomForest' in results and 'model' in results['RandomForest']:
            self.trained_models['rf'] = results['RandomForest']['model']

        logger.info(f"üèÜ ‡πÇ‡∏°‡πÄ‡∏î‡∏• Random Forest (Accuracy: {self.best_score:.3f})")
        logger.info(f"üì¶ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•: {list(self.trained_models.keys())}")

        return results
    
    def build_model_data(self, course_profiles=None, training_results=None):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á model_data dict ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å (‡πÑ‡∏°‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏≠‡∏á ‚Äî ‡πÉ‡∏´‡πâ storage ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£)"""
        if self.best_model is None:
            return None
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á performance_metrics ‡∏ó‡∏µ‡πà serializable (‡πÑ‡∏°‡πà‡πÄ‡∏Å‡πá‡∏ö model objects / numpy arrays)
        clean_metrics = {}
        if training_results:
            for name, res in training_results.items():
                clean_metrics[name] = {
                    'accuracy': float(res.get('accuracy', 0)),
                    'f1_score': float(res.get('f1_score', 0)),
                    'precision': float(res.get('precision', 0)),
                    'recall': float(res.get('recall', 0)),
                }
        return {
            'models': self.trained_models if self.trained_models else {'rf': self.best_model},
            'scaler': self.scaler,
            'feature_columns': [
                'current_gpa', 'gpa_std', 'min_grade', 'max_grade', 'grade_range',
                'high_grades', 'medium_grades', 'low_grades',
                'avg_course_difficulty', 'avg_course_fail_rate',
                'num_killer_courses', 'num_easy_courses',
                'num_failed_courses', 'killer_course_ratio', 'gpa_gap',
                'recent_trend', 'progress_ratio', 'consistency_score',
                'academic_momentum', 'risk_score',
                'credits_earned', 'graduation_progress', 'credits_per_semester',
                'on_track_ratio', 'semester_progress',
                'weighted_difficulty', 'hard_course_gpa', 'grade_variance_exposure',
                'num_retake_courses', 'retake_f_count',
                'retake_ratio', 'avg_retake_improvement'
            ],
            'course_profiles': course_profiles or getattr(self, 'course_profiles', {}),
            'course_credit_map': self.course_credit_map,
            'grade_mapping': self.grade_mapping,
            'data_format': 'subject_based',
            'training_type': 'tan1_advanced',
            'created_at': datetime.now().isoformat(),
            'performance_metrics': clean_metrics,
        }
    
    def load_model(self, model_path):
        """‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞ scaler"""
        try:
            model_data = joblib.load(model_path)
            # Support both old format ('model') and new unified format ('models')
            if 'models' in model_data:
                # New unified format ‚Äî pick the first available model
                models = model_data['models']
                self.best_model = next(iter(models.values()))
            else:
                self.best_model = model_data['model']
            self.scaler = model_data['scaler']
            self.course_credit_map = model_data.get('course_credit_map', {})
            self.grade_mapping = model_data.get('grade_mapping', self.grade_mapping)
            if 'course_profiles' in model_data:
                self.course_profiles = model_data['course_profiles']
            return True
        except Exception as e:
            print(f"‚ö†Ô∏è Error loading model: {str(e)}")
            return False  # Don't create fake fallback
    
    # _create_fallback_model removed ‚Äî an untrained model would crash with NotFittedError.
    # If load_model() fails it now returns False so callers fall back to basic prediction.

# ‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á EnhancedPredictionSystem
class EnhancedPredictionSystem:
    def __init__(self):
        self.grade_mapping = {
            'A': 4.0, 'B+': 3.5, 'B': 3.0, 'C+': 2.5, 'C': 2.0,
            'D+': 1.5, 'D': 1.0, 'F': 0.0,
            'W': None, 'S': None, 'S*': None, 'AU': None, 'I': None, 'U': None
        }
        self.advanced_trainer = AdvancedModelTrainer()
        self.advanced_model = None
        self.course_profiles = None
        self.scaler = None
        self.course_credit_map = {}
        
    def load_advanced_model(self, model_path=None):
        """‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏•‡πâ‡∏ß"""
        try:
            if model_path and os.path.exists(model_path):
                if self.advanced_trainer.load_model(model_path):
                    self.advanced_model = self.advanced_trainer.best_model
                    self.scaler = self.advanced_trainer.scaler
                    self.course_credit_map = self.advanced_trainer.course_credit_map
                    # Load course_profiles for consistent feature engineering
                    self.course_profiles = getattr(self.advanced_trainer, 'course_profiles', {})
                    self.advanced_trainer.course_profiles = self.course_profiles
                    print(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {model_path}")
                    return True
            else:
                # ‡∏´‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå models
                models_dir = "models"
                if os.path.exists(models_dir):
                    model_files = [f for f in os.listdir(models_dir) if f.endswith('.joblib')]
                    if model_files:
                        latest_model = max(model_files, key=lambda x: os.path.getctime(os.path.join(models_dir, x)))
                        model_path = os.path.join(models_dir, latest_model)
                        return self.load_advanced_model(model_path)
                
                print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô")
                return False
                
        except Exception as e:
            print(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á: {e}")
            return False
    
    def calculate_gpa(self, grades_dict):
        """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA ‡∏à‡∏≤‡∏Å‡πÄ‡∏Å‡∏£‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö (‡πÉ‡∏ä‡πâ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ß‡∏¥‡∏ä‡∏≤)"""
        if not grades_dict:
            return 0.0
            
        total_points = 0
        total_credits = 0
        
        for subject_id, grade in grades_dict.items():
            if grade in self.grade_mapping and self.grade_mapping[grade] is not None:
                grade_point = self.grade_mapping[grade]
                # ‡πÉ‡∏ä‡πâ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏≤‡∏Å course_credit_map ‡∏´‡∏£‡∏∑‡∏≠ default ‡πÄ‡∏õ‡πá‡∏ô 3
                credits = self.course_credit_map.get(subject_id, 3)
                
                total_points += grade_point * credits
                total_credits += credits
        
        return total_points / total_credits if total_credits > 0 else 0.0
    
    def predict_graduation(self, grades_dict):
        """‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á"""
        try:
            if self.advanced_model is None or self.scaler is None:
                print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô")
                return self.predict_graduation_basic(grades_dict)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if not hasattr(self.advanced_model, 'classes_'):
                print("‚ö†Ô∏è ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏ó‡∏£‡∏ô ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô")
                return self.predict_graduation_basic(grades_dict)
            
            # Use the SAME feature engineering as training
            total_courses_taken = len(grades_dict)
            current_gpa = self.calculate_gpa(grades_dict)
            course_profiles = self.course_profiles or {}
            features = self.advanced_trainer.create_dynamic_features(
                grades_dict, course_profiles, total_courses_taken
            )

            # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏î‡πâ‡∏ß‡∏¢ scaler
            features_scaled = self.scaler.transform([features])
            
            # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•
            prediction = self.advanced_model.predict(features_scaled)[0]
            
            try:
                probability = self.advanced_model.predict_proba(features_scaled)[0]
            except:
                # ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì probability ‡πÑ‡∏î‡πâ
                probability = [0.3, 0.7] if prediction else [0.7, 0.3]
            
            result = {
                'will_graduate': bool(prediction),
                'confidence': float(max(probability)),
                'probability_graduate': float(probability[1]) if len(probability) > 1 else 0.0,
                'probability_not_graduate': float(probability[0]),
                'current_gpa': current_gpa,
                'total_courses': total_courses_taken,
                'model_type': 'advanced'
            }
            
            return result
            
        except Exception as e:
            print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á: {e}")
            return self.predict_graduation_basic(grades_dict)
    
    def predict_graduation_basic(self, grades_dict):
        """‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô (fallback)"""
        current_gpa = self.calculate_gpa(grades_dict)
        total_courses = len(grades_dict)
        
        # ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô: GPA >= 2.0 ‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏Ñ‡∏£‡∏ö 45 ‡∏ß‡∏¥‡∏ä‡∏≤ (‡∏™‡∏°‡∏°‡∏ï‡∏¥)
        will_graduate = current_gpa >= 2.0 and total_courses >= 45
        confidence = min(0.8, max(0.5, current_gpa / 4.0)) if will_graduate else 0.3
        
        return {
            'will_graduate': will_graduate,
            'confidence': confidence,
            'probability_graduate': confidence if will_graduate else 1 - confidence,
            'probability_not_graduate': 1 - confidence if will_graduate else confidence,
            'current_gpa': current_gpa,
            'total_courses': total_courses,
            'model_type': 'basic'
        }

# ‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á Flask endpoints
    def analyze_performance(self, grades_dict):
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°"""
        if not grades_dict:
            return {
                "gpa": 0.0,
                "total_subjects": 0,
                "grade_distribution": {},
                "strengths": ["‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏£‡∏î"],
                "weaknesses": ["‡∏Ñ‡∏ß‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ú‡∏•‡∏á‡∏≤‡∏ô"],
                "recommendations": ["‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏ö‡πâ‡∏≤‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏°‡πà‡∏≥‡πÄ‡∏™‡∏°‡∏≠"],
                "risk_level": "‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö"
            }
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        gpa = self.calculate_gpa(grades_dict)
        grade_counts = Counter(grades_dict.values())
        total_subjects = len(grades_dict)
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏à‡∏∏‡∏î‡πÅ‡∏Ç‡πá‡∏á
        strengths = []
        if grade_counts.get('A', 0) >= 2:
            strengths.append("‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏π‡∏á‡πÉ‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤")
        if grade_counts.get('B+', 0) + grade_counts.get('A', 0) >= total_subjects * 0.6:
            strengths.append("‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°‡∏î‡∏µ‡∏°‡∏≤‡∏Å")
        if gpa >= 3.5:
            strengths.append("‡∏°‡∏µ GPA ‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÄ‡∏Å‡∏µ‡∏¢‡∏£‡∏ï‡∏¥‡∏ô‡∏¥‡∏¢‡∏°")
        if total_subjects >= 6:
            strengths.append("‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏†‡∏≤‡∏£‡∏∞‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ")
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏à‡∏∏‡∏î‡∏≠‡πà‡∏≠‡∏ô
        weaknesses = []
        if grade_counts.get('F', 0) > 0:
            weaknesses.append("‡∏°‡∏µ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ã‡πâ‡∏≥")
        if grade_counts.get('D', 0) + grade_counts.get('D+', 0) > 0:
            weaknesses.append("‡∏°‡∏µ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ï‡πà‡∏≥ ‡∏Ñ‡∏ß‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á")
        if gpa < 2.0:
            weaknesses.append("GPA ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥")
        elif gpa < 2.5:
            weaknesses.append("GPA ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏û‡∏≠‡πÉ‡∏ä‡πâ ‡∏Ñ‡∏ß‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°
        recommendations = []
        
        # ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ï‡∏≤‡∏° GPA
        if gpa >= 3.5:
            recommendations.extend([
                "üéØ ‡∏Ñ‡∏ß‡∏£‡∏•‡∏á‡∏ß‡∏¥‡∏ä‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏µ‡πà‡∏ó‡πâ‡∏≤‡∏ó‡∏≤‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏ó‡∏±‡∏Å‡∏©‡∏∞",
                "üìö ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏ó‡∏≥‡πÇ‡∏Ñ‡∏£‡∏á‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡∏´‡∏£‡∏∑‡∏≠ Independent Study",
                "üèÜ ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ï‡∏±‡∏ß‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ï‡πà‡∏≠‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏ó‡∏∏‡∏ô‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤",
                "üë• ‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÅ‡∏Ç‡πà‡∏á‡∏Ç‡∏±‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£"
            ])
        elif gpa >= 3.0:
            recommendations.extend([
                "üìñ ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏ô‡πâ‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡πâ‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏Å‡∏£‡πà‡∏á‡∏Å‡πà‡∏≠‡∏ô‡∏•‡∏á‡∏ß‡∏¥‡∏ä‡∏≤‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á",
                "‚è∞ ‡∏à‡∏±‡∏î‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏ö‡∏ó‡∏ß‡∏ô‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏°‡πà‡∏≥‡πÄ‡∏™‡∏°‡∏≠",
                "üë®‚Äçüè´ ‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‡πÉ‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡∏à‡∏≥",
                "üìù ‡∏ó‡∏≥‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÉ‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏Å"
            ])
        elif gpa >= 2.5:
            recommendations.extend([
                "‚öñÔ∏è ‡∏Ñ‡∏ß‡∏£‡∏•‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ï‡πà‡∏≠‡πÄ‡∏ó‡∏≠‡∏°‡πÅ‡∏•‡∏∞‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û",
                "ü§ù ‡∏´‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ä‡πà‡∏ß‡∏¢‡∏™‡∏≠‡∏ô",
                "üîÑ ‡∏õ‡∏£‡∏±‡∏ö‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á",
                "üìä ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏Å‡∏•‡πâ‡∏ä‡∏¥‡∏î"
            ])
        elif gpa >= 2.0:
            recommendations.extend([
                "üö® ‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÄ‡∏£‡πà‡∏á‡∏î‡πà‡∏ß‡∏ô",
                "üìö ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ã‡πâ‡∏≥‡πÉ‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô",
                "üí° ‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô",
                "üéØ ‡πÄ‡∏ô‡πâ‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏´‡∏•‡∏±‡∏Å‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡∏ä‡∏≤‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏Å‡πà‡∏≠‡∏ô"
            ])
        else:
            recommendations.extend([
                "üÜò ‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏à‡∏≤‡∏Å‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‡πÅ‡∏•‡∏∞‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏£‡∏±‡∏ß",
                "üìã ‡∏ó‡∏ö‡∏ó‡∏ß‡∏ô‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÅ‡∏•‡∏∞‡πÅ‡∏£‡∏á‡∏à‡∏π‡∏á‡πÉ‡∏à",
                "üîß ‡∏õ‡∏£‡∏±‡∏ö‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏´‡∏°‡πà‡∏´‡∏°‡∏î",
                "‚öïÔ∏è ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏±‡∏à‡∏à‡∏±‡∏¢‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß"
            ])
        
        # ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ï‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÄ‡∏Å‡∏£‡∏î
        if grade_counts.get('F', 0) > 0:
            recommendations.append(f"üî¥ ‡∏°‡∏µ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô {grade_counts['F']} ‡∏ß‡∏¥‡∏ä‡∏≤ - ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ã‡πâ‡∏≥")
        
        if grade_counts.get('A', 0) >= 3:
            recommendations.append("‚≠ê ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏π‡∏á - ‡∏Ñ‡∏ß‡∏£‡∏ó‡πâ‡∏≤‡∏ó‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô")
        
        # ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤
        if total_subjects < 5:
            recommendations.append("üìà ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡∏±‡∏á‡∏ô‡πâ‡∏≠‡∏¢ - ‡∏Ñ‡∏ß‡∏£‡∏™‡∏∞‡∏™‡∏°‡∏ú‡∏•‡∏á‡∏≤‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°")
        elif total_subjects > 25:
            recommendations.append("üí™ ‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏°‡∏≤‡∏Å - ‡πÉ‡∏ä‡πâ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ó‡∏µ‡πà‡∏™‡∏±‡πà‡∏á‡∏™‡∏°")
        
        # ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á
        if gpa >= 3.5:
            risk_level = "‡∏ï‡πà‡∏≥ - ‡∏°‡∏µ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏™‡∏π‡∏á"
        elif gpa >= 2.5:
            risk_level = "‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á - ‡∏Ñ‡∏ß‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏£‡∏∞‡∏î‡∏±‡∏ö"
        elif gpa >= 2.0:
            risk_level = "‡∏™‡∏π‡∏á - ‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÄ‡∏£‡πà‡∏á‡∏î‡πà‡∏ß‡∏ô"
        else:
            risk_level = "‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å - ‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏±‡∏á"
        
        return {
            "gpa": round(gpa, 2),
            "total_subjects": total_subjects,
            "grade_distribution": dict(grade_counts),
            "strengths": strengths if strengths else ["‡∏°‡∏µ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ"],
            "weaknesses": weaknesses if weaknesses else ["‡∏Ñ‡∏ß‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ"],
            "recommendations": recommendations,
            "risk_level": risk_level
        }
    
    def predict_future_performance(self, current_grades, model=None):
        """‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏• AI ‡∏à‡∏£‡∏¥‡∏á"""
        current_gpa = self.calculate_gpa(current_grades)
        
        # ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á‡∏Å‡πà‡∏≠‡∏ô
        if self.advanced_model and self.course_profiles:
            try:
                # ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏•‡πâ‡∏ß
                features = self.advanced_trainer.create_dynamic_features(current_grades, self.course_profiles, len(current_grades))
                
                # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á
                prediction_result = self.advanced_model.predict([features])[0]
                prediction_proba = self.advanced_model.predict_proba([features])[0]
                
                confidence = float(max(prediction_proba))
                
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÄ‡∏Å‡∏£‡∏î‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á
                predicted_grades = self._generate_grade_distribution_from_model(
                    prediction_result, confidence, current_gpa
                )
                
                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
                predicted_gpa = self._calculate_predicted_gpa(predicted_grades, current_gpa)
                
                # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°
                if predicted_gpa > current_gpa + 0.1:
                    trend = "improving"
                elif predicted_gpa < current_gpa - 0.1:
                    trend = "declining"
                else:
                    trend = "stable"
                
                return {
                    "predicted_grades": predicted_grades,
                    "predicted_gpa": round(predicted_gpa, 2),
                    "confidence": round(confidence, 3),
                    "trend": trend,
                    "ai_prediction": {
                        "result": prediction_result,
                        "confidence": confidence,
                        "model_used": True,
                        "model_type": "Advanced AI Model"
                    },
                    "analysis_note": f"‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ Advanced AI ‡∏à‡∏≤‡∏Å {len(current_grades)} ‡∏ß‡∏¥‡∏ä‡∏≤, GPA ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô {current_gpa:.2f}"
                }
                
            except Exception as e:
                logger.warning(f"Advanced model prediction failed: {e}")
        
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á ‡∏´‡∏£‡∏∑‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏õ‡∏Å‡∏ï‡∏¥
        if model and current_grades:
            try:
                # Use the same feature engineering as training
                features = self.advanced_trainer.create_dynamic_features(
                    current_grades, self.course_profiles or {}, len(current_grades)
                )
                
                # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏õ‡∏Å‡∏ï‡∏¥
                prediction_result = model.predict([features])[0]
                prediction_proba = model.predict_proba([features])[0]
                
                confidence = float(max(prediction_proba))
                
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÄ‡∏Å‡∏£‡∏î‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
                predicted_grades = self._generate_grade_distribution_from_model(
                    prediction_result, confidence, current_gpa
                )
                
                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
                predicted_gpa = self._calculate_predicted_gpa(predicted_grades, current_gpa)
                
                # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°
                if predicted_gpa > current_gpa + 0.1:
                    trend = "improving"
                elif predicted_gpa < current_gpa - 0.1:
                    trend = "declining"
                else:
                    trend = "stable"
                
                return {
                    "predicted_grades": predicted_grades,
                    "predicted_gpa": round(predicted_gpa, 2),
                    "confidence": round(confidence, 3),
                    "trend": trend,
                    "ai_prediction": {
                        "result": prediction_result,
                        "confidence": confidence,
                        "model_used": True,
                        "model_type": "Standard AI Model"
                    },
                    "analysis_note": f"‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ Standard AI ‡∏à‡∏≤‡∏Å {len(current_grades)} ‡∏ß‡∏¥‡∏ä‡∏≤, GPA ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô {current_gpa:.2f}"
                }
                
            except Exception as e:
                logger.warning(f"Standard AI model prediction failed: {e}")
        
        # ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏™‡∏≥‡∏£‡∏≠‡∏á‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
        return self._fallback_prediction(current_gpa, current_grades)
    
    def _prepare_advanced_features(self, grades):
        """‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• AI"""
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        gpa = self.calculate_gpa(grades)
        total_subjects = len(grades)
        grade_counts = Counter(grades.values())
        
        # ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        features = [
            gpa,  # GPA ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
            total_subjects,  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
            total_subjects * 3,  # ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏™‡∏∞‡∏™‡∏° (‡∏™‡∏°‡∏°‡∏ï‡∏¥ 3 ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï/‡∏ß‡∏¥‡∏ä‡∏≤)
        ]
        
        # ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÄ‡∏Å‡∏£‡∏î
        for grade in ['A', 'B+', 'B', 'C+', 'C', 'D+', 'D', 'F', 'W']:
            features.append(grade_counts.get(grade, 0))
        
        # ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏Å‡∏£‡∏î
        if total_subjects > 0:
            features.extend([
                grade_counts.get('A', 0) / total_subjects,  # ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡πÄ‡∏Å‡∏£‡∏î A
                grade_counts.get('F', 0) / total_subjects,  # ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡πÄ‡∏Å‡∏£‡∏î F
                (grade_counts.get('A', 0) + grade_counts.get('B+', 0)) / total_subjects,  # ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡πÄ‡∏Å‡∏£‡∏î‡∏î‡∏µ
                (grade_counts.get('D+', 0) + grade_counts.get('D', 0) + grade_counts.get('F', 0)) / total_subjects,  # ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡πÄ‡∏Å‡∏£‡∏î‡πÅ‡∏¢‡πà
            ])
        else:
            features.extend([0, 0, 0, 0])
        
        # ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å
        features.extend([
            1 if gpa >= 3.5 else 0,  # ‡πÄ‡∏Å‡∏£‡∏î‡∏î‡∏µ‡∏°‡∏≤‡∏Å
            1 if gpa < 2.0 else 0,   # ‡πÄ‡∏Å‡∏£‡∏î‡∏ï‡πà‡∏≥‡∏°‡∏≤‡∏Å
            1 if grade_counts.get('F', 0) > 0 else 0,  # ‡πÄ‡∏Ñ‡∏¢‡∏ï‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤
            min(4.0, gpa + 0.2) if gpa >= 3.0 else max(1.0, gpa - 0.1),  # ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏° GPA
        ])
        
        return features
    
    def _generate_grade_distribution_from_model(self, prediction_result, confidence, current_gpa):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÄ‡∏Å‡∏£‡∏î‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏•"""
        # Model outputs 1 (graduated) or 0 (not graduated)
        will_graduate = (prediction_result == 1 or prediction_result is True)

        if will_graduate and confidence > 0.7:
            if current_gpa >= 3.5:
                return {"A": 0.4, "B+": 0.35, "B": 0.20, "C+": 0.05}
            elif current_gpa >= 3.0:
                return {"A": 0.25, "B+": 0.40, "B": 0.25, "C+": 0.10}
            else:
                return {"B+": 0.20, "B": 0.35, "C+": 0.30, "C": 0.15}

        elif will_graduate and confidence <= 0.7:
            return {"B": 0.25, "C+": 0.35, "C": 0.30, "D+": 0.10}

        else:
            return {"C": 0.20, "D+": 0.30, "D": 0.35, "F": 0.15}
    
    def _calculate_predicted_gpa(self, predicted_grades, current_gpa):
        """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÄ‡∏Å‡∏£‡∏î"""
        predicted_gpa = 0
        for grade, probability in predicted_grades.items():
            predicted_gpa += self.grade_mapping.get(grade, 0) * probability
        
        # ‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏™‡∏°‡πÄ‡∏´‡∏ï‡∏∏‡∏™‡∏°‡∏ú‡∏•‡∏Å‡∏±‡∏ö GPA ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
        if abs(predicted_gpa - current_gpa) > 1.0:
            predicted_gpa = current_gpa + (0.2 if predicted_gpa > current_gpa else -0.2)
        
        return max(0.0, min(4.0, predicted_gpa))
    
    def _fallback_prediction(self, current_gpa, current_grades):
        """‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏™‡∏≥‡∏£‡∏≠‡∏á‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô"""
        total_subjects = len(current_grades)
        
        # ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏ï‡∏≤‡∏° GPA
        if current_gpa >= 3.5:
            predicted_grades = {"A": 0.35, "B+": 0.30, "B": 0.25, "C+": 0.10}
            next_term_gpa = min(4.0, current_gpa + 0.05)
            confidence = 0.75
        elif current_gpa >= 3.0:
            predicted_grades = {"A": 0.20, "B+": 0.35, "B": 0.30, "C+": 0.15}
            next_term_gpa = current_gpa + 0.02
            confidence = 0.70
        elif current_gpa >= 2.5:
            predicted_grades = {"B": 0.20, "C+": 0.35, "C": 0.30, "D+": 0.15}
            next_term_gpa = current_gpa
            confidence = 0.65
        else:
            predicted_grades = {"C": 0.25, "D+": 0.30, "D": 0.30, "F": 0.15}
            next_term_gpa = max(1.5, current_gpa - 0.1)
            confidence = 0.60
        
        # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        if total_subjects < 5:
            confidence *= 0.8
        elif total_subjects > 15:
            confidence *= 1.1
        
        confidence = min(0.95, confidence)
        
        return {
            "predicted_grades": predicted_grades,
            "predicted_gpa": round(next_term_gpa, 2),
            "confidence": round(confidence, 3),
            "trend": "improving" if next_term_gpa > current_gpa else "stable" if next_term_gpa == current_gpa else "declining",
            "ai_prediction": {
                "result": "‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö",
                "confidence": confidence,
                "model_used": False
            },
            "analysis_note": f"‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏à‡∏≤‡∏Å {total_subjects} ‡∏ß‡∏¥‡∏ä‡∏≤, GPA ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô {current_gpa:.2f} (‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏°‡πÄ‡∏î‡∏• AI)"
        }
    
    def _prepare_model_features(self, grades):
        """‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• - ‡πÉ‡∏ä‡πâ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á"""
        return self._prepare_advanced_features(grades)
    
    def create_visualization_charts(self, grades_dict, prediction_data):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        
        # ‡∏Å‡∏£‡∏≤‡∏ü 1: ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÄ‡∏Å‡∏£‡∏î
        if grades_dict:
            grade_counts = Counter(grades_dict.values())
            grades = list(grade_counts.keys())
            counts = list(grade_counts.values())
            colors = ['#2E8B57', '#32CD32', '#FFD700', '#FFA500', '#FF6347', '#DC143C']
            
            wedges, texts, autotexts = ax1.pie(counts, labels=grades, autopct='%1.1f%%', 
                                             colors=colors[:len(grades)], startangle=90)
            ax1.set_title('‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÄ‡∏Å‡∏£‡∏î‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô', fontsize=14, fontweight='bold')
        
        # ‡∏Å‡∏£‡∏≤‡∏ü 2: ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö GPA
        current_gpa = self.calculate_gpa(grades_dict)
        predicted_gpa = prediction_data.get('predicted_gpa', current_gpa)
        
        categories = ['GPA ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô', 'GPA ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢', '‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥', '‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏î‡∏µ', '‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏î‡∏µ‡∏°‡∏≤‡∏Å']
        values = [current_gpa, predicted_gpa, 2.0, 2.75, 3.5]
        colors = ['blue', 'red', 'gray', 'orange', 'green']
        
        bars = ax2.bar(categories, values, color=colors)
        ax2.set_ylabel('GPA')
        ax2.set_title('‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö GPA', fontsize=14, fontweight='bold')
        ax2.set_ylim(0, 4)
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡πà‡∏≤‡∏ö‡∏ô‡πÅ‡∏ó‡πà‡∏á‡∏Å‡∏£‡∏≤‡∏ü
        for bar, value in zip(bars, values):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.05,
                    f'{value:.2f}', ha='center', va='bottom', fontweight='bold')
        
        plt.setp(ax2.get_xticklabels(), rotation=45, ha='right')
        
        # ‡∏Å‡∏£‡∏≤‡∏ü 3: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡∏£‡∏î‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï
        predicted_grades = prediction_data.get('predicted_grades', {})
        if predicted_grades:
            grades = list(predicted_grades.keys())
            probs = list(predicted_grades.values())
            colors = plt.cm.RdYlGn([p for p in probs])
            
            bars = ax3.bar(grades, probs, color=colors)
            ax3.set_xlabel('‡πÄ‡∏Å‡∏£‡∏î')
            ax3.set_ylabel('‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô')
            ax3.set_title('‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏ó‡∏≠‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ', fontsize=14, fontweight='bold')
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡πà‡∏≤‡∏ö‡∏ô‡πÅ‡∏ó‡πà‡∏á‡∏Å‡∏£‡∏≤‡∏ü
            for bar, prob in zip(bars, probs):
                height = bar.get_height()
                ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                        f'{prob:.2f}', ha='center', va='bottom', fontweight='bold')
        
        # ‡∏Å‡∏£‡∏≤‡∏ü 4: ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤
        terms = ['‡πÄ‡∏ó‡∏≠‡∏° 1', '‡πÄ‡∏ó‡∏≠‡∏° 2', '‡πÄ‡∏ó‡∏≠‡∏° 3', '‡πÄ‡∏ó‡∏≠‡∏°‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô', '‡πÄ‡∏ó‡∏≠‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ']
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°
        trend_gpas = [2.5, 2.8, 3.1, current_gpa, predicted_gpa]
        
        ax4.plot(terms[:-1], trend_gpas[:-1], 'o-', label='GPA ‡∏à‡∏£‡∏¥‡∏á', linewidth=3, markersize=8, color='blue')
        ax4.plot([terms[-2], terms[-1]], [trend_gpas[-2], trend_gpas[-1]], 'r--', 
                label='GPA ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢', linewidth=3, markersize=8)
        ax4.set_xlabel('‡πÄ‡∏ó‡∏≠‡∏°')
        ax4.set_ylabel('GPA')
        ax4.set_title('‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤ GPA', fontsize=14, fontweight='bold')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        ax4.set_ylim(0, 4)
        
        plt.tight_layout()
        
        # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô base64
        buffer = BytesIO()
        plt.savefig(buffer, format='png', dpi=300, bbox_inches='tight', facecolor='white')
        buffer.seek(0)
        image_base64 = base64.b64encode(buffer.getvalue()).decode()
        plt.close()
        
        return image_base64

# ‡∏™‡∏£‡πâ‡∏≤‡∏á instance ‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö
enhanced_system = EnhancedPredictionSystem()

# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
enhanced_system.load_advanced_model()

warnings.filterwarnings('ignore')

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('app_startup.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ==========================================
# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Environment Variables (deferred - runs after MongoDB loads settings)
# ==========================================
def check_env_variables():
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ R2 environment variables"""
    logger.info("=" * 70)
    logger.info("üîß CLOUDFLARE R2 CONFIGURATION CHECK (post-MongoDB)")
    logger.info("=" * 70)

    env_status = {
        'all_present': True,
        'missing': [],
        'details': {}
    }

    required_vars = {
        'CLOUDFLARE_R2_ACCESS_KEY_ID': 'Access Key for R2 authentication',
        'CLOUDFLARE_R2_SECRET_ACCESS_KEY': 'Secret Key for R2 authentication',
        'CLOUDFLARE_R2_ENDPOINT': 'R2 endpoint URL',
        'CLOUDFLARE_R2_BUCKET_NAME': 'R2 bucket name'
    }

    for var_name, description in required_vars.items():
        value = os.environ.get(var_name, '')

        if value:
            if 'SECRET' in var_name or 'KEY' in var_name:
                display_value = f"{value[:8]}...{value[-4:]}" if len(value) > 12 else "***"
            else:
                display_value = value
            logger.info(f"‚úÖ {var_name}: {display_value}")
            env_status['details'][var_name] = {'present': True, 'length': len(value), 'description': description}
        else:
            logger.warning(f"‚ö†Ô∏è {var_name}: NOT FOUND - {description}")
            env_status['missing'].append(var_name)
            env_status['all_present'] = False
            env_status['details'][var_name] = {'present': False, 'description': description}

    if not env_status['all_present']:
        logger.warning(f"‚ö†Ô∏è R2 missing vars: {', '.join(env_status['missing'])} ‚Äî will use LOCAL storage until set via Admin Settings")
    else:
        logger.info("‚úÖ All R2 credentials found")
    logger.info("=" * 70)
    return env_status

# Create Flask app
ACTIVE_CONFIG = config.get_config()
app = Flask(__name__)
app.config.from_object(ACTIVE_CONFIG)
app.secret_key = os.environ.get('FLASK_SECRET_KEY', secrets.token_hex(32))


class MongoAdminManager:
    """MongoDB-backed admin/auth/settings manager.
    NOTE: MONGODB_URI remains in .env only.
    """

    SETTING_KEYS = [
        'GEMINI_API_KEY',
        'GEMINI_MODEL_NAME',
        'GEMINI_MODEL_FALLBACKS',
        'CLOUDFLARE_R2_ACCESS_KEY_ID',
        'CLOUDFLARE_R2_SECRET_ACCESS_KEY',
        'CLOUDFLARE_R2_ENDPOINT',
        'CLOUDFLARE_R2_BUCKET_NAME',
    ]

    def __init__(self):
        self.enabled = False
        self.init_error = None
        self.client = None
        self.db = None
        self.users = None
        self.settings = None
        self.audit = None
        self.bootstrap_password = None

        self._mongo_uri = os.environ.get('MONGODB_URI', '').strip()
        self._db_name = os.environ.get('MONGODB_DB_NAME', 'student_ai_system').strip()
        self._enc_key = os.environ.get('APP_MASTER_ENCRYPTION_KEY', '').strip()

        if not self._mongo_uri:
            self.init_error = 'MONGODB_URI not configured'
            logger.warning("‚ö†Ô∏è MONGODB_URI not configured. Admin backend is disabled.")
            return
        if MongoClient is None:
            self.init_error = 'pymongo not installed'
            logger.warning("‚ö†Ô∏è pymongo package not installed. Admin backend is disabled.")
            return

        try:
            self.client = MongoClient(self._mongo_uri, serverSelectionTimeoutMS=5000)
            self.client.admin.command('ping')
            self.db = self.client[self._db_name]
            self.users = self.db['users']
            self.settings = self.db['system_settings']
            self.audit = self.db['audit_logs']

            self.users.create_index('username', unique=True)
            self.settings.create_index('key', unique=True)
            self.audit.create_index('created_at')

            self.enabled = True
            self.init_error = None
            self._bootstrap_default_admin()
            logger.info(f"‚úÖ Mongo admin backend ready (db={self._db_name})")
        except Exception as e:
            logger.error(f"‚ùå Mongo admin backend init failed: {e}")
            self.init_error = str(e)
            self.enabled = False

    def _log_audit(self, actor_user_id, action, target, status='success', details=None):
        if not self.enabled:
            return
        try:
            self.audit.insert_one({
                'actor_user_id': actor_user_id,
                'action': action,
                'target': target,
                'status': status,
                'details': details or {},
                'ip_address': request.remote_addr if request else None,
                'user_agent': request.headers.get('User-Agent') if request else None,
                'created_at': datetime.utcnow().isoformat()
            })
        except Exception:
            pass

    def _derive_key(self):
        return hashlib.sha256(self._enc_key.encode('utf-8')).digest()

    def _encrypt(self, raw_value: str) -> str:
        if raw_value is None:
            return ''
        key = self._derive_key()
        raw = raw_value.encode('utf-8')
        out = bytearray(len(raw))
        for i, b in enumerate(raw):
            out[i] = b ^ key[i % len(key)]
        return base64.urlsafe_b64encode(bytes(out)).decode('utf-8')

    def _decrypt(self, encrypted_value: str) -> str:
        if not encrypted_value:
            return ''
        key = self._derive_key()
        raw = base64.urlsafe_b64decode(encrypted_value.encode('utf-8'))
        out = bytearray(len(raw))
        for i, b in enumerate(raw):
            out[i] = b ^ key[i % len(key)]
        return bytes(out).decode('utf-8')

    def _bootstrap_default_admin(self):
        if not self.enabled:
            return
        if self.users.count_documents({}) > 0:
            return

        username = os.environ.get('INITIAL_ADMIN_USERNAME', 'admin')
        temp_password = os.environ.get('INITIAL_ADMIN_PASSWORD', 'admin123')
        self.bootstrap_password = temp_password

        self.users.insert_one({
            'username': username,
            'email': None,
            'password_hash': generate_password_hash(temp_password),
            'role': 'super_admin',
            'must_change_password': True,
            'is_active': True,
            'created_at': datetime.utcnow().isoformat(),
            'updated_at': datetime.utcnow().isoformat(),
            'last_login_at': None
        })

        logger.warning("=" * 70)
        logger.warning("üîê FIRST RUN: Default super_admin account created")
        logger.warning(f"   username: {username}")
        logger.warning(f"   temporary password: {temp_password}")
        logger.warning("   Action required: login and change password immediately")
        logger.warning("=" * 70)

    def apply_runtime_env(self):
        """Load settings from MongoDB and apply to process env (except MONGODB_URI)."""
        if not self.enabled:
            return
        try:
            docs = list(self.settings.find({'key': {'$in': self.SETTING_KEYS}}))
            for doc in docs:
                key = doc.get('key')
                # Plaintext first (new behavior), encrypted fallback (legacy data)
                plain_value = doc.get('value')
                value_encrypted = doc.get('value_encrypted', '')

                if key and plain_value is not None:
                    os.environ[key] = str(plain_value)
                elif key and value_encrypted:
                    os.environ[key] = self._decrypt(value_encrypted)
        except Exception as e:
            logger.warning(f"Could not apply runtime env from MongoDB: {e}")

    def authenticate(self, username: str, password: str):
        if not self.enabled:
            return None
        user = self.users.find_one({'username': username, 'is_active': True})
        if not user:
            return None
        if not check_password_hash(user.get('password_hash', ''), password):
            return None
        self.users.update_one(
            {'_id': user['_id']},
            {'$set': {'last_login_at': datetime.utcnow().isoformat()}}
        )
        return user

    def change_password(self, user_id, new_password: str):
        if not self.enabled:
            return False
        self.users.update_one(
            {'_id': user_id},
            {'$set': {
                'password_hash': generate_password_hash(new_password),
                'must_change_password': False,
                'updated_at': datetime.utcnow().isoformat()
            }}
        )
        return True

    def set_setting(self, key: str, value: str, actor_user_id=None):
        if not self.enabled:
            return False
        self.settings.update_one(
            {'key': key},
            {'$set': {
                'key': key,
                # Save as plaintext by requirement
                'value': value or '',
                # keep legacy field for compatibility (clear when writing new value)
                'value_encrypted': '',
                'updated_by': str(actor_user_id) if actor_user_id else None,
                'updated_at': datetime.utcnow().isoformat()
            }},
            upsert=True
        )
        self._log_audit(actor_user_id, 'UPDATE_SETTING', key)
        return True

    def get_setting(self, key: str, default: str = ''):
        if not self.enabled:
            return default
        doc = self.settings.find_one({'key': key})
        if not doc:
            return default
        try:
            # Plaintext first (new behavior)
            if 'value' in doc and doc.get('value') is not None:
                return str(doc.get('value'))
            # Encrypted fallback (legacy data)
            encrypted = doc.get('value_encrypted', '')
            if encrypted:
                return self._decrypt(encrypted)
            return default
        except Exception:
            return default

    @staticmethod
    def mask_value(raw: str):
        if not raw:
            return ''
        if len(raw) <= 6:
            return '***'
        return f"{raw[:4]}***{raw[-2:]}"


admin_manager = MongoAdminManager()
admin_manager.apply_runtime_env()

# Now that MongoDB has loaded settings into os.environ, check R2 config
check_env_variables()

COURSE_LOOKUP = {course['id']: course for course in getattr(ACTIVE_CONFIG, 'COURSES_DATA', [])}
GRADE_POINT_MAP = ACTIVE_CONFIG.DATA_CONFIG.get('grade_mapping', {})

GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY')
# Use gemini-3-flash-preview as default (Gemini 3 models are still in preview)
# Fallback chain: gemini-3-flash-preview ‚Üí gemini-3-pro-preview ‚Üí gemini-2.5-flash ‚Üí gemini-2.5-pro
# Reference: https://ai.google.dev/gemini-api/docs/models
GEMINI_MODEL_NAME = os.environ.get('GEMINI_MODEL_NAME', 'gemini-3-flash-preview')
GEMINI_MAX_FILE_SIZE_MB = float(os.environ.get('GEMINI_MAX_FILE_SIZE_MB', 5))
# Default fallback models for high availability (in order of preference)
GEMINI_DEFAULT_FALLBACKS = ['gemini-3-flash-preview', 'gemini-3-pro-preview', 'gemini-2.5-flash', 'gemini-2.5-flash-lite', 'gemini-2.5-pro']

# Deprecated model name ‚Üí recommended replacement (auto-fix on startup)
GEMINI_DEPRECATED_MODELS = {
    'gemini-1.5-flash': 'gemini-3-flash-preview',
    'gemini-1.5-pro': 'gemini-3-pro-preview',
    'gemini-2.5-flash-preview-05-20': 'gemini-2.5-flash',
    'gemini-2.5-pro-preview-05-06': 'gemini-2.5-pro',
    'gemini-3-flash': 'gemini-3-flash-preview',
    'gemini-3-pro': 'gemini-3-pro-preview',
    'gemini-2.0-flash': 'gemini-2.5-flash',
    'gemini-2.0-flash-lite': 'gemini-2.5-flash-lite',
}

# Auto-fix deprecated model name stored in env/MongoDB
if GEMINI_MODEL_NAME in GEMINI_DEPRECATED_MODELS:
    _old_model = GEMINI_MODEL_NAME
    GEMINI_MODEL_NAME = GEMINI_DEPRECATED_MODELS[_old_model]
    os.environ['GEMINI_MODEL_NAME'] = GEMINI_MODEL_NAME
    logger.warning(f"‚ö†Ô∏è Auto-corrected deprecated model '{_old_model}' ‚Üí '{GEMINI_MODEL_NAME}'")

# Known Gemini models for the admin dropdown (label ‚Üí model id)
# Use exact model IDs from Google AI ‚Äî click üîÑ in admin to fetch live list
GEMINI_KNOWN_MODELS = [
    {'id': 'gemini-3-flash-preview', 'name': 'Gemini 3 Flash', 'desc': '‚≠ê ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ ‚Äî ‡πÄ‡∏£‡πá‡∏ß ‡∏™‡∏°‡∏î‡∏∏‡∏• ‡∏£‡∏∏‡πà‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î (Preview)'},
    {'id': 'gemini-3-pro-preview', 'name': 'Gemini 3 Pro', 'desc': 'üß† ‡∏â‡∏•‡∏≤‡∏î‡∏™‡∏∏‡∏î ‡∏ó‡∏£‡∏á‡∏û‡∏•‡∏±‡∏á‡∏™‡∏∏‡∏î (Preview)'},
    {'id': 'gemini-2.5-flash', 'name': 'Gemini 2.5 Flash', 'desc': '‡πÄ‡∏£‡πá‡∏ß ‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£ ‡∏ü‡∏£‡∏µ'},
    {'id': 'gemini-2.5-flash-lite', 'name': 'Gemini 2.5 Flash Lite', 'desc': '‡πÄ‡∏ö‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î ‡∏ü‡∏£‡∏µ'},
    {'id': 'gemini-2.5-pro', 'name': 'Gemini 2.5 Pro', 'desc': '‡∏â‡∏•‡∏≤‡∏î‡∏™‡∏∏‡∏î ‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤'},
]


def _build_gemini_model_candidates(primary_name: str) -> List[str]:
    """Collect preferred + fallback Gemini model names with preserved order."""
    candidates: List[str] = []

    def _add(name: Optional[str]):
        if name and name not in candidates:
            candidates.append(name)

    _add(primary_name)

    fallback_env = os.environ.get('GEMINI_MODEL_FALLBACKS', '')
    if fallback_env:
        for item in fallback_env.split(','):
            _add(item.strip())

    # Add default fallback models from centralized constant
    for default_name in GEMINI_DEFAULT_FALLBACKS:
        _add(default_name)

    return candidates


GEMINI_MODEL_CANDIDATES = _build_gemini_model_candidates(GEMINI_MODEL_NAME)
gemini_client_ready = False

if genai and GEMINI_API_KEY:
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        gemini_client_ready = True
        logger.info(f"‚úÖ Gemini API initialized with model {GEMINI_MODEL_NAME}")
    except Exception as gemini_init_error:
        gemini_client_ready = False
        logger.error(f"‚ùå Failed to initialize Gemini API: {gemini_init_error}")
elif not GEMINI_API_KEY:
    logger.info("‚ÑπÔ∏è GEMINI_API_KEY not found. Gemini routes will be disabled.")
else:
    logger.warning("‚ö†Ô∏è google-generativeai package not available. Install to enable Gemini features.")


def refresh_gemini_runtime_from_settings():
    """Reload Gemini runtime variables from Mongo settings (if enabled)."""
    global GEMINI_API_KEY, GEMINI_MODEL_NAME, GEMINI_MODEL_CANDIDATES, gemini_client_ready

    if admin_manager.enabled:
        admin_manager.apply_runtime_env()

    GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY')
    GEMINI_MODEL_NAME = os.environ.get('GEMINI_MODEL_NAME', 'gemini-3-flash-preview')
    # Auto-fix deprecated model names
    if GEMINI_MODEL_NAME in GEMINI_DEPRECATED_MODELS:
        _old = GEMINI_MODEL_NAME
        GEMINI_MODEL_NAME = GEMINI_DEPRECATED_MODELS[_old]
        os.environ['GEMINI_MODEL_NAME'] = GEMINI_MODEL_NAME
        logger.warning(f"‚ö†Ô∏è Auto-corrected deprecated model '{_old}' ‚Üí '{GEMINI_MODEL_NAME}'")
        # Also update MongoDB if available
        if admin_manager.enabled:
            try:
                admin_manager.set_setting('GEMINI_MODEL_NAME', GEMINI_MODEL_NAME)
            except Exception:
                pass
    GEMINI_MODEL_CANDIDATES = _build_gemini_model_candidates(GEMINI_MODEL_NAME)

    gemini_client_ready = False
    if genai and GEMINI_API_KEY:
        try:
            genai.configure(api_key=GEMINI_API_KEY)
            gemini_client_ready = True
            logger.info(f"‚úÖ Gemini runtime refreshed with model {GEMINI_MODEL_NAME}")
        except Exception as gemini_init_error:
            gemini_client_ready = False
            logger.error(f"‚ùå Failed to refresh Gemini runtime: {gemini_init_error}")


def _json_error(message: str, status: int = 403):
    return jsonify({'success': False, 'error': message}), status


def current_user_role() -> str:
    return session.get('user_role', 'anonymous')


def has_any_role(*roles) -> bool:
    return current_user_role() in roles


def is_logged_in() -> bool:
    return bool(session.get('user_id'))


def has_fallback_admin_auth() -> bool:
    """Fallback admin auth via environment variables when MongoDB backend is unavailable."""
    return bool(os.environ.get('ADMIN_USERNAME') and os.environ.get('ADMIN_PASSWORD'))


def admin_login_required(view_func):
    @wraps(view_func)
    def wrapper(*args, **kwargs):
        if not is_logged_in():
            if request.path.startswith('/api/'):
                return _json_error('Unauthorized', 401)
            return redirect(url_for('admin_login', next=request.path))
        return view_func(*args, **kwargs)
    return wrapper


def roles_required(*roles):
    def decorator(view_func):
        @wraps(view_func)
        def wrapper(*args, **kwargs):
            if not is_logged_in():
                return _json_error('Unauthorized', 401)
            if not has_any_role(*roles):
                return _json_error('Forbidden', 403)
            return view_func(*args, **kwargs)
        return wrapper
    return decorator


@app.before_request
def enforce_first_login_password_change():
    if not admin_manager.enabled:
        return None

    endpoint = request.endpoint or ''
    allow_endpoints = {
        'admin_login',
        'admin_logout',
        'admin_change_password',
        'static'
    }

    if not session.get('user_id'):
        return None

    if session.get('must_change_password') and endpoint not in allow_endpoints:
        if request.path.startswith('/api/'):
            return _json_error('Password change required before using this endpoint', 403)
        return redirect(url_for('admin_change_password'))

    return None


@app.before_request
def enforce_login_for_non_prediction_pages():
    """Allow anonymous access only to home, prediction page and required prediction APIs."""
    endpoint = request.endpoint or ''

    public_endpoints = {
        'static',
        'admin_login',
        'admin_logout',
        'admin_change_password',

        # Public pages
        'main_page',
        'curriculum_prediction_form',
        'predict_batch_page',
        'batch_prediction_page',

        # Public prediction APIs
        'list_models',
        'get_config_for_frontend',
        'analyze_curriculum',
        'gemini_predict_route',
        'explain_prediction',
        'get_three_line_chart_data',
        'get_graduation_analysis',
        'get_next_term_prediction',
        'get_comprehensive_analysis',
        'predict_manual_input',
        'api_predict_batch',
        'api_batch_template',
        'api_batch_gemini_analyze',
    }

    if endpoint in public_endpoints:
        return None

    # Force landing page to prediction page for anonymous users
    if endpoint == 'index' and not session.get('user_id'):
        return redirect(url_for('curriculum_prediction_form'))

    if not session.get('user_id'):
        if request.path.startswith('/api/'):
            return _json_error('Unauthorized', 401)
        return redirect(url_for('admin_login', next=request.path))

    return None

# ==========================================
# RETRY MECHANISM ‡πÅ‡∏•‡∏∞ RATE LIMITER ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GEMINI API
# ==========================================

def retry_on_quota_error(max_retries=3, initial_delay=20):
    """Decorator ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö retry ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏à‡∏≠ quota error
    
    Args:
        max_retries: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏≠‡∏á (‡∏£‡∏ß‡∏°‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å)
        initial_delay: ‡πÄ‡∏ß‡∏•‡∏≤‡∏£‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô (‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            delay = initial_delay
            
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    error_msg = str(e)
                    is_quota_error = ('429' in error_msg or 
                                     'quota' in error_msg.lower() or 
                                     'resource exhausted' in error_msg.lower())
                    
                    if is_quota_error and attempt < max_retries - 1:
                        logger.warning(f"Quota exceeded, retrying in {delay}s (attempt {attempt+1}/{max_retries})")
                        time.sleep(delay)
                        delay *= 2  # Exponential backoff
                        continue
                    
                    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà quota error ‡∏´‡∏£‡∏∑‡∏≠‡∏´‡∏°‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô retry ‡πÅ‡∏•‡πâ‡∏ß ‡πÉ‡∏´‡πâ raise
                    raise
                
        return wrapper
    return decorator


class RateLimiter:
    """Rate limiter ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Gemini API"""
    def __init__(self, max_requests=10, time_window=60):
        self.max_requests = max_requests
        self.time_window = timedelta(seconds=time_window)
        self.requests = deque()
    
    def can_proceed(self):
        now = datetime.now()
        # ‡∏•‡∏ö request ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πà‡∏≤‡πÄ‡∏Å‡∏¥‡∏ô‡∏Å‡∏ß‡πà‡∏≤ time window
        while self.requests and now - self.requests[0] > self.time_window:
            self.requests.popleft()
        
        if len(self.requests) < self.max_requests:
            self.requests.append(now)
            return True, None
        else:
            wait_time = (self.requests[0] + self.time_window - now).total_seconds()
            # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô negative wait time ‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
            return False, max(1, int(wait_time))


# ‡∏™‡∏£‡πâ‡∏≤‡∏á rate limiter instance
gemini_rate_limiter = RateLimiter(max_requests=10, time_window=60)

# ==========================================
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
# ==========================================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
UPLOAD_FOLDER = os.path.join(BASE_DIR, 'uploads')
MODEL_FOLDER = os.path.join(BASE_DIR, 'models')

app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MODEL_FOLDER'] = MODEL_FOLDER

logger.info("=" * 70)
logger.info("üìÅ DIRECTORY SETUP")
logger.info("=" * 70)

for folder_name, folder_path in [('uploads', UPLOAD_FOLDER), ('models', MODEL_FOLDER)]:
    try:
        if not os.path.exists(folder_path):
            os.makedirs(folder_path, exist_ok=True)
            logger.info(f"‚úÖ Created {folder_name} folder: {folder_path}")
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå .gitkeep ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ git track ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ß‡πà‡∏≤‡∏á
            gitkeep_path = os.path.join(folder_path, '.gitkeep')
            if not os.path.exists(gitkeep_path):
                with open(gitkeep_path, 'w') as f:
                    f.write('')
                logger.info(f"   Created .gitkeep in {folder_name}")
        else:
            logger.info(f"‚úÖ {folder_name} folder exists: {folder_path}")
            
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö permissions
        if os.access(folder_path, os.W_OK):
            logger.info(f"   ‚úÖ Write permission: OK")
        else:
            logger.error(f"   ‚ùå Write permission: DENIED")
            
        # ‡πÅ‡∏™‡∏î‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà
        files = os.listdir(folder_path)
        if files:
            logger.info(f"   Files in {folder_name}: {len(files)} file(s)")
            for file in files[:5]:  # ‡πÅ‡∏™‡∏î‡∏á‡πÅ‡∏Ñ‡πà 5 ‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏£‡∏Å
                file_path = os.path.join(folder_path, file)
                file_size = os.path.getsize(file_path) if os.path.isfile(file_path) else 0
                logger.info(f"     - {file} ({file_size} bytes)")
                
    except Exception as e:
        logger.error(f"‚ùå Error with {folder_name} folder: {str(e)}")
        logger.error(f"   Current working directory: {os.getcwd()}")
        logger.error(f"   Python executable: {sys.executable}")

# Variables to store loaded models
models = {
    'subject_model': None,
    'gpa_model': None,
    'subject_model_info': None,
    'gpa_model_info': None,
    'subject_feature_cols': None,
    'gpa_feature_cols': None
}

# ==========================================
# Enhanced S3 Storage Class with Detailed Error Reporting
# ==========================================
class S3Storage:
    def __init__(self):
        """Initialize S3 client with detailed error reporting"""
        self.connection_errors = []
        self.use_local = False
        self.s3_client = None
        
        logger.info("=" * 70)
        logger.info("üöÄ INITIALIZING CLOUDFLARE R2 STORAGE")
        logger.info("=" * 70)
        
        try:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö credentials
            self.access_key = os.environ.get('CLOUDFLARE_R2_ACCESS_KEY_ID', '')
            self.secret_key = os.environ.get('CLOUDFLARE_R2_SECRET_ACCESS_KEY', '')
            self.endpoint_url = os.environ.get('CLOUDFLARE_R2_ENDPOINT', '')
            self.bucket_name = os.environ.get('CLOUDFLARE_R2_BUCKET_NAME', 'pjai')
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ credential
            missing_creds = []
            if not self.access_key:
                missing_creds.append('CLOUDFLARE_R2_ACCESS_KEY_ID')
            if not self.secret_key:
                missing_creds.append('CLOUDFLARE_R2_SECRET_ACCESS_KEY')
            if not self.endpoint_url:
                missing_creds.append('CLOUDFLARE_R2_ENDPOINT')
                
            if missing_creds:
                error_msg = f"Missing R2 credentials: {', '.join(missing_creds)}"
                logger.error(f"‚ùå {error_msg}")
                self.connection_errors.append(error_msg)
                self.use_local = True
                logger.info("üìÇ Fallback: Using LOCAL storage")
                return
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö boto3
            try:
                import boto3
                from botocore.config import Config
                from botocore.exceptions import NoCredentialsError, ClientError
            except ImportError as e:
                error_msg = "boto3 library not installed. Run: pip install boto3"
                logger.error(f"‚ùå {error_msg}")
                self.connection_errors.append(error_msg)
                self.use_local = True
                logger.info("üìÇ Fallback: Using LOCAL storage")
                return
            
            # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á S3 client
            logger.info("üîó Attempting to connect to Cloudflare R2...")
            logger.info(f"   Endpoint: {self.endpoint_url}")
            logger.info(f"   Bucket: {self.bucket_name}")
            
            try:
                config = Config(
                    region_name='auto',
                    signature_version='s3v4',
                    retries={'max_attempts': 3, 'mode': 'standard'},
                    connect_timeout=10,
                    read_timeout=10
                )
                
                self.s3_client = boto3.client(
                    's3',
                    endpoint_url=self.endpoint_url,
                    aws_access_key_id=self.access_key,
                    aws_secret_access_key=self.secret_key,
                    config=config
                )
                
                # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠
                logger.info("üß™ Testing R2 connection...")
                test_response = self.s3_client.list_objects_v2(
                    Bucket=self.bucket_name,
                    MaxKeys=1
                )
                
                logger.info("‚úÖ R2 CONNECTION SUCCESSFUL!")
                logger.info(f"   Objects in bucket: {len(test_response.get('Contents', []))}")
                self.use_local = False
                
            except ClientError as e:
                error_code = e.response.get('Error', {}).get('Code', 'Unknown')
                error_message = e.response.get('Error', {}).get('Message', str(e))
                
                if error_code == 'NoSuchBucket':
                    error_msg = f"Bucket '{self.bucket_name}' does not exist"
                    logger.error(f"‚ùå {error_msg}")
                    logger.info("üî® Attempting to create bucket...")
                    
                    try:
                        self.s3_client.create_bucket(Bucket=self.bucket_name)
                        logger.info(f"‚úÖ Bucket '{self.bucket_name}' created successfully!")
                        self.use_local = False
                    except Exception as create_error:
                        error_msg = f"Failed to create bucket: {str(create_error)}"
                        logger.error(f"‚ùå {error_msg}")
                        self.connection_errors.append(error_msg)
                        self.use_local = True
                        
                elif error_code == 'InvalidAccessKeyId':
                    error_msg = "Invalid Access Key ID - Check your CLOUDFLARE_R2_ACCESS_KEY_ID"
                    logger.error(f"‚ùå {error_msg}")
                    self.connection_errors.append(error_msg)
                    self.use_local = True
                    
                elif error_code == 'SignatureDoesNotMatch':
                    error_msg = "Invalid Secret Key - Check your CLOUDFLARE_R2_SECRET_ACCESS_KEY"
                    logger.error(f"‚ùå {error_msg}")
                    self.connection_errors.append(error_msg)
                    self.use_local = True
                    
                else:
                    error_msg = f"R2 Error [{error_code}]: {error_message}"
                    logger.error(f"‚ùå {error_msg}")
                    self.connection_errors.append(error_msg)
                    self.use_local = True
                    
            except Exception as e:
                error_msg = f"Connection failed: {str(e)}"
                logger.error(f"‚ùå {error_msg}")
                self.connection_errors.append(error_msg)
                self.use_local = True
            
        except Exception as e:
            error_msg = f"Initialization error: {str(e)}"
            logger.error(f"‚ùå {error_msg}")
            self.connection_errors.append(error_msg)
            self.use_local = True
        
        # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•
        logger.info("=" * 70)
        if self.use_local:
            logger.warning("‚ö†Ô∏è R2 STORAGE: DISABLED")
            logger.info("üìÇ Using LOCAL FILE STORAGE")
            if self.connection_errors:
                logger.error("Connection errors:")
                for error in self.connection_errors:
                    logger.error(f"  ‚Ä¢ {error}")
        else:
            logger.info("‚úÖ R2 STORAGE: ENABLED AND READY")
        logger.info("=" * 70)
    
    def get_connection_status(self):
        """Get detailed connection status for API"""
        return {
            'connected': not self.use_local,
            'storage_type': 'local' if self.use_local else 'cloudflare_r2',
            'bucket_name': self.bucket_name if not self.use_local else None,
            'endpoint': self.endpoint_url if not self.use_local else None,
            'errors': self.connection_errors
        }
    
    def save_model(self, model_data: Dict[str, Any], filename: str) -> bool:
        """Save model with detailed error reporting"""
        if self.use_local:
            return self._save_model_locally(model_data, filename)
        
        tmp_path = None
        try:
            with tempfile.NamedTemporaryFile(suffix='.joblib', delete=False) as tmp_file:
                joblib.dump(model_data, tmp_file.name)
                tmp_path = tmp_file.name
            
            s3_key = f"models/{filename}"
            with open(tmp_path, 'rb') as f:
                self.s3_client.put_object(
                    Bucket=self.bucket_name,
                    Key=s3_key,
                    Body=f,
                    ContentType='application/octet-stream',
                    Metadata={
                        'created_at': datetime.now().isoformat(),
                        'data_format': model_data.get('data_format', 'unknown'),
                        'accuracy': str(model_data.get('performance_metrics', {}).get('accuracy', 0))
                    }
                )
            
            os.remove(tmp_path)
            logger.info(f"‚úÖ Model {filename} saved to R2")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå R2 save error: {str(e)}")
            if tmp_path and os.path.exists(tmp_path):
                os.remove(tmp_path)
            # Fallback to local
            logger.info("üìÇ Trying local save as fallback...")
            return self._save_model_locally(model_data, filename)
    
    def _save_model_locally(self, model_data: Dict[str, Any], filename: str) -> bool:
        """Save model locally"""
        try:
            model_folder = app.config['MODEL_FOLDER']
            if not os.path.exists(model_folder):
                os.makedirs(model_folder)
            
            filepath = os.path.join(model_folder, filename)
            joblib.dump(model_data, filepath)
            logger.info(f"üíæ Model {filename} saved locally")
            return True
        except Exception as e:
            logger.error(f"‚ùå Local save error: {str(e)}")
            return False
    
    def load_model(self, filename: str) -> Optional[Dict[str, Any]]:
        """Load model"""
        if self.use_local:
            return self._load_model_locally(filename)
        
        tmp_path = None
        try:
            s3_key = f"models/{filename}"
            with tempfile.NamedTemporaryFile(suffix='.joblib', delete=False) as tmp_file:
                self.s3_client.download_file(
                    Bucket=self.bucket_name,
                    Key=s3_key,
                    Filename=tmp_file.name
                )
                tmp_path = tmp_file.name
            
            model_data = joblib.load(tmp_path)
            os.remove(tmp_path)
            logger.info(f"‚úÖ Model {filename} loaded from R2")
            return model_data
            
        except Exception as e:
            logger.warning(f"R2 load failed: {str(e)}, trying local...")
            if tmp_path and os.path.exists(tmp_path):
                os.remove(tmp_path)
            # ‡∏´‡∏≤‡∏Å‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å R2 ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å local ‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡∏°‡πà
            local_result = self._load_model_locally(filename)
            if local_result is None:
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏£‡∏≠‡∏á
                return self._create_fallback_model_data()
            return local_result
    
    def _create_fallback_model_data(self) -> Dict[str, Any]:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏£‡∏≠‡∏á‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ"""
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.preprocessing import StandardScaler
        
        model = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=42
        )
        scaler = StandardScaler()
        
        logger.info("‚úÖ Created fallback model data")
        return {
            'model': model,
            'scaler': scaler,
            'course_credit_map': {},
            'grade_mapping': {
                'A': 4.0, 'B+': 3.5, 'B': 3.0, 'C+': 2.5, 'C': 2.0,
                'D+': 1.5, 'D': 1.0, 'F': 0.0,
                'W': None, 'WF': None, 'WU': None, 'S': None, 'S*': None, 'AU': None, 'I': None, 'U': None
            }
        }
    
    def _load_model_locally(self, filename: str) -> Optional[Dict[str, Any]]:
        """Load model from local storage"""
        try:
            filepath = os.path.join(app.config['MODEL_FOLDER'], filename)
            if os.path.exists(filepath):
                model_data = joblib.load(filepath)
                logger.info(f"üìÇ Model {filename} loaded from local")
                return model_data
            return None
        except Exception as e:
            logger.error(f"‚ùå Local load error: {str(e)}")
            # ‡∏´‡∏≤‡∏Å‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å local ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏£‡∏≠‡∏á
            return self._create_fallback_model_data()
    
    def list_models(self) -> List[Dict[str, Any]]:
        """List all models"""
        models = []
        
        # Try R2 first
        if not self.use_local and self.s3_client:
            try:
                response = self.s3_client.list_objects_v2(
                    Bucket=self.bucket_name,
                    Prefix='models/',
                    Delimiter='/'
                )
                
                if 'Contents' in response:
                    for obj in response['Contents']:
                        filename = obj['Key'].replace('models/', '')
                        if filename and filename.endswith('.joblib'):
                            models.append({
                                'filename': filename,
                                'created_at': obj['LastModified'].isoformat(),
                                'size': obj['Size'],
                                'storage': 'r2',
                                'performance_metrics': {},
                                'data_format': 'unknown'
                            })
                logger.info(f"üìä Found {len(models)} models in R2")
            except Exception as e:
                logger.warning(f"R2 list failed: {str(e)}")
        
        # Also check local (skip files already listed from R2)
        r2_filenames = {m['filename'] for m in models}
        try:
            model_folder = app.config['MODEL_FOLDER']
            if os.path.exists(model_folder):
                local_count = 0
                for filename in os.listdir(model_folder):
                    if filename.endswith('.joblib') and filename not in r2_filenames:
                        filepath = os.path.join(model_folder, filename)
                        try:
                            model_data = joblib.load(filepath)
                            # ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ metrics ‡∏ó‡∏µ‡πà JSON-safe (‡πÑ‡∏°‡πà‡πÄ‡∏≠‡∏≤ model objects)
                            raw_perf = model_data.get('performance_metrics', {})
                            safe_perf = {}
                            if isinstance(raw_perf, dict):
                                for k, v in raw_perf.items():
                                    if isinstance(v, dict):
                                        safe_perf[k] = {
                                            mk: float(mv) for mk, mv in v.items()
                                            if isinstance(mv, (int, float)) or (hasattr(mv, 'item') and callable(mv.item))
                                        }
                                    elif isinstance(v, (int, float)):
                                        safe_perf[k] = float(v)
                            models.append({
                                'filename': filename,
                                'created_at': model_data.get('created_at', ''),
                                'data_format': model_data.get('data_format', 'unknown'),
                                'performance_metrics': safe_perf,
                                'storage': 'local'
                            })
                            local_count += 1
                        except Exception:
                            models.append({
                                'filename': filename,
                                'created_at': datetime.fromtimestamp(os.path.getctime(filepath)).isoformat(),
                                'storage': 'local',
                                'performance_metrics': {},
                                'data_format': 'unknown'
                            })
                            local_count += 1
                logger.info(f"üìÇ Found {local_count} models locally")
        except Exception as e:
            logger.error(f"Local list error: {str(e)}")
        
        return sorted(models, key=lambda x: x.get('created_at', ''), reverse=True)
    
    def delete_model(self, filename: str) -> bool:
        """Delete model"""
        deleted = False
        
        # Try R2
        if not self.use_local and self.s3_client:
            try:
                s3_key = f"models/{filename}"
                self.s3_client.delete_object(
                    Bucket=self.bucket_name,
                    Key=s3_key
                )
                logger.info(f"‚úÖ Model {filename} deleted from R2")
                deleted = True
            except Exception as e:
                logger.warning(f"R2 delete failed: {str(e)}")
        
        # Also try local
        try:
            filepath = os.path.join(app.config['MODEL_FOLDER'], filename)
            if os.path.exists(filepath):
                os.remove(filepath)
                logger.info(f"üóëÔ∏è Model {filename} deleted from local")
                deleted = True
        except Exception as e:
            logger.warning(f"Local delete failed: {str(e)}")
        
        return deleted

# Global Training Status
TRAINING_STATUS = {
    'status': 'idle',
    'message': '',
    'progress': 0,
    'result': None,
    'error': None
}

# Create global storage instance
storage = S3Storage()

# ==========================================
# Sync Models from R2 at Startup
# ==========================================
def sync_models_from_r2():
    """Download models from R2 to local storage for faster access"""
    if storage.use_local:
        logger.info("üìÇ R2 not connected, skipping model sync")
        return 0
    
    try:
        logger.info("üîÑ Syncing models from R2 to local storage...")
        model_folder = app.config['MODEL_FOLDER']
        
        if not os.path.exists(model_folder):
            os.makedirs(model_folder, exist_ok=True)
        
        # List R2 models
        r2_models = []
        response = storage.s3_client.list_objects_v2(
            Bucket=storage.bucket_name,
            Prefix='models/'
        )
        
        if 'Contents' in response:
            r2_models = [obj['Key'].replace('models/', '') 
                        for obj in response['Contents'] 
                        if obj['Key'].endswith('.joblib')]
        
        synced_count = 0
        for model_filename in r2_models:
            local_path = os.path.join(model_folder, model_filename)
            
            # Skip if already exists locally
            if os.path.exists(local_path):
                continue
            
            try:
                s3_key = f"models/{model_filename}"
                storage.s3_client.download_file(
                    Bucket=storage.bucket_name,
                    Key=s3_key,
                    Filename=local_path
                )
                synced_count += 1
                logger.info(f"  ‚úÖ Downloaded {model_filename}")
            except Exception as e:
                logger.warning(f"  ‚ö†Ô∏è Failed to download {model_filename}: {e}")
        
        logger.info(f"‚úÖ Synced {synced_count} models from R2")
        return synced_count
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Model sync failed: {e}")
        return 0

# Run model sync at startup
try:
    sync_models_from_r2()
except Exception as e:
    logger.warning(f"Model sync at startup failed: {e}")

# ===============================
# TRAINING STATUS ROUTE
# ===============================
@app.route('/training_status')
def get_training_status():
    return jsonify(TRAINING_STATUS)


# ===============================
# ENHANCED API ROUTES
# ===============================

@app.route('/api/enhanced_analysis', methods=['POST'])
def enhanced_analysis():
    """API ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏ö‡∏ö‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á"""
    try:
        data = request.get_json()
        grades = data.get('grades', {})
        model_name = data.get('model_name', '')
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô
        analysis = enhanced_system.analyze_performance(grades)
        
        # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô
        model = None
        if model_name and models.get('subject_model'):
            model = models['subject_model']
        
        prediction = enhanced_system.predict_future_performance(grades, model)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü
        chart_data = enhanced_system.create_visualization_charts(grades, prediction)
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°
        total_credits = len(grades) * 3
        passing_grades = [g for g in grades.values() if g not in ['F', 'W', 'WF', 'WU']]
        passing_rate = len(passing_grades) / len(grades) * 100 if grades else 0
        
        response = {
            "success": True,
            "analysis": analysis,
            "prediction": prediction,
            "chart_data": chart_data,
            "statistics": {
                "total_credits": total_credits,
                "passing_rate": round(passing_rate, 1),
                "subjects_count": len(grades)
            }
        }
        
        return jsonify(response)
        
    except Exception as e:
        logger.error(f"Error in enhanced analysis: {e}")
        return jsonify({"error": str(e), "success": False}), 500

@app.route('/api/quick_prediction', methods=['POST'])
def quick_prediction():
    """API ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡πá‡∏ß"""
    try:
        data = request.get_json()
        grades = data.get('grades', {})
        
        if not grades:
            return jsonify({"error": "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏£‡∏î", "success": False}), 400
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡πá‡∏ß
        current_gpa = enhanced_system.calculate_gpa(grades)
        prediction = enhanced_system.predict_future_performance(grades)
        
        response = {
            "success": True,
            "current_gpa": round(current_gpa, 2),
            "predicted_gpa": prediction["predicted_gpa"],
            "trend": prediction["trend"],
            "confidence": round(prediction["confidence"], 2)
        }
        
        return jsonify(response)
        
    except Exception as e:
        logger.error(f"Error in quick prediction: {e}")
        return jsonify({"error": str(e), "success": False}), 500

# ‡πÄ‡∏û‡∏¥‡πà‡∏° API endpoint ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞
@app.route('/api/storage/status')
def get_storage_status():
    """Get detailed storage status"""
    status = storage.get_connection_status()
    return jsonify(status)

# Variables to store loaded models (re-declared for clarity in this combined file)
models = {
    'subject_model': None,
    'gpa_model': None,
    'subject_model_info': None,
    'gpa_model_info': None,
    'subject_feature_cols': None,
    'gpa_feature_cols': None
}

# ==========================================
# Original Functions (re-pasted here for a complete block)
# ==========================================

def detect_data_format(df):
    """Detects the data format of the uploaded DataFrame."""
    try:
        columns = [col.lower() for col in df.columns.tolist()]
        has_name_id = any(keyword in col for col in columns for keyword in ['‡∏ä‡∏∑‡πà‡∏≠', 'name', '‡∏£‡∏´‡∏±‡∏™', 'id', 'student_id'])
        has_year = any(keyword in col for col in columns for keyword in ['‡∏õ‡∏µ', 'year'])
        has_subject_like_columns = any(
            not any(kw in col for kw in ['gpa', '‡πÄ‡∏Å‡∏£‡∏î', '‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à', '‡∏à‡∏ö', 'success', 'graduated']) for col in columns
        ) and any(col.lower().startswith(('‡∏ß‡∏¥‡∏ä‡∏≤', 'subj', 'course')) or len(col) > 5 for col in columns if col not in ['‡∏ä‡∏∑‡πà‡∏≠', 'name', '‡∏£‡∏´‡∏±‡∏™', 'id', 'year'])

        if has_name_id and has_year and has_subject_like_columns:
            logger.debug(f"Detected subject_based format for columns: {df.columns.tolist()}")
            return 'subject_based'

        has_gpa = any(keyword in col for col in columns for keyword in ['‡πÄ‡∏Å‡∏£‡∏î', 'gpa', '‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢', '‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢'])
        has_success = any(keyword in col for col in columns for keyword in ['‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à', '‡∏à‡∏ö', 'success', 'graduated', '‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞'])

        if has_gpa and has_success:
            logger.debug(f"Detected gpa_based format for columns: {df.columns.tolist()}")
            return 'gpa_based'

        logger.debug(f"Could not detect data format for columns: {df.columns.tolist()}")
        return 'unknown'

    except Exception as e:
        logger.error(f"Error detecting data format: {str(e)}")
        return 'unknown'

def grade_to_numeric(grade):
    """Converts a letter grade to a numeric GPA value."""
    if pd.isna(grade):
        return 0.0
    try:
        return float(grade)
    except ValueError:
        pass

    grade_str = str(grade).strip().upper()
    return app.config['DATA_CONFIG']['grade_mapping'].get(grade_str, 0.0)

def categorize_subject(subject_name):
    """Categorizes subjects based on keywords defined in config."""
    if pd.isna(subject_name):
        return '‡∏≠‡∏∑‡πà‡∏ô‡πÜ'

    subject_name = str(subject_name).lower()
    for category, info in app.config['SUBJECT_CATEGORIES'].items():
        if any(keyword in subject_name for keyword in info['keywords']):
            return category
    return '‡∏≠‡∏∑‡πà‡∏ô‡πÜ'

def process_subject_data(df):
    """Processes subject-based DataFrame to create features for model training."""
    try:
        name_col = None
        possible_name_cols = ['‡∏ä‡∏∑‡πà‡∏≠-‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•', '‡∏ä‡∏∑‡πà‡∏≠', '‡∏ä‡∏∑‡πà‡∏≠‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤', 'name', 'student_name', '‡∏£‡∏´‡∏±‡∏™', 'student_id']
        for col in possible_name_cols:
            if col in df.columns:
                name_col = col
                break
        if not name_col:
            name_candidates = [col for col in df.columns if '‡∏ä‡∏∑‡πà‡∏≠' in col.lower() or '‡∏£‡∏´‡∏±‡∏™' in col.lower() or 'id' in col.lower()]
            if name_candidates:
                name_col = name_candidates[0]
            else:
                raise ValueError("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ä‡∏∑‡πà‡∏≠‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏´‡∏±‡∏™‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")

        year_in_col = None
        year_out_col = None
        possible_year_in = ['‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤', '‡∏õ‡∏µ‡πÄ‡∏Ç‡πâ‡∏≤', 'year_in', 'admission_year']
        possible_year_out = ['‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏à‡∏ö', '‡∏õ‡∏µ‡∏à‡∏ö', 'year_out', 'graduation_year']

        for col in possible_year_in:
            if col in df.columns:
                year_in_col = col
                break
        for col in possible_year_out:
            if col in df.columns:
                year_out_col = col
                break

        exclude_cols_keywords = ['gpa', '‡πÄ‡∏Å‡∏£‡∏î', '‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à', '‡∏à‡∏ö', 'success', 'graduated', 'status']
        exclude_cols = [name_col]
        if year_in_col: exclude_cols.append(year_in_col)
        if year_out_col: exclude_cols.append(year_out_col)

        subject_cols = [
            col for col in df.columns
            if col not in exclude_cols and not any(kw in col.lower() for kw in exclude_cols_keywords)
        ]
        
        target_col_found = False
        for kw in ['‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à', '‡∏à‡∏ö', 'success', 'graduated', '‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞']:
            for col in df.columns:
                if kw in col.lower():
                    df['graduated'] = df[col].apply(lambda x: 1 if str(x).lower() in ['‡∏à‡∏ö', 'success', '1', 'pass'] else 0)
                    target_col_found = True
                    break
            if target_col_found:
                break
        if not target_col_found:
            if year_out_col and not df[year_out_col].isnull().all():
                df['graduated'] = df[year_out_col].apply(lambda x: 1 if pd.notna(x) and x > 0 else 0)
                logger.warning("Target column 'graduated' not found, inferred from 'year_out'.")
            else:
                raise ValueError("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤")

        logger.info(f"Found name column: {name_col}")
        logger.info(f"Found subject columns: {len(subject_cols)} subjects")

        processed_data = []

        for idx, row in df.iterrows():
            try:
                student_name = row[name_col]
                year_in = row.get(year_in_col, 0) if year_in_col else 0
                year_out = row.get(year_out_col, 0) if year_out_col else 0
                graduated_status = row.get('graduated', 0)

                grades = []
                subject_categories_grades = {cat: [] for cat in app.config['SUBJECT_CATEGORIES'].keys()}
                subject_categories_grades['‡∏≠‡∏∑‡πà‡∏ô‡πÜ'] = []

                for subject in subject_cols:
                    grade_value = row[subject]
                    if pd.notna(grade_value) and str(grade_value).strip():
                        numeric_grade = grade_to_numeric(grade_value)
                        grades.append(numeric_grade)

                        category = categorize_subject(subject)
                        subject_categories_grades.get(category, subject_categories_grades['‡∏≠‡∏∑‡πà‡∏ô‡πÜ']).append(numeric_grade)

                min_subjects_required = app.config['DATA_CONFIG']['min_subjects_per_student']
                if len(grades) >= min_subjects_required:
                    gpa = np.mean(grades)
                    min_grade = np.min(grades)
                    max_grade = np.max(grades)
                    std_grade = np.std(grades) if len(grades) > 1 else 0
                    fail_count = sum(1 for g in grades if g <= app.config['DATA_CONFIG']['grade_mapping'].get('D', 1.0) - 0.01)
                    fail_rate = fail_count / len(grades) if grades else 0

                    student_data = {
                        '‡∏ä‡∏∑‡πà‡∏≠': student_name,
                        'gpa': gpa,
                        'min_grade': min_grade,
                        'max_grade': max_grade,
                        'std_grade': std_grade,
                        'fail_count': fail_count,
                        'fail_rate': fail_rate,
                        'total_subjects': len(grades),
                        'year_in': year_in if pd.notna(year_in) else 0,
                        'year_out': year_out if pd.notna(year_out) else 0,
                        'graduated': graduated_status
                    }

                    for cat, cat_grades in subject_categories_grades.items():
                        if cat_grades:
                            student_data[f'gpa_{cat}'] = np.mean(cat_grades)
                            student_data[f'min_{cat}'] = np.min(cat_grades)
                            student_data[f'max_{cat}'] = np.max(cat_grades)
                            student_data[f'fail_rate_{cat}'] = sum(1 for g in cat_grades if g <= app.config['DATA_CONFIG']['grade_mapping'].get('D', 1.0) - 0.01) / len(cat_grades)
                        else:
                            student_data[f'gpa_{cat}'] = 0.0
                            student_data[f'min_{cat}'] = 0.0
                            student_data[f'max_{cat}'] = 0.0
                            student_data[f'fail_rate_{cat}'] = 0.0

                    processed_data.append(student_data)

            except Exception as e:
                logger.warning(f"Could not process row {idx} for student '{row.get(name_col, 'N/A')}': {str(e)}")
                continue

        if not processed_data:
            raise ValueError("No data could be processed. Please check the data format and ensure sufficient subjects per student.")

        result_df = pd.DataFrame(processed_data)
        result_df['graduated'] = result_df['graduated'].astype(int)
        logger.info(f"Successfully processed data: {len(result_df)} students")

        return result_df

    except Exception as e:
        logger.error(f"Error processing subject data: {str(e)}")
        raise

def process_gpa_data(df):
    """Processes GPA-based data to create features for model training."""
    try:
        processed_data = []

        name_col = None
        possible_name_cols = ['‡∏ä‡∏∑‡πà‡∏≠-‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•', '‡∏ä‡∏∑‡πà‡∏≠', '‡∏ä‡∏∑‡πà‡∏≠‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤', 'name', 'student_name', '‡∏£‡∏´‡∏±‡∏™', 'student_id']
        for col in possible_name_cols:
            if col in df.columns:
                name_col = col
                break
        if not name_col:
            name_candidates = [col for col in df.columns if '‡∏ä‡∏∑‡πà‡∏≠' in col.lower() or '‡∏£‡∏´‡∏±‡∏™' in col.lower() or 'id' in col.lower()]
            if name_candidates:
                name_col = name_candidates[0]
            else:
                name_col = df.columns[0]
                logger.warning(f"No explicit name/ID column found, using '{name_col}' as student identifier.")

        target_col_found = False
        graduated_col = None
        for kw in ['‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à', '‡∏à‡∏ö', 'success', 'graduated', '‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞']:
            for col in df.columns:
                if kw in col.lower():
                    graduated_col = col
                    target_col_found = True
                    break
            if target_col_found:
                break
        if not target_col_found:
            raise ValueError("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤")

        for idx, row in df.iterrows():
            try:
                student_name = row.get(name_col, f'‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤_{idx}')

                gpa_cols = [col for col in df.columns if any(kw in col.lower() for kw in ['‡πÄ‡∏Å‡∏£‡∏î', 'gpa', '‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢', '‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢'])]
                gpas = []

                for col in gpa_cols:
                    gpa = row.get(col, 0)
                    if pd.notna(gpa) and gpa != 0:
                        try:
                            gpas.append(float(gpa))
                        except ValueError:
                            logger.debug(f"Skipping non-numeric GPA value '{gpa}' in column '{col}'.")
                            continue

                if gpas:
                    success_value = str(row.get(graduated_col, '')).lower()
                    graduated = 1 if any(keyword in success_value for keyword in ['‡∏à‡∏ö', 'success', '1', 'pass']) else 0

                    student_data = {
                        '‡∏ä‡∏∑‡πà‡∏≠': student_name,
                        'gpa': np.mean(gpas),
                        'min_grade': np.min(gpas),
                        'max_grade': np.max(gpas),
                        'std_grade': np.std(gpas) if len(gpas) > 1 else 0,
                        'total_terms': len(gpas),
                        'graduated': graduated
                    }
                    processed_data.append(student_data)
                else:
                    logger.warning(f"Skipping row {idx} for student '{student_name}' due to no valid GPA data.")

            except Exception as e:
                logger.warning(f"Could not process row {idx} for student '{row.get(name_col, 'N/A')}': {str(e)}")
                continue

        if not processed_data:
            raise ValueError("No data could be processed. Please check '‡πÄ‡∏Å‡∏£‡∏î' and '‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à/‡∏à‡∏ö' columns for valid entries.")

        result_df = pd.DataFrame(processed_data)
        result_df['graduated'] = result_df['graduated'].astype(int)
        logger.info(f"Successfully processed GPA data: {len(result_df)} students")

        return result_df

    except Exception as e:
        logger.error(f"Error processing GPA data: {str(e)}")
        raise

def train_ensemble_model(X, y):
    """Trains an Ensemble model with GridSearchCV and SMOTE."""
    try:
        logger.info("Starting Ensemble model training...")

        unique_labels, label_counts = np.unique(y, return_counts=True)
        if len(unique_labels) < 2 or np.min(label_counts) < 2:
            logger.warning(f"Class imbalance or too little data in y before Oversampling: {dict(zip(unique_labels, label_counts))}")

            if len(unique_labels) < 2:
                existing_label = unique_labels[0]
                new_label_to_create = 1 - existing_label
                if not X.empty:
                    X_first_row = X.iloc[[0]].copy()
                    y_new_label = pd.Series([new_label_to_create], index=[X.index.max() + 1 if not X.empty else 0])
                    X = pd.concat([X, X_first_row], ignore_index=True)
                    y = pd.concat([y, y_new_label], ignore_index=True)
                else:
                    raise ValueError("Cannot create synthetic data as DataFrame is empty.")
                unique_labels, label_counts = np.unique(y, return_counts=True)

            while np.min(label_counts) < 2:
                minority_class_label = unique_labels[np.argmin(label_counts)]
                needed_to_reach_two = 2 - label_counts[np.argmin(label_counts)]
                logger.info(f"Oversampling: Adding {needed_to_reach_two} samples for class {minority_class_label} for Train/Test Split.")
                minority_X_samples = X[y == minority_class_label]
                if not minority_X_samples.empty:
                    sample_to_add_X = minority_X_samples.iloc[[0]].copy()
                    for _ in range(needed_to_reach_two):
                        new_index = X.index.max() + 1 if not X.empty else 0
                        y_new_label_entry = pd.Series([minority_class_label], index=[new_index])
                        X = pd.concat([X, sample_to_add_X], ignore_index=True)
                        y = pd.concat([y, y_new_label_entry], ignore_index=True)
                else:
                    logger.error(f"No samples found for class {minority_class_label} for Oversampling.")
                    break
                unique_labels, label_counts = np.unique(y, return_counts=True)

        logger.info(f"Number of data points after initial oversampling: {len(X)}, Features: {len(X.columns)}")
        logger.info(f"Label distribution after initial oversampling: {y.value_counts().to_dict()}")

        if len(X) < app.config['DATA_CONFIG']['min_students_for_training']:
            raise ValueError(f"Insufficient data for model training (at least {app.config['DATA_CONFIG']['min_students_for_training']} samples required).")

        total_samples = len(X)
        test_size_actual = app.config['ML_CONFIG']['test_size']

        min_samples_per_split = 1
        if total_samples * test_size_actual < min_samples_per_split:
            if total_samples > min_samples_per_split:
                test_size_actual = min_samples_per_split / total_samples
            else:
                test_size_actual = 0
                logger.warning(f"Very small dataset ({total_samples} samples), no Test Set will be used.")

        can_stratify = len(np.unique(y)) >= 2 and np.min(np.unique(y, return_counts=True)[1]) >= min_samples_per_split

        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size_actual, random_state=app.config['ML_CONFIG']['random_state'],
            stratify=y if can_stratify else None
        )
        logger.info(f"Data split: Training {len(X_train)} samples, Testing {len(X_test)} samples.")
        logger.info(f"Before SMOTE - Label distribution in Training Set: {Counter(y_train)}")

        if len(np.unique(y_train)) > 1 and np.min(list(Counter(y_train).values())) > 0:
            try:
                min_class_samples = min(Counter(y_train).values())
                
                if min_class_samples >= 6:
                    smote = SMOTE(random_state=app.config['ML_CONFIG']['random_state'])
                    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)
                elif min_class_samples >= 2:
                    k_neighbors = min(5, min_class_samples - 1)
                    k_neighbors = max(1, k_neighbors)
                    smote = SMOTE(
                        random_state=app.config['ML_CONFIG']['random_state'],
                        k_neighbors=k_neighbors
                    )
                    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)
                else:
                    logger.warning("Not enough samples for SMOTE, using original data.")
                    X_train_resampled, y_train_resampled = X_train, y_train
                
                X_train = X_train_resampled
                y_train = y_train_resampled
                logger.info(f"After SMOTE - Label distribution: {Counter(y_train)}")
                
            except Exception as e:
                logger.warning(f"SMOTE failed: {str(e)}. Using original data.")
                
        else:
            logger.warning("SMOTE not applied (single class or empty class in training set).")

        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test) if len(X_test) > 0 else np.array([])

        param_grid_rf = app.config['MODEL_HYPERPARAMETERS']['RandomForest']

        best_rf = None

        try:
            logger.info("Performing GridSearchCV for RandomForest...")
            grid_search_rf = GridSearchCV(
                RandomForestClassifier(random_state=app.config['ML_CONFIG']['random_state'], n_jobs=1),
                param_grid_rf,
                cv=min(app.config['ML_CONFIG']['cv_folds'], len(X_train) // 2) if len(X_train) >= 4 else 2,
                scoring='accuracy',
                n_jobs=1,
                verbose=0
            )
            grid_search_rf.fit(X_train, y_train)
            best_rf = grid_search_rf.best_estimator_
            logger.info(f"RandomForest Best Params: {grid_search_rf.best_params_}")
            logger.info(f"RandomForest Best Score: {grid_search_rf.best_score_:.3f}")
        except Exception as e:
            logger.warning(f"GridSearchCV for RandomForest failed: {str(e)}. Falling back to default parameters.")
            best_rf = RandomForestClassifier(
                n_estimators=app.config['ML_CONFIG']['n_estimators'],
                max_depth=app.config['ML_CONFIG']['max_depth'],
                random_state=app.config['ML_CONFIG']['random_state'],
                n_jobs=4
            )
            best_rf.fit(X_train, y_train)

        accuracy, precision, recall, f1 = 0.0, 0.0, 0.0, 0.0
        if len(X_test) > 0:
            y_pred = best_rf.predict(X_test)
            accuracy = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred, zero_division=0)
            recall = recall_score(y_test, y_pred, zero_division=0)
            f1 = f1_score(y_test, y_pred, zero_division=0)
            logger.info(f"RandomForest results - Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}")
        else:
            accuracy = None
            precision = None
            recall = None
            f1 = None
            logger.warning("No test data for model evaluation. Metrics not available.")

        return {
            'models': {'rf': best_rf},
            'scaler': scaler,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'training_samples': len(X_train),
            'validation_samples': len(X_test),
            'features_count': X.shape[1],
            'best_rf_params': best_rf.get_params() if best_rf else {},
        }

    except Exception as e:
        logger.error(f"Error training Ensemble model: {str(e)}")
        raise

# ==========================================
# Updated Routes with S3 Storage
# ==========================================

@app.route('/train', methods=['POST'])
def train_model():
    """Handles model training with the uploaded file."""
    try:
        if admin_manager.enabled and not has_any_role('admin', 'super_admin'):
            return _json_error('‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÅ‡∏≠‡∏î‡∏°‡∏¥‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏î‡πâ', 403)

        logger.info("üöÄ Starting ADVANCED model training process...")
        data = request.get_json() or {}
        filename = data.get('filename')
        use_advanced = data.get('use_advanced_training', True)  # Default to advanced
        enable_gemini_analysis = data.get('enable_gemini_analysis', True)
        gemini_analysis_goal = data.get('gemini_analysis_goal')

        if not filename:
            logger.warning("No filename provided for training")
            return jsonify({'success': False, 'error': 'No filename provided.'})

        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        if not os.path.exists(filepath):
            logger.error(f"File not found: {filepath}")
            return jsonify({'success': False, 'error': 'Specified file not found.'})

        logger.info(f"üìÅ Processing file: {filename}")

        # ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå
        # Handle missing extension gracefully
        if '.' in filename:
            file_extension = filename.rsplit('.', 1)[1].lower()
        else:
            # ‡∏•‡∏≠‡∏á‡πÄ‡∏î‡∏≤ extension ‡∏à‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå
            if 'xlsx' in filename.lower():
                file_extension = 'xlsx'
            elif 'xls' in filename.lower():
                file_extension = 'xls'
            elif 'csv' in filename.lower():
                file_extension = 'csv'
            else:
                # ‡∏•‡∏≠‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏õ‡πá‡∏ô Excel ‡∏Å‡πà‡∏≠‡∏ô
                file_extension = 'xlsx'
        
        df = None
        if file_extension == 'csv':
            encodings = app.config['DATA_CONFIG']['fallback_encodings']
            for encoding in encodings:
                try:
                    df = pd.read_csv(filepath, encoding=encoding)
                    logger.info(f"‚úÖ Successfully read CSV with encoding: {encoding}")
                    break
                except Exception as e:
                    logger.debug(f"Failed to read CSV with {encoding}: {e}")
                    continue
            if df is None:
                raise ValueError("Could not read CSV file with any supported encoding.")
        elif file_extension in ['xlsx', 'xls']:
            df = pd.read_excel(filepath)
            logger.info(f"‚úÖ Successfully read Excel file")
        else:
            raise ValueError("Unsupported file type for training.")

        if df is None:
            return jsonify({'success': False, 'error': 'Could not read file.'})

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        data_format = detect_data_format(df)
        logger.info(f"üìä Detected data format for training: {data_format}")

        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
        if use_advanced and data_format == 'subject_based':
            logger.info("üß¨ Using ADVANCED Context-Aware Training Strategy")
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• TAN1 format ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå)
            tan1_column_sets = [
                ['STUDENT_ID', 'COURSE_ID', 'GRADE', 'CREDIT'],
                ['Dummy StudentNO', 'COURSE_CODE', 'GRADE', 'CREDIT'],
                ['Dummy StudentNO', 'COURSE_ID', 'GRADE', 'CREDIT'],
                ['STUDENT_ID', 'COURSE_CODE', 'GRADE', 'CREDIT'],
            ]
            is_tan1_format = any(all(col in df.columns for col in cols) for cols in tan1_column_sets)
            
            if is_tan1_format:
                logger.info("üìã Detected TAN1 format - preprocessing data...")
                # ‡πÉ‡∏ä‡πâ preprocess_tan1_data
                df_wide_format, df_long_format, course_credit_map, retake_stats = preprocess_tan1_data(filepath)
                logger.info(f"‚úÖ Preprocessed {len(df_wide_format)} students, {len(df_long_format)} records")

                # ‡πÉ‡∏ä‡πâ AdvancedModelTrainer
                trainer = AdvancedModelTrainer()
                X, y, course_profiles = trainer.prepare_training_data(df_wide_format, df_long_format, retake_stats=retake_stats)
                
                # ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•
                results = trainer.train_models(X, y)
                
                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ú‡πà‡∏≤‡∏ô storage (R2 + local fallback)
                model_fname = f"subject_based_model_tan1_{datetime.now().strftime('%Y%m%d_%H%M%S')}.joblib"
                model_data_to_save = trainer.build_model_data(course_profiles=course_profiles, training_results=results)
                if model_data_to_save:
                    save_ok = storage.save_model(model_data_to_save, model_fname)
                    if save_ok:
                        logger.info(f"üíæ Model saved: {model_fname} (storage: {'R2' if not storage.use_local else 'local'})")
                    else:
                        logger.warning(f"‚ö†Ô∏è Model save returned False for {model_fname}")
                else:
                    logger.error("‚ùå build_model_data returned None ‚Äî no best model")
                model_path = os.path.join(app.config['MODEL_FOLDER'], model_fname)

                tan1_gemini_analysis = None
                if enable_gemini_analysis and is_gemini_available():
                    try:
                        # ‡∏™‡∏£‡∏∏‡∏õ Course DNA ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ Gemini
                        dna_summary = summarize_course_dna(course_profiles)
                        
                        # ‡∏™‡∏£‡∏∏‡∏õ retake ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Gemini
                        retake_gemini_summary = {}
                        if retake_stats:
                            rs_with = sum(1 for v in retake_stats.values() if v['num_retake_courses'] > 0)
                            rs_total = len(retake_stats)
                            retake_gemini_summary = {
                                'students_with_retakes': rs_with,
                                'retake_percentage': round(rs_with / rs_total * 100, 1) if rs_total > 0 else 0,
                                'total_f_retakes': sum(v['retake_f_count'] for v in retake_stats.values()),
                                'total_w_retakes': sum(v['retake_w_count'] for v in retake_stats.values()),
                            }

                        training_context = {
                            'data_format': data_format,
                            'training_type': 'advanced',
                            'use_advanced_pipeline': True,
                            'rows_in_file': len(df),
                            'columns_in_file': len(df.columns),
                            'prepared_samples': len(X),
                            'feature_count': X.shape[1] if hasattr(X, 'shape') else 0,
                            'label_distribution': {int(k): int(v) for k, v in zip(*np.unique(y, return_counts=True))} if len(y) > 0 else {},
                            'course_profiles_count': len(course_profiles) if course_profiles else 0,
                            'course_dna_insight': dna_summary,
                            'retake_analysis': retake_gemini_summary,
                            'model_metrics': {
                                name: {
                                    'accuracy': float(res.get('accuracy', 0)),
                                    'f1_score': float(res.get('f1_score', 0))
                                } for name, res in results.items()
                            },
                            'timestamp': datetime.now().isoformat(),
                            'source_file': filename
                        }
                        tan1_gemini_analysis = run_gemini_training_analysis(df, gemini_analysis_goal, training_context)
                    except Exception as tan_exc:
                        logger.warning(f"Gemini analysis for TAN1 skipped: {tan_exc}")
                elif enable_gemini_analysis and not is_gemini_available():
                    logger.info("Gemini analysis requested but API key is missing (TAN1 flow)")
                
                # ‡∏´‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
                best_name = max(results, key=lambda k: results[k].get('accuracy', 0))
                best = results[best_name]
                model_fname = os.path.basename(model_path)

                # ‡∏™‡∏£‡πâ‡∏≤‡∏á feature importance ‡∏à‡∏≤‡∏Å best model (‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô tree-based)
                tan1_feature_importances = {}
                best_model_obj = best.get('model')
                if best_model_obj and hasattr(best_model_obj, 'feature_importances_'):
                    feature_names = [
                        'current_gpa', 'gpa_std', 'min_grade', 'max_grade', 'grade_range',
                        'high_grades', 'medium_grades', 'low_grades',
                        'avg_course_difficulty', 'avg_course_fail_rate',
                        'num_killer_courses', 'num_easy_courses',
                        'num_failed_courses', 'killer_course_ratio', 'gpa_gap',
                        'recent_trend', 'progress_ratio', 'consistency_score',
                        'academic_momentum', 'risk_score',
                        'credits_earned', 'graduation_progress', 'credits_per_semester',
                        'on_track_ratio', 'semester_progress',
                        'weighted_difficulty', 'hard_course_gpa', 'grade_variance_exposure',
                        'num_retake_courses', 'retake_f_count',
                        'retake_ratio', 'avg_retake_improvement'
                    ]
                    importances = best_model_obj.feature_importances_
                    pairs = sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True)
                    tan1_feature_importances = {name: float(imp) for name, imp in pairs[:15]}

                # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏∏‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢) ‚Äî cast ‡πÄ‡∏õ‡πá‡∏ô float ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ JSON serializable
                all_model_results = {}
                for name, res in results.items():
                    all_model_results[name] = {
                        'accuracy': float(res.get('accuracy', 0)),
                        'f1_score': float(res.get('f1_score', 0)),
                        'precision': float(res.get('precision', 0)),
                        'recall': float(res.get('recall', 0)),
                    }

                # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô label
                unique_labels, label_counts = np.unique(y, return_counts=True)
                label_dist = dict(zip([int(l) for l in unique_labels], [int(c) for c in label_counts]))

                # ‡∏™‡∏£‡∏∏‡∏õ retake analysis ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö frontend
                retake_summary = {}
                if retake_stats:
                    students_with_retakes = sum(1 for v in retake_stats.values() if v['num_retake_courses'] > 0)
                    total_retake_students = len(retake_stats)
                    all_retake_counts = [v['num_retake_courses'] for v in retake_stats.values() if v['num_retake_courses'] > 0]
                    all_f_counts = [v['retake_f_count'] for v in retake_stats.values() if v['retake_f_count'] > 0]
                    all_w_counts = [v['retake_w_count'] for v in retake_stats.values() if v['retake_w_count'] > 0]
                    all_improvements = [v['avg_retake_improvement'] for v in retake_stats.values() if v['num_retake_courses'] > 0]

                    retake_summary = {
                        'students_with_retakes': students_with_retakes,
                        'total_students': total_retake_students,
                        'retake_percentage': round(students_with_retakes / total_retake_students * 100, 1) if total_retake_students > 0 else 0,
                        'total_retake_courses': sum(v['num_retake_courses'] for v in retake_stats.values()),
                        'total_f_retakes': sum(v['retake_f_count'] for v in retake_stats.values()),
                        'total_w_retakes': sum(v['retake_w_count'] for v in retake_stats.values()),
                        'avg_retake_per_student': round(float(np.mean(all_retake_counts)), 1) if all_retake_counts else 0,
                        'max_retake_courses': int(max(all_retake_counts)) if all_retake_counts else 0,
                        'avg_improvement': round(float(np.mean(all_improvements)), 2) if all_improvements else 0,
                        'top_retakers': sorted(
                            [{'student_id': str(sid), **stats} for sid, stats in retake_stats.items() if stats['num_retake_courses'] >= 5],
                            key=lambda x: x['num_retake_courses'], reverse=True
                        )[:10],
                    }

                return jsonify({
                    'success': True,
                    'message': '‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• AI ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß',
                    'model_filename': model_fname,
                    'model_path': model_path,
                    'best_model_name': best_name,
                    'accuracy': float(best.get('accuracy', 0)),
                    'precision': float(best.get('precision', 0)),
                    'recall': float(best.get('recall', 0)),
                    'f1_score': float(best.get('f1_score', 0)),
                    'all_model_results': all_model_results,
                    'training_samples': int(len(X)),
                    'features_count': int(X.shape[1]) if len(X) > 0 else 0,
                    'course_profiles_count': int(len(course_profiles)) if course_profiles else 0,
                    'feature_importances': tan1_feature_importances,
                    'label_distribution': label_dist,
                    'students_count': int(len(df_wide_format)),
                    'courses_count': int(len(course_credit_map)),
                    'training_type': 'tan1_advanced',
                    'retake_analysis': retake_summary,
                    'gemini_analysis': tan1_gemini_analysis
                })
            else:
                # ‡πÉ‡∏ä‡πâ AdvancedFeatureEngineer ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏Å‡πà‡∏≤
                engineer = AdvancedFeatureEngineer(
                    grade_mapping=app.config['DATA_CONFIG']['grade_mapping']
                )
                
                # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö Advanced
                X, y = engineer.prepare_training_data(df)
            
            if len(X) == 0:
                return jsonify({'success': False, 'error': 'Could not prepare training data'})
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å course profiles ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ predict
            course_profiles = engineer.course_profiles
            
        else:
            # ‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏î‡∏¥‡∏°
            logger.info("üìä Using standard training strategy")
            if data_format == 'subject_based':
                processed_df = process_subject_data(df)
            elif data_format == 'gpa_based':
                processed_df = process_gpa_data(df)
            else:
                return jsonify({'success': False, 'error': 'Unsupported data format.'})

            feature_cols = [col for col in processed_df.columns if col not in ['‡∏ä‡∏∑‡πà‡∏≠', 'graduated']]
            X = processed_df[feature_cols].fillna(0)
            y = processed_df['graduated']
            course_profiles = None

        course_profiles_count = len(course_profiles) if course_profiles else 0
        gemini_training_analysis = None
        training_type = 'advanced' if use_advanced else 'standard'

        min_students_for_training = app.config['DATA_CONFIG']['min_students_for_training']
        if len(X) < min_students_for_training:
            return jsonify({'success': False, 
                            'error': f'Insufficient data ({min_students_for_training} samples required).'})

        logger.info(f"üéØ Training data prepared: {len(X)} samples, {X.shape[1]} features")
        logger.info(f"üìà Label distribution: {y.value_counts().to_dict()}")

        # ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•
        logger.info("ü§ñ Starting ensemble model training...")
        model_result = train_ensemble_model(X, y)

        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì feature importance
        feature_importances = {}
        if 'rf' in model_result['models']:
            rf_model = model_result['models']['rf']
            if hasattr(rf_model, 'feature_importances_'):
                feature_cols = X.columns.tolist()
                importances = pd.Series(
                    rf_model.feature_importances_, 
                    index=feature_cols
                ).sort_values(ascending=False)
                feature_importances = importances.head(10).to_dict()

        if enable_gemini_analysis and is_gemini_available():
            try:
                logger.info("ü§ñ Auto-Prompting Gemini for Curriculum Analysis...")
                
                # ‡∏™‡∏£‡∏∏‡∏õ Course DNA ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ Gemini
                dna_summary = summarize_course_dna(course_profiles)
                
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á Auto-Prompt ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏≠ user ‡∏û‡∏¥‡∏°‡∏û‡πå)
                killer_list = dna_summary.get('killer_courses', [])[:10]
                killer_str = ", ".join([f"{k['course_id']} (‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏ï‡∏Å {k['fail_rate_percent']})" for k in killer_list]) if killer_list else "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡∏¥‡∏ä‡∏≤‡∏õ‡∏£‡∏≤‡∏ö‡πÄ‡∏ã‡∏µ‡∏¢‡∏ô‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô"
                
                easy_list = dna_summary.get('easy_courses', [])[:10]
                easy_str = ", ".join([f"{e['course_id']} (‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ {e['avg_grade']:.2f})" for e in easy_list]) if easy_list else "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡∏¥‡∏ä‡∏≤‡∏á‡πà‡∏≤‡∏¢‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô"
                
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á Auto-Analysis Goal ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
                auto_analysis_goal = f"""
                ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏à‡∏£‡∏¥‡∏á:
                1. ‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô "‡∏à‡∏∏‡∏î‡∏ï‡∏≤‡∏¢" (Killer Courses) ‡∏ó‡∏µ‡πà‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏°‡∏±‡∏Å‡∏™‡∏≠‡∏ö‡∏ï‡∏Å: {killer_str}
                2. ‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô "‡∏ß‡∏¥‡∏ä‡∏≤‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏Å‡∏£‡∏î" (Easy Courses): {easy_str}
                3. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ß‡πà‡∏≤‡∏ß‡∏¥‡∏ä‡∏≤‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£
                4. ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏ä‡∏¥‡∏á‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡∏ï‡∏Å‡πÉ‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏õ‡∏£‡∏≤‡∏ö‡πÄ‡∏ã‡∏µ‡∏¢‡∏ô
                5. ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£ (‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å-‡∏á‡πà‡∏≤‡∏¢‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡πÑ‡∏´‡∏°)
                """
                
                training_context = {
                    'data_format': data_format,
                    'training_type': training_type,
                    'use_advanced_pipeline': bool(use_advanced),
                    'rows_in_file': len(df),
                    'columns_in_file': len(df.columns),
                    'prepared_samples': len(X),
                    'feature_count': int(X.shape[1]) if hasattr(X, 'shape') else len(model_result.get('feature_columns', [])),
                    'label_distribution': {int(k): int(v) for k, v in y.value_counts().items()} if hasattr(y, 'value_counts') else ({int(k): int(v) for k, v in zip(*np.unique(y, return_counts=True))} if len(y) > 0 else {}),
                    'course_profiles_count': course_profiles_count,
                    'course_dna_insight': dna_summary,  # ‡πÄ‡∏û‡∏¥‡πà‡∏° Course DNA summary
                    'model_metrics': {
                        'accuracy': float(model_result['accuracy']),
                        'precision': float(model_result['precision']),
                        'recall': float(model_result['recall']),
                        'f1_score': float(model_result['f1_score'])
                    },
                    'timestamp': datetime.now().isoformat(),
                    'source_file': filename
                }
                
                # ‡πÉ‡∏ä‡πâ auto_analysis_goal ‡πÅ‡∏ó‡∏ô gemini_analysis_goal (‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏≠‡∏á)
                gemini_training_analysis = run_gemini_training_analysis(
                    df,
                    auto_analysis_goal.strip(),  # ‡πÉ‡∏ä‡πâ goal ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏≠‡∏á
                    training_context
                )
                logger.info("‚úÖ Gemini Auto-Analysis Completed")
            except Exception as gem_exc:
                logger.warning(f"Gemini analysis skipped due to error: {gem_exc}")
        elif enable_gemini_analysis and not is_gemini_available():
            logger.info("Skipping Gemini training analysis because API is not configured")

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏•
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        model_filename = f'{data_format}_model_{training_type}_{timestamp}.joblib'

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
        model_data = {
            'models': model_result['models'],
            'scaler': model_result['scaler'],
            'feature_columns': X.columns.tolist(),
            'data_format': data_format,
            'training_type': training_type,
            'course_profiles': course_profiles,  # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å course DNA
            'created_at': datetime.now().isoformat(),
          'training_data_info': {
              'rows': len(X),
              'features': X.shape[1],
              'graduated_count': int(y.sum()),
              'not_graduated_count': int(len(y) - y.sum()),
              'source_file': filename
          },
          'performance_metrics': {
              'accuracy': float(model_result['accuracy']),
              'precision': float(model_result['precision']),
              'recall': float(model_result['recall']),
              'f1_score': float(model_result['f1_score'])
          },
          'feature_importances': feature_importances,
          'gemini_training_analysis': gemini_training_analysis,
          'hyperparameters': {
              'best_rf_params': model_result.get('best_rf_params', {}),
          }
        }

        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
        logger.info(f"üíæ Saving model: {model_filename}")
        save_success = storage.save_model(model_data, model_filename)
        
        if save_success:
            logger.info(f"‚úÖ Model saved successfully: {model_filename}")
        else:
            logger.warning(f"‚ö†Ô∏è Model save failed, but continuing...")

        logger.info("üéâ Model training completed successfully!")

        return jsonify({
            'success': True,
            'model_filename': model_filename,
            'training_type': training_type,
            'accuracy': float(model_result['accuracy']),
            'precision': float(model_result['precision']),
            'recall': float(model_result['recall']),
            'f1_score': float(model_result['f1_score']),
          'training_samples': int(len(X)),
          'validation_samples': int(model_result.get('validation_samples', 0)),
          'features_count': int(X.shape[1]),
          'data_format': data_format,
          'feature_importances': feature_importances,
          'course_profiles_count': course_profiles_count,
          'gemini_analysis': gemini_training_analysis,
          'storage_provider': 'cloudflare_r2' if not storage.use_local else 'local'
        })

    except Exception as e:
        logger.error(f"‚ùå Error during model training: {str(e)}", exc_info=True)
        return jsonify({'success': False, 'error': f'Training error: {str(e)}'})


def _is_subject_model(m):
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏õ‡πá‡∏ô subject-based (‡∏£‡∏ß‡∏° TAN1 advanced) ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
    fn = m.get('filename', '').lower()
    return ('subject_based' in fn or 'advanced_model' in fn or
            m.get('data_format') == 'subject_based')

@app.route('/predict', methods=['POST'])
def predict():
    """Predicts outcome from an uploaded CSV/Excel file using a specified model."""
    try:
        logger.info("üîÆ Starting prediction process...")
        data = request.get_json()
        filename = data.get('filename')
        model_filename = data.get('model_filename')

        if not filename:
            return jsonify({'success': False, 'error': 'No filename provided for prediction data.'})
        
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡πÉ‡∏´‡πâ‡∏´‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
        if not model_filename:
            logger.info("üîç No model specified, finding latest subject-based model...")
            models_list = storage.list_models()
            subject_models = [m for m in models_list if _is_subject_model(m)]
            if subject_models:
                model_filename = subject_models[0]['filename']
                logger.info(f"‚úÖ Auto-selected latest model: {model_filename}")
            else:
                return jsonify({'success': False, 'error': 'No trained model found for prediction.'})

        # ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•
        logger.info(f"üíæ Loading model: {model_filename}")
        model_data = storage.load_model(model_filename)
        if not model_data:
            return jsonify({'success': False, 'error': 'Failed to load the specified model.'})

        subject_model = model_data.get('models', {}).get('rf')
        scaler = model_data.get('scaler')
        feature_columns = model_data.get('feature_columns')
        course_profiles = model_data.get('course_profiles')
        data_format = model_data.get('data_format')

        if not all([subject_model, scaler, feature_columns]):
            return jsonify({'success': False, 'error': 'Incomplete model data. Missing model, scaler, or feature columns.'})

        # ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        if not os.path.exists(filepath):
            return jsonify({'success': False, 'error': 'Prediction data file not found.'})
            
        file_extension = filename.rsplit('.', 1)[1].lower()
        df = None
        if file_extension == 'csv':
            df = pd.read_csv(filepath)
        elif file_extension in ['xlsx', 'xls']:
            df = pd.read_excel(filepath)
        else:
            return jsonify({'success': False, 'error': 'Unsupported prediction data file type.'})
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        if data_format == 'subject_based':
            # ‡πÉ‡∏ä‡πâ AdvancedFeatureEngineer ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            engineer = AdvancedFeatureEngineer(
                grade_mapping=app.config['DATA_CONFIG']['grade_mapping']
            )
            # ‡∏ï‡πâ‡∏≠‡∏á‡πÇ‡∏´‡∏•‡∏î course_profiles ‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ engineer ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ
            engineer.course_profiles = course_profiles
            
            # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Transcript ‡πÄ‡∏õ‡πá‡∏ô Student Records
            student_records = engineer._transform_transcript_to_students(df)
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á features ‡∏à‡∏≤‡∏Å snapshot ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤
            all_predictions = []
            
            for student_id, student_record in student_records.items():
                if not student_record['data'].empty:
                    # ‡∏™‡∏£‡πâ‡∏≤‡∏á snapshot ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
                    last_snapshot = engineer._create_snapshot_features(
                        student_id=student_id,
                        snapshot_id=f"{student_id}_final",
                        courses_data=student_record['data'],
                        course_col=engineer._find_column(student_record['data'], ['course_code', 'course', 'subject', '‡∏£‡∏´‡∏±‡∏™‡∏ß‡∏¥‡∏ä‡∏≤']),
                        grade_col=engineer._find_column(student_record['data'], ['grade', '‡πÄ‡∏Å‡∏£‡∏î']),
                        credit_col=engineer._find_column(student_record['data'], ['credit', '‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï']),
                        graduated=student_record['graduated'] # ‡πÉ‡∏ä‡πâ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏à‡∏£‡∏¥‡∏á
                    )
                    
                    if not last_snapshot:
                        logger.warning(f"Could not create snapshot for student {student_id}")
                        continue
                    
                    # ‡πÅ‡∏õ‡∏•‡∏á snapshot ‡πÄ‡∏õ‡πá‡∏ô DataFrame
                    X_pred = pd.DataFrame([last_snapshot])
                    X_pred = engineer._generate_advanced_features(X_pred)
                    
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢‡πÑ‡∏õ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                    missing_cols = set(feature_columns) - set(X_pred.columns)
                    if missing_cols:
                        for c in missing_cols:
                            X_pred[c] = 0
                        logger.warning(f"Missing features added with value 0: {missing_cols}")
                    
                    # ‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏ó‡∏£‡∏ô‡∏°‡∏≤
                    X_pred = X_pred[feature_columns]
                    
                    # Normalize features
                    X_pred_scaled = scaler.transform(X_pred)
                    
                    # Predict probability
                    prediction_proba = subject_model.predict_proba(X_pred_scaled)[:, 1]
                    prediction = (prediction_proba > 0.5).astype(int)[0]
                    
                    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA History ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏ó‡∏≠‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
                    gpa_history = _calculate_gpa_history(student_record['data'], engineer)
                    
                    # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏ó‡∏≠‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
                    predicted_next_gpa = _predict_next_semester_gpa(gpa_history[-1]['gpa'] if gpa_history else 0.0)
                    
                    all_predictions.append({
                        'student_id': student_id,
                        'prediction_success': bool(prediction),
                        'confidence': round(float(prediction_proba[0]), 4),
                        'gpa_history': gpa_history,
                        'predicted_next_gpa': predicted_next_gpa
                    })

            if not all_predictions:
                return jsonify({'success': False, 'error': 'No valid student records found in the data for prediction.'})

            return jsonify({
                'success': True,
                'predictions': all_predictions
            })

        else:
            return jsonify({'success': False, 'error': 'Unsupported data format for this prediction route.'})
            
    except Exception as e:
        logger.error(f"‚ùå Error during prediction: {str(e)}", exc_info=True)
        return jsonify({'success': False, 'error': f'Prediction error: {str(e)}'})

# Helper function to calculate GPA history
def _calculate_gpa_history(df: pd.DataFrame, engineer: AdvancedFeatureEngineer):
    """Calculates GPA for each semester based on the student's data."""
    # Find relevant columns
    term_col = engineer._find_column(df, ['term', 'semester', '‡∏†‡∏≤‡∏Ñ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô', '‡πÄ‡∏ó‡∏≠‡∏°'])
    grade_col = engineer._find_column(df, ['grade', '‡πÄ‡∏Å‡∏£‡∏î'])
    
    if not term_col or not grade_col:
        return []
    
    gpa_history = []
    
    # Group by term and calculate GPA
    for term, term_data in df.groupby(term_col):
        grades = [engineer._convert_grade_to_numeric(g) for g in term_data[grade_col] if pd.notna(g) and engineer._convert_grade_to_numeric(g) is not None]
        if grades:
            gpa = np.mean(grades)
            gpa_history.append({
                'semester': str(term),
                'gpa': float(f"{gpa:.2f}")
            })
            
    return gpa_history

# Helper function to predict next semester's GPA
def _predict_next_semester_gpa(current_gpa: float) -> float:
    """Predicts next semester's GPA using deterministic regression toward the mean."""
    mean_gpa = 2.5
    regression_factor = 0.1
    predicted_gpa = current_gpa + regression_factor * (mean_gpa - current_gpa)
    return round(np.clip(predicted_gpa, 0.0, 4.0), 2)
    
@app.route('/api/models', methods=['GET'])
def list_models():
    """Lists all available trained models."""
    try:
        model_files = []

        # Get all models (R2 + local, deduplicated, safe metrics)
        try:
            model_files = storage.list_models()
            logger.info(f"Found {len(model_files)} models total")
        except Exception as e:
            logger.warning(f"Could not list models: {e}")
        
        # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞ subject_based (‡∏ï‡∏±‡∏î GPA-based ‡∏≠‡∏≠‡∏Å)
        model_files = [m for m in model_files if 'gpa_based' not in m.get('filename', '').lower() and m.get('data_format') != 'gpa_based']
        
        # Enrich with additional metadata
        for model in model_files:
            if 'performance_metrics' not in model or not model['performance_metrics']:
                model['performance_metrics'] = {
                    'accuracy': 0,
                    'precision': 0,
                    'recall': 0,
                    'f1_score': 0
                }
            # Rename for frontend compatibility
            model['performance'] = model.get('performance_metrics', {})

        # Sort by date
        model_files.sort(key=lambda x: x.get('created_at', ''), reverse=True)

        return jsonify({'success': True, 'models': model_files})
    except Exception as e:
        logger.error(f"Error listing models: {str(e)}")
        return jsonify({'success': False, 'error': f'An error occurred while listing models: {str(e)}'}), 500

@app.route('/api/models/<filename>', methods=['DELETE'])
def delete_model(filename):
    """Deletes a specified model file."""
    try:
        if admin_manager.enabled and not has_any_role('admin', 'super_admin'):
            return _json_error('‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÅ‡∏≠‡∏î‡∏°‡∏¥‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏•‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏î‡πâ', 403)

        # Delete from S3 or local storage
        if storage.delete_model(filename):
            # Update in-memory models if needed
            if models['subject_model_info'] and models['subject_model_info'].get('filename') == filename:
                models['subject_model'] = None
                models['subject_model_info'] = None
                models['subject_feature_cols'] = None
            elif models['gpa_model_info'] and models['gpa_model_info'].get('filename') == filename:
                models['gpa_model'] = None
                models['gpa_model_info'] = None
                models['gpa_feature_cols'] = None
                logger.info(f"Model {filename} deleted successfully.")
            return jsonify({'success': True, 'message': f'Model {filename} deleted successfully.'})
        else:
            return jsonify({'success': False, 'error': 'Could not delete model file.'}), 404
            
    except Exception as e:
        logger.error(f"Error deleting model {filename}: {str(e)}")
        return jsonify({'success': False, 'error': f'An error occurred while deleting the model: {str(e)}'}), 500

def load_existing_models():
    """Loads existing trained models from S3 or local storage."""
    try:
        logger.info("üîç Searching for existing models...")
        
        # Get models list from storage
        models_list = storage.list_models()
        
        if not models_list:
            logger.info("No existing models found")
            return
        
        # Load subject-based model
        subject_models = [m for m in models_list if _is_subject_model(m)]
        if subject_models:
            latest_subject = subject_models[0]
            loaded_data = storage.load_model(latest_subject['filename'])
            if loaded_data:
                models['subject_model'] = {
                    'models': loaded_data['models'],
                    'scaler': loaded_data['scaler']
                }
                models['subject_feature_cols'] = loaded_data['feature_columns']
                models['subject_model_info'] = loaded_data.get('performance_metrics', 
                    {'accuracy': 0.85, 'precision': 0.85, 'recall': 0.85, 'f1_score': 0.85})
                models['subject_model_info']['created_at'] = loaded_data.get('created_at', datetime.now().isoformat())
                models['subject_model_info']['loaded_from_file'] = True
                models['subject_model_info']['filename'] = latest_subject['filename']
                logger.info(f"‚úÖ Loaded latest subject model: {latest_subject['filename']}")

        # Load GPA-based model
        gpa_models = [m for m in models_list if 'gpa_based' in m.get('filename', '')]
        if gpa_models:
            latest_gpa = gpa_models[0]
            loaded_data = storage.load_model(latest_gpa['filename'])
            if loaded_data:
                models['gpa_model'] = {
                    'models': loaded_data['models'],
                    'scaler': loaded_data['scaler']
                }
                models['gpa_feature_cols'] = loaded_data['feature_columns']
                models['gpa_model_info'] = loaded_data.get('performance_metrics', 
                    {'accuracy': 0.85, 'precision': 0.85, 'recall': 0.85, 'f1_score': 0.85})
                models['gpa_model_info']['created_at'] = loaded_data.get('created_at', datetime.now().isoformat())
                models['gpa_model_info']['loaded_from_file'] = True
                models['gpa_model_info']['filename'] = latest_gpa['filename']
                logger.info(f"‚úÖ Loaded latest GPA model: {latest_gpa['filename']}")

    except Exception as e:
        logger.error(f"‚ùå Error loading existing models: {str(e)}")

# ==========================================
# Keep all other original functions unchanged
# ==========================================

def calculate_gpa_and_failed_courses_backend(course_grades, courses_data):
    total_points = 0
    completed_credits = 0
    credits_for_gpa = 0  # ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA (‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏° S, W, WF, WU)
    failed_courses = []
    
    grade_mapping_points = app.config['DATA_CONFIG']['grade_mapping']
    # ‡πÄ‡∏Å‡∏£‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ô‡∏≥‡∏°‡∏≤‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA
    excluded_grades = {'S', 'W', 'WF', 'WU'}

    for cid, grade_char in course_grades.items():
        course = next((c for c in courses_data if c['id'] == cid), None)
        if not course:
            continue

        if not grade_char or grade_char == "":
            continue
            
        grade_upper = str(grade_char).upper()
        
        # ‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏Å‡∏£‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ô‡∏≥‡∏°‡∏≤‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA
        if grade_upper in excluded_grades:
            continue
        
        numeric_grade = None
        
        # ‡∏•‡∏≠‡∏á‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏Å‡πà‡∏≠‡∏ô
        try:
            numeric_grade = float(grade_char)
            if not (0.0 <= numeric_grade <= 4.0):
                numeric_grade = None
        except ValueError:
            numeric_grade = None
        
        # ‡∏ñ‡πâ‡∏≤‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ grade_mapping
        if numeric_grade is None:
            numeric_grade = grade_mapping_points.get(grade_upper)
        
        # ‡∏ñ‡πâ‡∏≤ numeric_grade ‡πÄ‡∏õ‡πá‡∏ô None (‡πÄ‡∏ä‡πà‡∏ô S) ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°
        if numeric_grade is None:
            continue
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA (‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏° S, W, WF, WU)
        if numeric_grade is not None:
            credits = course['credit']
            total_points += numeric_grade * credits
            credits_for_gpa += credits
            completed_credits += credits
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏Å (F, WF, WU)
        if numeric_grade == 0.0 or grade_upper in ['F', 'WF', 'WU']:
            if grade_upper in ['F', 'WF', 'WU']:
                failed_courses.append(cid)
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA ‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA ‡πÑ‡∏î‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
    avg_gpa = total_points / credits_for_gpa if credits_for_gpa > 0 else 0
    avg_gpa = round(avg_gpa, 2)  # ‡∏õ‡∏±‡∏î‡πÄ‡∏õ‡πá‡∏ô 2 ‡∏ó‡∏®‡∏ô‡∏¥‡∏¢‡∏°

    return {
        'avgGPA': avg_gpa,
        'completedCredits': completed_credits,
        'failedCourses': failed_courses
    }

def check_prerequisites_backend(course_id, course_grades, courses_data, passing_grades_list_from_config):
    course = next((c for c in courses_data if c['id'] == course_id), None)
    if not course:
        return False
    if not course['prereq'] or len(course['prereq']) == 0:
        return True
    
    grade_mapping_points = app.config['DATA_CONFIG']['grade_mapping']

    for pid in course['prereq']:
        if pid not in course_grades:
            return False
        
        prereq_grade_char = course_grades[pid]
        numeric_grade = None
        try:
            numeric_grade = float(prereq_grade_char)
            if not (0.0 <= numeric_grade <= 4.0):
                numeric_grade = 0.0
        except ValueError:
            numeric_grade = grade_mapping_points.get(str(prereq_grade_char).upper(), 0.0)
            
        if numeric_grade == 0.0:
            return False
    return True

def get_unmet_prerequisites_backend(course_id, course_grades, courses_data, passing_grades_list_from_config):
    course = next((c for c in courses_data if c['id'] == course_id), None)
    if not course or not course['prereq']:
        return []
    
    grade_mapping_points = app.config['DATA_CONFIG']['grade_mapping']
    unmet = []
    for pid in course['prereq']:
        prereq_grade_char = course_grades.get(pid, "")
        numeric_grade = None
        try:
            numeric_grade = float(prereq_grade_char)
            if not (0.0 <= numeric_grade <= 4.0):
                numeric_grade = 0.0
        except ValueError:
            numeric_grade = grade_mapping_points.get(str(prereq_grade_char).upper(), 0.0)

        if pid not in course_grades or numeric_grade == 0.0:
            unmet.append(pid)
    return unmet

def find_course_thai_name_backend(course_id, courses_data):
    course = next((c for c in courses_data if c['id'] == course_id), None)
    return course['thaiName'] if course else course_id

def get_loaded_courses_backend(loaded_terms_count, all_terms_data, courses_data, repeated_courses_in_this_term_ids):
    course_ids = []
    for i in range(loaded_terms_count):
        if i < len(all_terms_data):
            course_ids.extend(all_terms_data[i]['ids'])
    course_ids.extend(repeated_courses_in_this_term_ids)
    
    unique_ids = list(set(course_ids))
    return [c for c in courses_data if c['id'] in unique_ids]

def find_blocked_courses_backend(course_grades, loaded_courses, courses_data, passing_grades_list_from_config):
    blocked_courses_ids = []
    grade_mapping_points = app.config['DATA_CONFIG']['grade_mapping']

    for course_obj in loaded_courses:
        course_id = course_obj['id']
        
        current_grade_for_course = course_grades.get(course_id, "")
        
        numeric_current_grade = None
        try:
            numeric_current_grade = float(current_grade_for_course)
            if not (0.0 <= numeric_current_grade <= 4.0):
                numeric_current_grade = 0.0
        except ValueError:
            numeric_current_grade = grade_mapping_points.get(str(current_grade_for_course).upper(), 0.0)

        if numeric_current_grade == 0.0 and current_grade_for_course != "":
            pass
        elif not check_prerequisites_backend(course_id, course_grades, courses_data, passing_grades_list_from_config):
            blocked_courses_ids.append(course_id)
            
    return list(set(blocked_courses_ids))

def build_dependency_graph_backend(courses_subset):
    graph = {c['id']: [] for c in courses_subset}
    for course in courses_subset:
        for prereq_id in course['prereq']:
            if prereq_id in graph:
                graph[prereq_id].append(course['id'])
    return graph

def find_affected_courses_backend(course_id, graph):
    affected = set()
    queue = [course_id]
    while queue:
        current = queue.pop(0)
        dependents = graph.get(current, [])
        for dep in dependents:
            if dep not in affected:
                affected.add(dep)
                queue.append(dep)
    return list(affected)


# ==========================================
# Prerequisite Checking Functions
# ==========================================

def check_prerequisites_for_course(course_id, current_grades, courses_data, grade_mapping):
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ú‡πà‡∏≤‡∏ô prerequisite ‡∏Ç‡∏≠‡∏á‡∏ß‡∏¥‡∏ä‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
    course = next((c for c in courses_data if c['id'] == course_id), None)
    if not course or not course.get('prereq'):
        return True
    
    passing_grades = [g for g, v in grade_mapping.items() if v > 0]
    
    for prereq_id in course['prereq']:
        grade = current_grades.get(prereq_id, '')
        if not grade or grade not in passing_grades:
            return False
    return True

def get_unmet_prerequisites_for_course(course_id, current_grades, courses_data, grade_mapping):
    """‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ prerequisites ‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô"""
    course = next((c for c in courses_data if c['id'] == course_id), None)
    if not course or not course.get('prereq'):
        return []
    
    passing_grades = [g for g, v in grade_mapping.items() if v > 0]
    unmet = []
    
    for prereq_id in course['prereq']:
        grade = current_grades.get(prereq_id, '')
        if not grade or grade not in passing_grades:
            unmet.append(prereq_id)
    return unmet

def can_take_courses_together(course_a_id, course_b_id, current_grades, courses_data, grade_mapping):
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏•‡∏á‡∏™‡∏≠‡∏á‡∏ß‡∏¥‡∏ä‡∏≤‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡∏Å‡∏£‡∏ì‡∏µ‡πÅ‡∏•‡∏õ‡∏Å‡∏±‡∏ö‡∏ó‡∏§‡∏©‡∏é‡∏µ)"""
    course_a = next((c for c in courses_data if c['id'] == course_a_id), None)
    course_b = next((c for c in courses_data if c['id'] == course_b_id), None)
    
    if not course_a or not course_b:
        return False
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏£‡∏ì‡∏µ‡πÅ‡∏•‡∏õ‡∏Å‡∏±‡∏ö‡∏ó‡∏§‡∏©‡∏é‡∏µ
    lab_keywords = ["‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£", "‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô"]
    
    def is_lab_course(course):
        return any(kw in course['thaiName'] for kw in lab_keywords)
    
    a_is_lab = is_lab_course(course_a)
    b_is_lab = is_lab_course(course_b)
    
    # ‡∏ñ‡πâ‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏Ñ‡∏π‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏•‡∏õ‡∏´‡∏£‡∏∑‡∏≠‡∏ó‡∏±‡πâ‡∏á‡∏Ñ‡∏π‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏§‡∏©‡∏é‡∏µ
    if a_is_lab == b_is_lab:
        return False
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÅ‡∏•‡∏õ‡∏°‡∏µ‡∏ó‡∏§‡∏©‡∏é‡∏µ‡πÄ‡∏õ‡πá‡∏ô prerequisite ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    lab_course = course_a if a_is_lab else course_b
    theory_course = course_b if a_is_lab else course_a
    
    passing_grades = [g for g, v in grade_mapping.items() if v > 0]
    
    if theory_course['id'] in lab_course.get('prereq', []):
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ prereq ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏Ç‡∏≠‡∏á‡πÅ‡∏•‡∏õ‡∏ú‡πà‡∏≤‡∏ô‡∏´‡∏°‡∏î‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á
        other_prereqs_passed = True
        for prereq_id in lab_course['prereq']:
            if prereq_id != theory_course['id']:
                grade = current_grades.get(prereq_id, '')
                if not grade or grade not in passing_grades:
                    other_prereqs_passed = False
                    break
        return other_prereqs_passed
    
    return False

def find_all_blocked_courses(current_grades, loaded_courses_ids, courses_data, grade_mapping):
    """‡∏´‡∏≤‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ö‡∏•‡πá‡∏≠‡∏Å‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å prerequisite ‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô"""
    blocked = []
    
    for course_id in loaded_courses_ids:
        # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏Å‡∏£‡∏î‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏Å‡∏£‡∏î F
        current_grade = current_grades.get(course_id, '')
        if not current_grade or current_grade == 'F':
            if not check_prerequisites_for_course(course_id, current_grades, courses_data, grade_mapping):
                unmet = get_unmet_prerequisites_for_course(course_id, current_grades, courses_data, grade_mapping)
                course = next((c for c in courses_data if c['id'] == course_id), None)
                if course:
                    blocked.append({
                        'id': course_id,
                        'name': course['thaiName'],
                        'unmet_prereqs': [
                            next((c['thaiName'] for c in courses_data if c['id'] == pid), pid)
                            for pid in unmet
                        ]
                    })
    
    return blocked

def build_course_dependency_graph(courses_subset_ids, courses_data):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á dependency graph ‡∏Ç‡∏≠‡∏á‡∏ß‡∏¥‡∏ä‡∏≤"""
    graph = {cid: [] for cid in courses_subset_ids}
    
    for course_id in courses_subset_ids:
        course = next((c for c in courses_data if c['id'] == course_id), None)
        if course:
            for prereq_id in course.get('prereq', []):
                if prereq_id in graph:
                    graph[prereq_id].append(course_id)
    
    return graph

def find_courses_affected_by_failure(failed_course_id, dependency_graph):
    """‡∏´‡∏≤‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏à‡∏≤‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏Å"""
    affected = set()
    queue = [failed_course_id]
    
    while queue:
        current = queue.pop(0)
        dependents = dependency_graph.get(current, [])
        for dep in dependents:
            if dep not in affected:
                affected.add(dep)
                queue.append(dep)
    
    return list(affected)


def topological_sort_with_cycle_check_backend(loaded_courses_objects):
    """Performs a topological sort on a subset of courses and checks for cycles."""
    if not loaded_courses_objects:
        return {'order': [], 'cycle': False}

    course_map = {c['id']: c for c in loaded_courses_objects}

    in_degree = {c_id: 0 for c_id in course_map.keys()}
    adj_list = {c_id: [] for c_id in course_map.keys()}

    for course_id, course_obj in course_map.items():
        for prereq_id in course_obj['prereq']:
            if prereq_id in course_map:
                adj_list[prereq_id].append(course_id)
                in_degree[course_id] += 1

    queue = []
    for cid, degree in in_degree.items():
        if degree == 0:
            queue.append(cid)

    order = []
    count = 0

    while queue:
        u = queue.pop(0)
        order.append(u)
        count += 1
        
        for v in adj_list.get(u, []):
            in_degree[v] -= 1
            if in_degree[v] == 0:
                queue.append(v)

    if count != len(loaded_courses_objects):
        return {'order': [], 'cycle': True}
    return {'order': order, 'cycle': False}

def linear_regression_next_term_gpa_backend(term_gpas):
    """Predicts next term's GPA using simple linear regression."""
    if len(term_gpas) == 0:
        return 0.0
    if len(term_gpas) == 1:
        return term_gpas[0]['gpa']

    n = len(term_gpas)
    x_vals = np.array([(i + 1) for i in range(n)])
    y_vals = np.array([t['gpa'] for t in term_gpas])

    if np.all(y_vals == y_vals[0]):
        return y_vals[0]

    sum_x = np.sum(x_vals)
    sum_y = np.sum(y_vals)
    sum_xy = np.sum(x_vals * y_vals)
    sum_xx = np.sum(x_vals * x_vals)

    denominator = (n * sum_xx - sum_x * sum_y)
    if denominator == 0:
        return y_vals[-1]

    slope = (n * sum_xy - sum_x * sum_y) / denominator
    intercept = (sum_y - slope * sum_x) / n

    predicted = slope * (n + 1) + intercept
    return max(0.0, min(4.0, predicted))

def estimate_completion_rate_backend(avg_gpa, completed_credits, total_required_credits, blocked_courses_ids, fail_count, failed_courses_ids, repeat_count_this_term, core_subjects_ids, courses_data):
    """Estimates the completion rate based on academic progress and issues."""
    base_rate = (completed_credits / total_required_credits) * 100 if total_required_credits > 0 else 0

    blocked_credits = sum(
        (next((c for c in courses_data if c['id'] == cid), {'credit': 0})['credit'] for cid in blocked_courses_ids)
    )

    adjusted_rate = base_rate - blocked_credits * app.config['DATA_CONFIG']['risk_levels'].get('credit_penalty_per_blocked_course', 2)

    if 0 < avg_gpa < app.config['DATA_CONFIG']['risk_levels']['warning_gpa_threshold']:
        adjusted_rate -= app.config['DATA_CONFIG']['risk_levels'].get('low_gpa_penalty', 5)
    
    if fail_count >= 5:
        adjusted_rate -= app.config['DATA_CONFIG']['risk_levels'].get('high_fail_count_penalty', 10)
    elif fail_count >= 2:
        adjusted_rate -= app.config['DATA_CONFIG']['risk_levels'].get('medium_fail_count_penalty', 5)
    
    fail_core_count = len([f for f in failed_courses_ids if f in core_subjects_ids])
    adjusted_rate -= fail_core_count * app.config['DATA_CONFIG']['risk_levels'].get('core_fail_penalty', 3)
    
    if repeat_count_this_term >= 3:
        adjusted_rate -= app.config['DATA_CONFIG']['risk_levels'].get('repeat_course_penalty', 5)

    return max(0.0, min(100.0, adjusted_rate))

def determine_graduation_status_backend(completion_rate, avg_gpa, blocked_courses_ids, failed_courses_ids, loaded_courses_objects, current_grades, all_terms_data, courses_data, loaded_terms_count):
    """Determines the student's graduation status based on their progress and potential issues."""
    
    total_terms_in_curriculum = len(all_terms_data)
    is_at_or_past_final_standard_term = (loaded_terms_count >= total_terms_in_curriculum)

    all_courses_in_loaded_curriculum_ids = set()
    for i in range(loaded_terms_count):
        if i < len(all_terms_data):
            for course_id in all_terms_data[i]['ids']:
                all_courses_in_loaded_curriculum_ids.add(course_id)

    incomplete_courses = [
        cid for cid in all_courses_in_loaded_curriculum_ids
        if cid not in current_grades or current_grades[cid] == "" or app.config['DATA_CONFIG']['grade_mapping'].get(str(current_grades[cid]).upper(), 0.0) == 0.0
    ]
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPA ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (‡∏ï‡πâ‡∏≠‡∏á >= 2.00 ‡∏ñ‡∏∂‡∏á‡∏à‡∏∞‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå)
    # ‡∏ñ‡πâ‡∏≤ GPA ‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡πÅ‡∏•‡πâ‡∏ß ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô GPA ‡∏ï‡πà‡∏≥
    gpa_threshold = 2.00
    if avg_gpa > 0 and avg_gpa < gpa_threshold:
        return f"GPA ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏ì‡∏ë‡πå ({avg_gpa:.2f} < {gpa_threshold:.2f}) ‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏£‡πà‡∏á‡∏î‡πà‡∏ß‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤"
    # ‡∏ñ‡πâ‡∏≤ GPA >= 2.00 ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÅ‡∏•‡∏∞‡πÑ‡∏õ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏≠‡∏∑‡πà‡∏ô‡∏ï‡πà‡∏≠

    if is_at_or_past_final_standard_term:
        if len(failed_courses_ids) > 0:
            return "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏Å‡∏≥‡∏´‡∏ô‡∏î. ‡∏¢‡∏±‡∏á‡∏°‡∏µ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ï‡∏Å (‡πÄ‡∏Å‡∏£‡∏î F) ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç"
        if len(blocked_courses_ids) > 0:
            return "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏Å‡∏≥‡∏´‡∏ô‡∏î. ‡∏°‡∏µ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ö‡∏•‡πá‡∏≠‡∏Å (prerequisite ‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô)"
        if len(incomplete_courses) > 0:
            return f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏Å‡∏≥‡∏´‡∏ô‡∏î. ‡∏¢‡∏±‡∏á‡∏°‡∏µ {len(incomplete_courses)} ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô/‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏Å‡∏£‡∏î"
        
        if avg_gpa == 0.0 and (not failed_courses_ids and not incomplete_courses):
            return "‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏Å‡∏≥‡∏´‡∏ô‡∏î 4 ‡∏õ‡∏µ (‡∏ï‡∏≤‡∏°‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç)"
        
        return "‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏Å‡∏≥‡∏´‡∏ô‡∏î 4 ‡∏õ‡∏µ"
    
    else:
        if len(failed_courses_ids) > 0:
            return "‡∏°‡∏µ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ï‡∏Å (‡πÄ‡∏Å‡∏£‡∏î F) ‡∏≠‡∏≤‡∏à‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Å‡∏≤‡∏£‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏•‡πà‡∏≤‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤‡∏Å‡∏≥‡∏´‡∏ô‡∏î"
        if len(blocked_courses_ids) > 0:
            return "‡∏°‡∏µ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ö‡∏•‡πá‡∏≠‡∏Å (prerequisite ‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô) ‡∏Ñ‡∏ß‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ï‡πà‡∏≠‡∏ï‡∏≤‡∏°‡πÅ‡∏ú‡∏ô"
        if len(incomplete_courses) > 0:
            return "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á. ‡∏¢‡∏±‡∏á‡∏°‡∏µ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô/‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡∏£‡∏î‡πÉ‡∏ô‡∏†‡∏≤‡∏Ñ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ"
        
        return "‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏Å‡∏≥‡∏´‡∏ô‡∏î 4 ‡∏õ‡∏µ"

def update_recommendations_backend(failed_courses_ids, avg_gpa, blocked_courses_ids):
    """Generates specific recommendations based on academic issues."""
    recommendations = []
    if len(failed_courses_ids) > 0:
        recommendations.append(app.config['MESSAGES']['recommendations']['high_risk'][0])
        recommendations.append(app.config['MESSAGES']['recommendations']['high_risk'][1])
    if 0 < avg_gpa < app.config['DATA_CONFIG']['risk_levels']['warning_gpa_threshold']:
        recommendations.append(app.config['MESSAGES']['recommendations']['medium_risk'][1])
    if len(blocked_courses_ids) > 0:
        recommendations.append(app.config['MESSAGES']['recommendations']['high_risk'][3])
    if not recommendations:
        recommendations.append(app.config['MESSAGES']['recommendations']['low_risk'][0])
    return list(set(recommendations))

# ===============================
# ‡∏Å‡∏£‡∏≤‡∏ü 3 ‡πÄ‡∏™‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á
# ===============================

def generate_three_line_chart_data(current_grades, loaded_terms_count=8):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏≤‡∏ü 3 ‡πÄ‡∏™‡πâ‡∏ô ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤:
    1. ‡πÄ‡∏™‡πâ‡∏ô‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ (Target Line - ‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß): GPA ‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤
    2. ‡πÄ‡∏™‡πâ‡∏ô‡∏ú‡∏•‡∏à‡∏£‡∏¥‡∏á (Actual Line - ‡∏™‡∏µ‡∏ô‡πâ‡∏≥‡πÄ‡∏á‡∏¥‡∏ô): GPA ‡∏™‡∏∞‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á‡∏ï‡∏≤‡∏°‡πÄ‡∏ó‡∏≠‡∏°
    3. ‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (Prediction Line - ‡∏™‡∏µ‡∏™‡πâ‡∏°): ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏ó‡∏≠‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
    """
    try:
        # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        grade_mapping = app.config.get('DATA_CONFIG', {}).get('grade_mapping', {
            'A': 4.0, 'B+': 3.5, 'B': 3.0, 'C+': 2.5, 'C': 2.0,
            'D+': 1.5, 'D': 1.0, 'F': 0.0,
            'W': None, 'I': None, 'S': None, 'AU': None, 'U': None
        })
        courses_data = app.config.get('COURSES_DATA', [])
        all_terms_data = app.config.get('ALL_TERMS_DATA', [])
        
        # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ó‡∏≠‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£ (8 ‡πÄ‡∏ó‡∏≠‡∏°‡∏õ‡∏Å‡∏ï‡∏¥)
        total_terms = len(all_terms_data)
        
        # === 1. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏™‡πâ‡∏ô‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ (Target Line) ===
        # GPA ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ = 3.0 (‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤)
        target_gpa = 3.0
        target_line = [target_gpa] * (total_terms + 2)  # +2 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï
        
        # === 2. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏™‡πâ‡∏ô‡∏ú‡∏•‡∏à‡∏£‡∏¥‡∏á (Actual Line) - ‡∏à‡∏≤‡∏Å GPA ‡∏™‡∏∞‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á‡∏ï‡∏≤‡∏°‡πÄ‡∏ó‡∏≠‡∏° ===
        actual_line = []
        term_gpa_list = []
        
        # ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ß‡∏¥‡∏ä‡∏≤‡∏ï‡∏≤‡∏°‡πÄ‡∏ó‡∏≠‡∏°
        courses_by_term = {}
        for term_idx, term_data in enumerate(all_terms_data):
            if term_idx >= loaded_terms_count:
                break
            courses_by_term[term_idx] = term_data['ids']
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA ‡∏™‡∏∞‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á‡∏ï‡∏≤‡∏°‡πÄ‡∏ó‡∏≠‡∏°
        cumulative_points = 0
        cumulative_credits = 0
        
        for term_idx in range(loaded_terms_count):
            term_courses = courses_by_term.get(term_idx, [])
            term_points = 0
            term_credits = 0
            
            for course_id in term_courses:
                grade = current_grades.get(course_id, '')
                if grade and grade in grade_mapping:
                    # ‡∏´‡∏≤‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á‡∏ß‡∏¥‡∏ä‡∏≤
                    course = next((c for c in courses_data if c['id'] == course_id), None)
                    credit = course['credit'] if course else 3
                    grade_point = grade_mapping[grade]

                    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ grade_point ‡∏à‡∏£‡∏¥‡∏á (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà W, I, S, AU, U)
                    if grade_point is not None and grade not in ['W', 'I', 'S', 'AU', 'U']:
                        term_points += grade_point * credit
                        term_credits += credit
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏∞‡∏™‡∏°
            cumulative_points += term_points
            cumulative_credits += term_credits
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA ‡∏™‡∏∞‡∏™‡∏°‡∏ì ‡∏à‡∏∏‡∏î‡∏ô‡∏µ‡πâ
            term_cumulative_gpa = cumulative_points / cumulative_credits if cumulative_credits > 0 else 0
            actual_line.append(round(term_cumulative_gpa, 2))
            term_gpa_list.append(round(term_cumulative_gpa, 2))
        
        # ‡πÄ‡∏ï‡∏¥‡∏° None ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ó‡∏≠‡∏°‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô
        for term_idx in range(loaded_terms_count, total_terms):
            actual_line.append(None)
        
        # === 3. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (Prediction Line) ===
        # prediction_line = actual values for completed terms, then predicted for future terms
        prediction_line = list(actual_line)  # copy actual (length = total_terms)

        if len(term_gpa_list) > 0:
            current_gpa = term_gpa_list[-1]

            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì trend ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á
            if len(term_gpa_list) >= 2:
                trend = term_gpa_list[-1] - term_gpa_list[-2]
                predicted_gpa_next_term = current_gpa + (trend * 0.5)
            else:
                predicted_gpa_next_term = current_gpa

            # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡πà‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ
            predicted_gpa_next_term = max(1.0, min(4.0, predicted_gpa_next_term))

            # ‡πÄ‡∏ï‡∏¥‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ó‡∏≠‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠ ‚Äî ‡∏Ñ‡πà‡∏≠‡∏¢‡πÜ ‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡∏´‡∏≤ target
            prev_predicted = current_gpa
            for i in range(loaded_terms_count, total_terms):
                # weighted average: 70% current prediction + 30% target
                next_val = prev_predicted * 0.7 + target_gpa * 0.3
                next_val = max(1.0, min(4.0, next_val))
                prediction_line[i] = round(next_val, 2)
                prev_predicted = next_val

            # ‡∏Ñ‡πà‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö +2 ‡πÄ‡∏ó‡∏≠‡∏°‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï
            future_vals = []
            for _ in range(2):
                next_val = prev_predicted * 0.7 + target_gpa * 0.3
                next_val = max(1.0, min(4.0, next_val))
                future_vals.append(round(next_val, 2))
                prev_predicted = next_val
        else:
            # ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            predicted_gpa_next_term = 0
            future_vals = [None, None]

        # === 4. ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏ó‡∏∏‡∏Å array ‡∏¢‡∏≤‡∏ß‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô (total_terms + 2) ===
        # actual_line: ‡πÄ‡∏ï‡∏¥‡∏° None ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö 2 ‡πÄ‡∏ó‡∏≠‡∏°‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï
        actual_line = actual_line + [None, None]
        # prediction_line: ‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡πà‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ 2 ‡πÄ‡∏ó‡∏≠‡∏°‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï
        prediction_line = prediction_line + future_vals
        # target_line: ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß total_terms + 2 ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏õ‡πâ‡∏≤‡∏¢‡∏Å‡∏≥‡∏Å‡∏±‡∏ö‡πÄ‡∏ó‡∏≠‡∏°
        term_labels = []
        for i, term_data in enumerate(all_terms_data):
            term_labels.append(f"‡∏õ‡∏µ {term_data['year']} ‡πÄ‡∏ó‡∏≠‡∏° {term_data['term']}")
        term_labels.extend(["‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ +1", "‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ +2"])
        
        # ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
        summary = {
            'current_gpa': term_gpa_list[-1] if term_gpa_list else 0,
            'predicted_next_gpa': round(predicted_gpa_next_term, 2) if term_gpa_list else 0,
            'target_gpa': target_gpa,
            'terms_completed': loaded_terms_count,
            'total_terms': total_terms,
            'on_track': (term_gpa_list[-1] >= target_gpa) if term_gpa_list else False
        }
        
        return {
            'terms': term_labels,
            'target_line': target_line,
            'actual_line': actual_line,
            'prediction_line': prediction_line,
            'colors': {
                'target': '#28a745',      # ‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß
                'actual': '#007bff',      # ‡∏™‡∏µ‡∏ô‡πâ‡∏≥‡πÄ‡∏á‡∏¥‡∏ô
                'prediction': '#fd7e14'   # ‡∏™‡∏µ‡∏™‡πâ‡∏°
            },
            'summary': summary
        }
        
    except Exception as e:
        logger.error(f"Error generating three line chart data: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return {
            'terms': [],
            'target_line': [],
            'actual_line': [],
            'prediction_line': [],
            'colors': {
                'target': '#28a745',
                'actual': '#007bff',
                'prediction': '#fd7e14'
            },
            'summary': {
                'current_gpa': 0,
                'predicted_next_gpa': 0,
                'target_gpa': 3.0,
                'terms_completed': 0,
                'total_terms': 8,
                'on_track': False
            }
        }

def analyze_graduation_failure_reasons(current_grades, loaded_terms_count=8, prediction_result=None):
    """
    ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°
    ‡πÉ‡∏ä‡πâ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• AI (prediction_result) ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç
    
    Args:
        current_grades: dict ‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡∏£‡∏î‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô {course_id: grade}
        loaded_terms_count: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ó‡∏≠‡∏°‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î
        prediction_result: ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• AI (dict) ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢:
            - prediction: '‡∏à‡∏ö' ‡∏´‡∏£‡∏∑‡∏≠ '‡πÑ‡∏°‡πà‡∏à‡∏ö'
            - prob_pass: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏à‡∏ö (0-1)
            - prob_fail: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏à‡∏ö (0-1)
            - confidence: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (0-1)
            - risk_level: ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á
            - method: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
    """
    try:
        grade_mapping = app.config.get('DATA_CONFIG', {}).get('grade_mapping', {})
        courses_data_list = app.config.get('COURSES_DATA', [])
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á dict ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö lookup ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏à‡∏≤‡∏Å course id
        courses_credit_map = {course['id']: course.get('credit', 3) for course in courses_data_list}
        
        reasons = []
        risk_factors = []
        
        # Helper function ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏∂‡∏á‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï
        def get_course_credit(course_id):
            return courses_credit_map.get(course_id, 3)
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        total_courses = len(current_grades)
        failed_courses = [course for course, grade in current_grades.items() 
                         if grade and grade_mapping.get(str(grade).upper(), 0) == 0 and str(grade).upper() not in ['W', 'WF', 'WU', 'I']]
        low_grade_courses = [course for course, grade in current_grades.items() 
                           if grade and 0 < grade_mapping.get(str(grade).upper(), 0) < 2.0]
        incomplete_courses = [course for course, grade in current_grades.items() 
                            if grade and str(grade).upper() in ['I', 'W', 'WF', 'WU']]
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA ‡πÅ‡∏•‡∏∞‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï (‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏° S, W, WF, WU)
        total_points = 0
        total_credits = 0
        passed_credits = 0
        
        # ‡πÄ‡∏Å‡∏£‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ô‡∏≥‡∏°‡∏≤‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA
        excluded_grades = {'S', 'W', 'WF', 'WU', 'I', None}
        
        for course, grade in current_grades.items():
            if not grade or str(grade).upper() in excluded_grades:
                continue
                
            grade_upper = str(grade).upper()
            grade_point = grade_mapping.get(grade_upper)
            
            # ‡∏ñ‡πâ‡∏≤ grade_point ‡πÄ‡∏õ‡πá‡∏ô None (‡πÄ‡∏ä‡πà‡∏ô S) ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°
            if grade_point is None:
                continue
            
            # ‡∏î‡∏∂‡∏á‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£
            credits = get_course_credit(course)
            
            # ‡∏£‡∏ß‡∏°‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏Å‡∏£‡∏î‡∏ó‡∏µ‡πà‡∏ô‡∏≥‡∏°‡∏≤‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA ‡πÑ‡∏î‡πâ
            if grade_point is not None:
                total_points += grade_point * credits
                total_credits += credits
                if grade_point > 0:
                    passed_credits += credits
        
        current_gpa = total_points / total_credits if total_credits > 0 else 0
        current_gpa = round(current_gpa, 2)  # ‡∏õ‡∏±‡∏î‡πÄ‡∏õ‡πá‡∏ô 2 ‡∏ó‡∏®‡∏ô‡∏¥‡∏¢‡∏°
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏à‡∏ö‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
        
        # 1. ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô (‡πÄ‡∏Å‡∏£‡∏î F)
        if len(failed_courses) > 0:
            failed_credits = sum([get_course_credit(course) for course in failed_courses])
            reasons.append({
                'type': 'critical',
                'title': f'‡∏°‡∏µ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô {len(failed_courses)} ‡∏ß‡∏¥‡∏ä‡∏≤ ({failed_credits} ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï)',
                'description': f'‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏Å‡∏£‡∏î F: {", ".join(failed_courses[:3])}{"..." if len(failed_courses) > 3 else ""}',
                'courses': failed_courses,
                'impact': '‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å - ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ã‡πâ‡∏≥‡∏Å‡πà‡∏≠‡∏ô‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤',
                'solution': '‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ã‡πâ‡∏≥‡πÉ‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏Å‡∏ó‡∏±‡∏ô‡∏ó‡∏µ ‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡πâ‡∏î‡∏µ‡∏Å‡πà‡∏≠‡∏ô‡∏™‡∏≠‡∏ö',
                'timeline': f'‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏° {math.ceil(len(failed_courses)/6)} ‡πÄ‡∏ó‡∏≠‡∏°'
            })
        
        # 2. GPA ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏ì‡∏ë‡πå
        if current_gpa < 2.0:
            gpa_deficit = 2.0 - current_gpa
            credits_needed = math.ceil(gpa_deficit * total_credits / 2.0)
            reasons.append({
                'type': 'critical',
                'title': f'GPA ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥ ({current_gpa:.2f} < 2.00)',
                'description': f'‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á GPA ‡∏≠‡∏µ‡∏Å {gpa_deficit:.2f} ‡∏à‡∏∏‡∏î',
                'impact': '‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å - ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÑ‡∏î‡πâ',
                'solution': f'‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏î‡πâ‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ A ‡πÉ‡∏ô {credits_needed} ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏ñ‡∏±‡∏î‡πÑ‡∏õ',
                'timeline': f'‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏° {math.ceil(credits_needed/18)} ‡πÄ‡∏ó‡∏≠‡∏°'
            })
        
        # 3. ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏Å‡∏£‡∏î‡∏ï‡πà‡∏≥
        if len(low_grade_courses) > 0:
            low_credits = sum([get_course_credit(course) for course in low_grade_courses])
            reasons.append({
                'type': 'warning',
                'title': f'‡∏°‡∏µ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏Å‡∏£‡∏î‡∏ï‡πà‡∏≥ {len(low_grade_courses)} ‡∏ß‡∏¥‡∏ä‡∏≤ ({low_credits} ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï)',
                'description': f'‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏Å‡∏£‡∏î D+, D: {", ".join(low_grade_courses[:3])}{"..." if len(low_grade_courses) > 3 else ""}',
                'courses': low_grade_courses,
                'impact': '‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á - ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏ï‡πà‡∏≠ GPA ‡∏£‡∏ß‡∏°',
                'solution': '‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ã‡πâ‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÄ‡∏Å‡∏£‡∏î‡πÅ‡∏•‡∏∞ GPA',
                'timeline': '‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡πÄ‡∏ó‡∏≠‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ'
            })
        
        # 4. ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
        if len(incomplete_courses) > 0:
            incomplete_credits = sum([get_course_credit(course) for course in incomplete_courses])
            reasons.append({
                'type': 'warning',
                'title': f'‡∏°‡∏µ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå {len(incomplete_courses)} ‡∏ß‡∏¥‡∏ä‡∏≤',
                'description': f'‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏Å‡∏£‡∏î I, W, WF, WU: {", ".join(incomplete_courses)}',
                'courses': incomplete_courses,
                'impact': '‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á - ‡πÑ‡∏°‡πà‡∏ô‡∏±‡∏ö‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï',
                'solution': '‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô',
                'timeline': '‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î'
            })
        
        # 5. ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ä‡πâ‡∏≤
        expected_credits = loaded_terms_count * 18  # ‡∏™‡∏°‡∏°‡∏ï‡∏¥ 18 ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏ï‡πà‡∏≠‡πÄ‡∏ó‡∏≠‡∏°
        if passed_credits < expected_credits * 0.8:
            reasons.append({
                'type': 'warning',
                'title': '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏ì‡∏ë‡πå',
                'description': f'‡∏ú‡πà‡∏≤‡∏ô {passed_credits}/{expected_credits} ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï ({passed_credits/expected_credits*100:.1f}%)',
                'impact': '‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á - ‡∏≠‡∏≤‡∏à‡∏à‡∏ö‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤‡πÅ‡∏ú‡∏ô',
                'solution': '‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡πÉ‡∏ô‡πÄ‡∏ó‡∏≠‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ',
                'timeline': f'‡∏≠‡∏≤‡∏à‡∏à‡∏ö‡∏ä‡πâ‡∏≤ {math.ceil((expected_credits-passed_credits)/18)} ‡πÄ‡∏ó‡∏≠‡∏°'
            })
        
        # ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
        risk_score = 0
        if len(failed_courses) > 0:
            risk_score += len(failed_courses) * 20
        if current_gpa < 2.0:
            risk_score += (2.0 - current_gpa) * 50
        if len(low_grade_courses) > 0:
            risk_score += len(low_grade_courses) * 5
        if len(incomplete_courses) > 0:
            risk_score += len(incomplete_courses) * 10
        
        if risk_score >= 80:
            risk_level = '‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å'
            risk_color = 'danger'
            risk_description = '‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏à‡∏ö'
        elif risk_score >= 60:
            risk_level = '‡∏™‡∏π‡∏á'
            risk_color = 'danger'
            risk_description = '‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏™‡∏π‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏à‡∏ö‡∏ä‡πâ‡∏≤'
        elif risk_score >= 40:
            risk_level = '‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á'
            risk_color = 'warning'
            risk_description = '‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á'
        elif risk_score >= 20:
            risk_level = '‡∏ï‡πà‡∏≥-‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á'
            risk_color = 'info'
            risk_description = '‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢'
        else:
            risk_level = '‡∏ï‡πà‡∏≥'
            risk_color = 'success'
            risk_description = '‡∏°‡∏µ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏à‡∏ö‡∏ï‡∏≤‡∏°‡πÅ‡∏ú‡∏ô'
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏à‡∏ö‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
        base_probability = 90
        if len(failed_courses) > 0:
            base_probability -= len(failed_courses) * 15
        if current_gpa < 2.0:
            base_probability -= (2.0 - current_gpa) * 30
        if len(low_grade_courses) > 0:
            base_probability -= len(low_grade_courses) * 3
        if len(incomplete_courses) > 0:
            base_probability -= len(incomplete_courses) * 5
        
        graduation_probability = max(5, min(95, base_probability))
        
        # ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏à‡∏ö
        if graduation_probability >= 80:
            graduation_status = '‡∏°‡∏µ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏à‡∏ö‡∏™‡∏π‡∏á'
            graduation_status_color = 'success'
        elif graduation_probability >= 60:
            graduation_status = '‡∏°‡∏µ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏à‡∏ö‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á'
            graduation_status_color = 'info'
        elif graduation_probability >= 40:
            graduation_status = '‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏à‡∏ö'
            graduation_status_color = 'warning'
        else:
            graduation_status = '‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏™‡∏π‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏à‡∏ö'
            graduation_status_color = 'danger'
        
        # ====== ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏à‡∏ö/‡πÑ‡∏°‡πà‡∏à‡∏ö: ‡πÉ‡∏ä‡πâ‡∏ú‡∏•‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• AI ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å ======
        # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏ú‡∏•‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• AI ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ú‡∏•‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å
        if prediction_result and isinstance(prediction_result, dict):
            # ‡πÉ‡∏ä‡πâ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• AI
            model_prediction = prediction_result.get('prediction', '')
            model_prob_pass = prediction_result.get('prob_pass', 0.5)
            model_confidence = prediction_result.get('confidence', 0.5)
            prediction_method = prediction_result.get('method', 'Unknown')
            
            # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î will_graduate ‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•
            will_graduate = model_prediction == '‡∏à‡∏ö' or model_prob_pass >= 0.5
            
            # ‡∏õ‡∏£‡∏±‡∏ö graduation_probability ‡πÉ‡∏´‡πâ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏ú‡∏•‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
            graduation_probability = model_prob_pass * 100
            
            logger.info(f"üìä Using AI Model prediction: {model_prediction} (prob: {model_prob_pass:.2f}, confidence: {model_confidence:.2f}) from {prediction_method}")
        else:
            # Fallback: ‡πÉ‡∏ä‡πâ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
            will_graduate = graduation_probability >= 50 and current_gpa >= 2.0 and len(failed_courses) == 0
            prediction_method = 'Rule-based Analysis'
            model_prob_pass = graduation_probability / 100
            model_confidence = 0.5
            logger.info(f"üìä Using rule-based prediction (no AI model result)")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤
        reasons_for_graduation = []
        if current_gpa >= 2.0:
            reasons_for_graduation.append({
                'icon': '‚úÖ',
                'title': f'GPA ‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå',
                'description': f'GPA ‡∏™‡∏∞‡∏™‡∏° {current_gpa:.2f} ‚â• 2.00 (‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥)',
                'type': 'success'
            })
        if len(failed_courses) == 0:
            reasons_for_graduation.append({
                'icon': '‚úÖ',
                'title': '‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏Å',
                'description': '‡∏ú‡πà‡∏≤‡∏ô‡∏ó‡∏∏‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô',
                'type': 'success'
            })
        if current_gpa >= 2.5:
            reasons_for_graduation.append({
                'icon': 'üåü',
                'title': '‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏î‡∏µ',
                'description': f'GPA {current_gpa:.2f} ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏î‡∏µ',
                'type': 'success'
            })
        if current_gpa >= 3.0:
            reasons_for_graduation.append({
                'icon': 'üèÜ',
                'title': '‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏î‡∏µ‡∏°‡∏≤‡∏Å',
                'description': f'GPA {current_gpa:.2f} ‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢',
                'type': 'success'
            })
        if passed_credits >= 100:
            reasons_for_graduation.append({
                'icon': 'üìö',
                'title': '‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡πÉ‡∏Å‡∏•‡πâ‡∏Ñ‡∏£‡∏ö',
                'description': f'‡∏™‡∏∞‡∏™‡∏°‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡πÅ‡∏•‡πâ‡∏ß {passed_credits} ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï',
                'type': 'success'
            })
        progress_pct = round(passed_credits/136*100, 1) if passed_credits > 0 else 0
        if progress_pct >= 75:
            reasons_for_graduation.append({
                'icon': 'üìà',
                'title': '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏î‡∏µ',
                'description': f'‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ú‡πà‡∏≤‡∏ô‡πÅ‡∏•‡πâ‡∏ß {progress_pct}% ‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£',
                'type': 'success'
            })
        if len(low_grade_courses) == 0 and total_courses > 0:
            reasons_for_graduation.append({
                'icon': 'üí™',
                'title': '‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏î‡∏µ',
                'description': '‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏Å‡∏£‡∏î‡∏ï‡πà‡∏≥ (D+, D)',
                'type': 'success'
            })
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤
        reasons_for_not_graduation = []
        if current_gpa < 2.0:
            gpa_deficit = 2.0 - current_gpa
            reasons_for_not_graduation.append({
                'icon': '‚ùå',
                'title': f'GPA ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏ì‡∏ë‡πå',
                'description': f'GPA ‡∏™‡∏∞‡∏™‡∏° {current_gpa:.2f} < 2.00 (‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏≠‡∏µ‡∏Å {gpa_deficit:.2f})',
                'type': 'critical'
            })
        if len(failed_courses) > 0:
            failed_names = ', '.join(failed_courses[:3])
            if len(failed_courses) > 3:
                failed_names += f' ‡πÅ‡∏•‡∏∞‡∏≠‡∏µ‡∏Å {len(failed_courses)-3} ‡∏ß‡∏¥‡∏ä‡∏≤'
            reasons_for_not_graduation.append({
                'icon': '‚ùå',
                'title': f'‡∏°‡∏µ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏Å {len(failed_courses)} ‡∏ß‡∏¥‡∏ä‡∏≤',
                'description': f'‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô: {failed_names}',
                'type': 'critical',
                'courses': failed_courses
            })
        if len(low_grade_courses) > 2:
            reasons_for_not_graduation.append({
                'icon': '‚ö†Ô∏è',
                'title': f'‡∏°‡∏µ‡∏ß‡∏¥‡∏ä‡∏≤‡πÄ‡∏Å‡∏£‡∏î‡∏ï‡πà‡∏≥‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å',
                'description': f'‡πÑ‡∏î‡πâ‡πÄ‡∏Å‡∏£‡∏î D+, D ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô {len(low_grade_courses)} ‡∏ß‡∏¥‡∏ä‡∏≤ ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏ï‡πà‡∏≠ GPA',
                'type': 'warning',
                'courses': low_grade_courses
            })
        if len(incomplete_courses) > 0:
            reasons_for_not_graduation.append({
                'icon': '‚ö†Ô∏è',
                'title': f'‡∏°‡∏µ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå',
                'description': f'‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ I/W/WF/WU: {len(incomplete_courses)} ‡∏ß‡∏¥‡∏ä‡∏≤',
                'type': 'warning',
                'courses': incomplete_courses
            })
        if progress_pct < 50 and loaded_terms_count >= 4:
            reasons_for_not_graduation.append({
                'icon': '‚ö†Ô∏è',
                'title': '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ä‡πâ‡∏≤',
                'description': f'‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏û‡∏µ‡∏¢‡∏á {progress_pct}% ‡∏≠‡∏≤‡∏à‡∏à‡∏ö‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤‡πÅ‡∏ú‡∏ô',
                'type': 'warning'
            })
        
        # ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏à‡∏ö - ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• AI
        if will_graduate:
            graduation_prediction_text = 'üéì ‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå'
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
            detail_parts = []
            if prediction_result and isinstance(prediction_result, dict):
                detail_parts.append(f'AI Model ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢: ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏à‡∏ö {graduation_probability:.0f}%')
                detail_parts.append(f'‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à: {model_confidence*100:.0f}%')
                if prediction_method:
                    detail_parts.append(f'‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢: {prediction_method}')
            
            if current_gpa >= 2.0:
                detail_parts.append(f'GPA {current_gpa:.2f} ‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥')
            if len(failed_courses) == 0:
                detail_parts.append('‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏Å')
            
            graduation_prediction_detail = ' | '.join(detail_parts) if detail_parts else f'‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤ {graduation_probability:.0f}%'
        else:
            graduation_prediction_text = '‚ö†Ô∏è ‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡πÑ‡∏°‡πà‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤'
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏û‡∏ö
            detail_parts = []
            if prediction_result and isinstance(prediction_result, dict):
                detail_parts.append(f'AI Model ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢: ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏à‡∏ö {graduation_probability:.0f}%')
                detail_parts.append(f'‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à: {model_confidence*100:.0f}%')
            
            main_issues = []
            if current_gpa < 2.0:
                main_issues.append(f'GPA ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏ì‡∏ë‡πå ({current_gpa:.2f} < 2.00)')
            if len(failed_courses) > 0:
                main_issues.append(f'‡∏°‡∏µ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ï‡∏Å {len(failed_courses)} ‡∏ß‡∏¥‡∏ä‡∏≤')
            if len(main_issues) == 0 and graduation_probability < 50:
                main_issues.append(f'‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏à‡∏ö‡∏ï‡πà‡∏≥‡∏ï‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á AI ({graduation_probability:.0f}%)')
            
            if main_issues:
                detail_parts.append('‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡∏´‡∏•‡∏±‡∏Å: ' + ', '.join(main_issues))
            
            graduation_prediction_detail = ' | '.join(detail_parts) if detail_parts else '‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô'
        
        return {
            'reasons': reasons,
            'risk_level': risk_level,
            'risk_color': risk_color,
            'risk_description': risk_description,
            'risk_score': risk_score,
            'graduation_probability': round(graduation_probability, 1),
            'graduation_status': graduation_status,
            'graduation_status_color': graduation_status_color,
            'current_gpa': round(current_gpa, 2),
            'failed_courses_count': len(failed_courses),
            'low_grade_courses_count': len(low_grade_courses),
            'incomplete_courses_count': len(incomplete_courses),
            'total_courses': total_courses,
            'passed_credits': passed_credits,
            'total_credits': total_credits,
            'expected_credits': expected_credits,
            'progress_percentage': round(passed_credits/136*100, 1) if passed_credits > 0 else 0,  # 136 ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏£‡∏ß‡∏°
            'recommendations': generate_improvement_recommendations(current_gpa, failed_courses, low_grade_courses, incomplete_courses),
            # ====== ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏à‡∏ö/‡πÑ‡∏°‡πà‡∏à‡∏ö (‡πÉ‡∏ä‡πâ AI Model ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å) ======
            'will_graduate': will_graduate,
            'graduation_prediction_text': graduation_prediction_text,
            'graduation_prediction_detail': graduation_prediction_detail,
            'reasons_for_graduation': reasons_for_graduation,
            'reasons_for_not_graduation': reasons_for_not_graduation,
            'failed_courses': failed_courses,
            'low_grade_courses': low_grade_courses,
            'incomplete_courses': incomplete_courses,
            # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• AI
            'prediction_method': prediction_method if 'prediction_method' in dir() else 'Rule-based',
            'model_prob_pass': model_prob_pass if 'model_prob_pass' in dir() else graduation_probability / 100,
            'model_confidence': model_confidence if 'model_confidence' in dir() else 0.5,
            'ai_prediction_used': prediction_result is not None
        }
        
    except Exception as e:
        logger.error(f"Error analyzing graduation failure reasons: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return {
            'reasons': [],
            'risk_level': '‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö',
            'risk_color': 'secondary',
            'risk_description': '‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÑ‡∏î‡πâ',
            'risk_score': 0,
            'graduation_probability': 0,
            'graduation_status': '‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö',
            'graduation_status_color': 'secondary',
            'current_gpa': 0,
            'failed_courses_count': 0,
            'low_grade_courses_count': 0,
            'incomplete_courses_count': 0,
            'total_courses': 0,
            'passed_credits': 0,
            'total_credits': 0,
            'expected_credits': 0,
            'progress_percentage': 0,
            'recommendations': [],
            # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (fallback)
            'will_graduate': False,
            'graduation_prediction_text': '‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÑ‡∏î‡πâ',
            'graduation_prediction_detail': '‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå',
            'reasons_for_graduation': [],
            'reasons_for_not_graduation': [],
            'failed_courses': [],
            'low_grade_courses': [],
            'incomplete_courses': [],
            'prediction_method': 'Error',
            'model_prob_pass': 0,
            'model_confidence': 0,
            'ai_prediction_used': False
        }

def generate_improvement_recommendations(current_gpa, failed_courses, low_grade_courses, incomplete_courses=None):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°"""
    if incomplete_courses is None:
        incomplete_courses = []
    
    recommendations = []
    
    # ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏£‡πà‡∏á‡∏î‡πà‡∏ß‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏Å
    if len(failed_courses) > 0:
        recommendations.extend([
            "üî¥ ‡πÄ‡∏£‡πà‡∏á‡∏î‡πà‡∏ß‡∏ô: ‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ã‡πâ‡∏≥‡πÉ‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏Å‡∏ó‡∏±‡∏ô‡∏ó‡∏µ",
            "üìö ‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ã‡πâ‡∏≥",
            "üë®‚Äçüè´ ‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‡∏ú‡∏π‡πâ‡∏™‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ç‡∏≠‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥",
            "üìù ‡∏ó‡∏≥‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î‡πÅ‡∏•‡∏∞‡∏™‡∏≠‡∏ö‡πÄ‡∏Å‡πà‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ï‡∏±‡∏ß",
            "ü§ù ‡∏´‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πà‡∏á‡∏°‡∏≤‡∏ä‡πà‡∏ß‡∏¢‡∏™‡∏≠‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏¥‡∏ß‡πÉ‡∏´‡πâ"
        ])
    
    # ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GPA ‡∏ï‡πà‡∏≥
    if current_gpa < 2.0:
        recommendations.extend([
            "‚ö†Ô∏è ‡πÄ‡∏£‡πà‡∏á‡∏î‡πà‡∏ß‡∏ô: ‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á GPA ‡πÉ‡∏´‡πâ‡∏ñ‡∏∂‡∏á 2.00",
            "üìä ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏Å‡∏£‡∏î‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ñ‡∏±‡∏î‡πÑ‡∏õ",
            "üéØ ‡πÄ‡∏ô‡πâ‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏™‡∏π‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏±‡∏ö GPA",
            "üìà ‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÄ‡∏Å‡∏£‡∏î‡∏î‡∏µ",
            "‚è∞ ‡∏à‡∏±‡∏î‡∏™‡∏£‡∏£‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÉ‡∏´‡πâ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô"
        ])
    elif current_gpa < 2.5:
        recommendations.extend([
            "üìà ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á GPA ‡πÉ‡∏´‡πâ‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô",
            "üéØ ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ"
        ])
    
    # ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏Å‡∏£‡∏î‡∏ï‡πà‡∏≥
    if len(low_grade_courses) > 0:
        recommendations.extend([
            "üîÑ ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ã‡πâ‡∏≥‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏Å‡∏£‡∏î‡∏ï‡πà‡∏≥",
            "üìà ‡πÉ‡∏ä‡πâ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô",
            "üí° ‡∏´‡∏≤‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á"
        ])
    
    # ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
    if len(incomplete_courses) > 0:
        recommendations.extend([
            "üìã ‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå",
            "‚è±Ô∏è ‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î",
            "üìû ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏°‡πà‡∏≥‡πÄ‡∏™‡∏°‡∏≠"
        ])
    
    # ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏ï‡∏≤‡∏° GPA
    if current_gpa >= 3.5:
        recommendations.extend([
            "üåü ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ï‡πà‡∏≠‡πÑ‡∏õ",
            "üéì ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ï‡∏±‡∏ß‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤",
            "üíº ‡∏´‡∏≤‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏ä‡πà‡∏ô internship"
        ])
    elif current_gpa >= 3.0:
        recommendations.extend([
            "üìö ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô",
            "üéØ ‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ GPA 3.5 ‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ"
        ])
    
    # ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
    recommendations.extend([
        "‚è∞ ‡∏à‡∏±‡∏î‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÅ‡∏•‡∏∞‡∏ó‡∏ö‡∏ó‡∏ß‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏°‡πà‡∏≥‡πÄ‡∏™‡∏°‡∏≠",
        "ü§ù ‡∏´‡∏≤‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ä‡πà‡∏ß‡∏¢‡∏™‡∏≠‡∏ô",
        "üí™ ‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏£‡∏∞‡∏¢‡∏∞‡∏™‡∏±‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß",
        "üè• ‡∏î‡∏π‡πÅ‡∏•‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏Å‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡πÉ‡∏à‡πÉ‡∏´‡πâ‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏£‡∏á",
        "üì± ‡πÉ‡∏ä‡πâ‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏ß‡∏•‡∏≤‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô",
        "üéØ ‡∏°‡∏∏‡πà‡∏á‡πÄ‡∏ô‡πâ‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏ú‡∏•‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏à‡∏ö",
        "üìû ‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤"
    ])
    
    return recommendations

def generate_next_term_grade_prediction_table(current_grades):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏ó‡∏≠‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ"""
    try:
        # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÉ‡∏ô‡πÄ‡∏ó‡∏≠‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ (‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á)
        next_term_courses = [
            {"id": "CS301", "name": "‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", "credits": 3, "difficulty": "‡∏™‡∏π‡∏á"},
            {"id": "CS302", "name": "‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°", "credits": 3, "difficulty": "‡∏™‡∏π‡∏á"},
            {"id": "CS303", "name": "‡∏£‡∏∞‡∏ö‡∏ö‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", "credits": 3, "difficulty": "‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á"},
            {"id": "CS304", "name": "‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÄ‡∏ß‡πá‡∏ö‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô", "credits": 3, "difficulty": "‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á"},
            {"id": "GE401", "name": "‡∏ß‡∏¥‡∏ä‡∏≤‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ", "credits": 3, "difficulty": "‡∏ï‡πà‡∏≥"}
        ]
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
        grade_mapping = app.config.get('DATA_CONFIG', {}).get('grade_mapping', {})
        total_points = 0
        total_credits = 0
        
        for course, grade in current_grades.items():
            if grade and grade in grade_mapping:
                grade_point = grade_mapping[grade]
                credits = 3
                total_points += grade_point * credits
                total_credits += credits
        
        current_gpa = total_points / total_credits if total_credits > 0 else 0
        
        # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏Å‡∏£‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ß‡∏¥‡∏ä‡∏≤
        predictions = []
        for course in next_term_courses:
            # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å‡πÅ‡∏•‡∏∞ GPA ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
            if course["difficulty"] == "‡∏™‡∏π‡∏á":
                if current_gpa >= 3.5:
                    predicted_grade = "B+"
                    confidence = 75
                elif current_gpa >= 3.0:
                    predicted_grade = "B"
                    confidence = 70
                elif current_gpa >= 2.5:
                    predicted_grade = "C+"
                    confidence = 65
                else:
                    predicted_grade = "C"
                    confidence = 60
            elif course["difficulty"] == "‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á":
                if current_gpa >= 3.5:
                    predicted_grade = "A"
                    confidence = 80
                elif current_gpa >= 3.0:
                    predicted_grade = "B+"
                    confidence = 75
                elif current_gpa >= 2.5:
                    predicted_grade = "B"
                    confidence = 70
                else:
                    predicted_grade = "C+"
                    confidence = 65
            else:  # ‡∏ï‡πà‡∏≥
                if current_gpa >= 3.0:
                    predicted_grade = "A"
                    confidence = 85
                elif current_gpa >= 2.5:
                    predicted_grade = "B+"
                    confidence = 80
                else:
                    predicted_grade = "B"
                    confidence = 75
            
            predictions.append({
                "course_id": course["id"],
                "course_name": course["name"],
                "credits": course["credits"],
                "difficulty": course["difficulty"],
                "predicted_grade": predicted_grade,
                "confidence": confidence,
                "grade_point": grade_mapping.get(predicted_grade, 0)
            })
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA ‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ
        predicted_points = sum(p["grade_point"] * p["credits"] for p in predictions)
        predicted_credits = sum(p["credits"] for p in predictions)
        predicted_term_gpa = predicted_points / predicted_credits if predicted_credits > 0 else 0
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA ‡∏™‡∏∞‡∏™‡∏°‡πÉ‡∏´‡∏°‡πà
        new_total_points = total_points + predicted_points
        new_total_credits = total_credits + predicted_credits
        predicted_cumulative_gpa = new_total_points / new_total_credits if new_total_credits > 0 else 0
        
        return {
            "predictions": predictions,
            "current_gpa": round(current_gpa, 2),
            "predicted_term_gpa": round(predicted_term_gpa, 2),
            "predicted_cumulative_gpa": round(predicted_cumulative_gpa, 2),
            "total_credits": predicted_credits,
            "improvement": round(predicted_cumulative_gpa - current_gpa, 2)
        }
        
    except Exception as e:
        logger.error(f"Error generating next term prediction table: {str(e)}")
        return {
            "predictions": [],
            "current_gpa": 0,
            "predicted_term_gpa": 0,
            "predicted_cumulative_gpa": 0,
            "total_credits": 0,
            "improvement": 0
        }

# ==========================================
# Gemini AI Integration Helpers
# ==========================================
GEMINI_SYSTEM_PROMPT = (
    "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏£‡∏î "
    "‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡πÉ‡∏ä‡πâ‡∏ô‡πâ‡∏≥‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏°‡∏∑‡∏≠‡∏≠‡∏≤‡∏ä‡∏µ‡∏û ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡πÅ‡∏•‡∏∞‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏ä‡∏¥‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥ "
    "‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏≤"
)

GEMINI_RESPONSE_SCHEMAS = {
    "insights": {
        "type": "object",
        "properties": {
            "analysis_markdown": {"type": "string"},
            "risk_level": {"type": "string"},
            "graduation_prediction": {
                "type": "object",
                "properties": {
                    "will_graduate": {"type": "boolean"},
                    "prediction_text": {"type": "string"},
                    "reason_why_graduate": {"type": "string"},
                    "reason_why_not_graduate": {"type": "string"},
                    "confidence_percent": {"type": "number"}
                },
                "required": ["will_graduate", "prediction_text"]
            },
            "outcome_summary": {
                "type": "object",
                "properties": {
                    "status": {"type": "string"},
                    "confidence": {"type": "number"},
                    "description": {"type": "string"}
                },
                "required": ["status"]
            },
            "key_metrics": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "label": {"type": "string"},
                        "value": {"type": "string"},
                        "trend": {"type": "string"}
                    }
                }
            },
            "recommendations": {
                "type": "array",
                "items": {"type": "string"}
            },
            "chart": {
                "type": "object",
                "properties": {
                    "title": {"type": "string"},
                    "type": {"type": "string"},
                    "labels": {
                        "type": "array",
                        "items": {"type": "string"}
                    },
                    "series": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "name": {"type": "string"},
                                "data": {
                                    "type": "array",
                                    "items": {"type": "number"}
                                }
                            },
                            "required": ["name", "data"]
                        }
                    }
                }
            }
        },
        "required": ["analysis_markdown"]
    }
}


def is_gemini_available() -> bool:
    return bool(genai and gemini_client_ready)


def _should_retry_with_fallback(exc: Exception) -> bool:
    """
    ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ error ‡∏ô‡∏µ‡πâ‡∏Ñ‡∏ß‡∏£‡∏•‡∏≠‡∏á fallback model ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏ì‡∏µ model ‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô region ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö generateContent
    """
    message = str(exc).lower()
    fallback_indicators = [
        'not found',
        '404',
        'not supported',
        'unsupported',
        'does not exist'
    ]
    return any(indicator in message for indicator in fallback_indicators)


def _to_native_value(value):
    """‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤ numpy/pandas ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Python ‡∏õ‡∏Å‡∏ï‡∏¥"""
    if pd.isna(value):
        return None
    if isinstance(value, (np.generic, np.bool_)):
        return value.item()
    if isinstance(value, (list, tuple, np.ndarray)):
        return [_to_native_value(v) for v in value]
    return value


def summarize_dataframe_for_gemini(df, max_columns=20, max_samples=10):
    summary = {
        'row_count': int(df.shape[0]),
        'column_count': int(df.shape[1]),
        'columns': []
    }
    
    for column in df.columns[:max_columns]:
        series = df[column]
        column_summary = {
            'name': str(column),
            'dtype': str(series.dtype),
            'missing': int(series.isna().sum()),
            'unique': int(series.nunique(dropna=True))
        }
        
        if pd.api.types.is_numeric_dtype(series):
            non_na = series.dropna()
            if not non_na.empty:
                column_summary.update({
                    'min': _to_native_value(non_na.min()),
                    'max': _to_native_value(non_na.max()),
                    'mean': _to_native_value(non_na.mean()),
                    'median': _to_native_value(non_na.median()),
                    'std': _to_native_value(non_na.std())
                })
        else:
            top_values = (
                series.dropna()
                .astype(str)
                .value_counts()
                .head(5)
                .index
                .tolist()
            )
            column_summary['top_values'] = top_values
        
        summary['columns'].append(column_summary)
    
    sample_rows = []
    for _, row in df.head(max_samples).iterrows():
        sample = {}
        for column in df.columns[:max_columns]:
            sample[column] = _to_native_value(row[column])
        sample_rows.append(sample)
    
    return summary, sample_rows


def summarize_grades_for_gemini(course_grades: Dict[str, str], loaded_terms_count: int):
    distribution = Counter(course_grades.values())
    course_details = []
    total_credits = 0
    total_grade_points = 0.0
    credits_for_gpa = 0.0
    failed_courses = []
    
    # Grade mapping ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA (W/WF/WU/S = None ‚Üí ‡πÑ‡∏°‡πà‡∏ô‡∏±‡∏ö‡πÉ‡∏ô GPA)
    grade_mapping = {
        'A': 4.0, 'B+': 3.5, 'B': 3.0, 'C+': 2.5, 'C': 2.0,
        'D+': 1.5, 'D': 1.0, 'F': 0.0,
        'W': None, 'WF': None, 'WU': None, 'S': None, 'S*': None, 'AU': None, 'I': None, 'U': None
    }
    
    for course_id, grade in course_grades.items():
        info = COURSE_LOOKUP.get(course_id, {})
        credits = info.get('credit', 3)
        total_credits += credits
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA ‡πÅ‡∏ö‡∏ö weighted ‡∏î‡πâ‡∏ß‡∏¢‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï (‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏° S, W, WF, WU)
        grade_upper = grade.upper() if isinstance(grade, str) else str(grade).upper()
        grade_point = grade_mapping.get(grade_upper)
        
        if grade_point is not None:
            # ‡∏£‡∏ß‡∏°‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA
            credits_for_gpa += credits
            total_grade_points += grade_point * credits
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏ö‡∏ï‡∏Å (F, U, WF, WU)
        if grade_upper in ['F', 'U', 'WF', 'WU']:
            failed_courses.append({
                'course_id': course_id,
                'course_name': info.get('thaiName') or info.get('name'),
                'credit': credits,
                'grade': grade
            })
        
        course_details.append({
            'course_id': course_id,
            'course_name': info.get('thaiName') or info.get('name'),
            'credit': credits,
            'grade': grade
        })
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA ‡πÅ‡∏ö‡∏ö weighted average (‡∏£‡∏ß‡∏°‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï)
    estimated_gpa = float(total_grade_points / credits_for_gpa) if credits_for_gpa > 0 else None
    
    return {
        'total_courses': len(course_grades),
        'estimated_gpa': estimated_gpa,
        'grade_distribution': dict(distribution),
        'course_details': course_details,
        'loaded_terms_count': loaded_terms_count,
        'total_credits_recorded': total_credits,
        'failed_courses': failed_courses,
        'failed_count': len(failed_courses)
    }


def summarize_course_dna(course_profiles: Optional[Dict[str, Dict[str, Any]]]) -> Dict[str, Any]:
    """
    ‡∏™‡∏£‡∏∏‡∏õ Course DNA ‡∏à‡∏≤‡∏Å course_profiles ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô Gemini Prompt
    ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ß‡∏¥‡∏ä‡∏≤‡∏õ‡∏£‡∏≤‡∏ö‡πÄ‡∏ã‡∏µ‡∏¢‡∏ô (Killer), ‡∏ß‡∏¥‡∏ä‡∏≤‡∏á‡πà‡∏≤‡∏¢ (Easy), ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ú‡∏±‡∏ô‡∏ú‡∏ß‡∏ô (Volatile), ‡∏ß‡∏¥‡∏ä‡∏≤‡πÄ‡∏ü‡πâ‡∏≠ (Inflation)
    ‡πÉ‡∏ä‡πâ enriched profiles: fail_rate (F only), w_rate, grade_std, low_grade_rate
    """
    if not course_profiles:
        return {
            "killer_courses": [],
            "easy_courses": [],
            "volatile_courses": [],
            "high_inflation_courses": [],
            "total_courses": 0
        }

    killer_courses = []
    easy_courses = []
    volatile_courses = []
    high_inflation_courses = []

    # Thresholds
    MIN_STUDENTS = 10
    KILLER_F_RATE = 0.15         # F rate >= 15% (‡πÄ‡∏Ç‡πâ‡∏°‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ô‡∏±‡∏ö F ‡∏à‡∏£‡∏¥‡∏á‡πÜ)
    KILLER_LOW_RATE = 0.30       # ‡∏´‡∏£‡∏∑‡∏≠ low_grade_rate >= 30%
    KILLER_W_RATE = 0.20         # ‡∏´‡∏£‡∏∑‡∏≠ W rate >= 20% (‡∏Ñ‡∏ô‡∏ñ‡∏≠‡∏ô‡πÄ‡∏¢‡∏≠‡∏∞ = ‡∏¢‡∏≤‡∏Å)
    EASY_AVG_GRADE = 3.5
    EASY_FAIL_RATE = 0.05        # F rate < 5%
    VOLATILE_STD = 1.2           # grade_std > 1.2 (‡πÄ‡∏Å‡∏£‡∏î‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏°‡∏≤‡∏Å)
    INFLATION_AVG = 3.7

    for course_id, profile in course_profiles.items():
        fail_rate = profile.get('fail_rate', 0)
        low_grade_rate = profile.get('low_grade_rate', fail_rate)  # fallback ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö old profiles
        w_rate = profile.get('w_rate', 0)
        avg_grade = profile.get('avg_grade', 0)
        grade_std = profile.get('grade_std', 0)
        difficulty_score = profile.get('difficulty_score', 0)
        total_students = profile.get('total_students', 0)

        if total_students < MIN_STUDENTS:
            continue

        # Killer Course: F rate ‡∏™‡∏π‡∏á OR low_grade_rate ‡∏™‡∏π‡∏á OR W rate ‡∏™‡∏π‡∏á
        is_killer = (fail_rate >= KILLER_F_RATE or low_grade_rate >= KILLER_LOW_RATE or w_rate >= KILLER_W_RATE)
        if is_killer:
            killer_courses.append({
                'course_id': course_id,
                'fail_rate': fail_rate,
                'low_grade_rate': low_grade_rate,
                'w_rate': w_rate,
                'avg_grade': avg_grade,
                'grade_std': grade_std,
                'difficulty_score': difficulty_score,
                'total_students': total_students,
                'fail_rate_percent': f"{fail_rate*100:.1f}%",
                'w_rate_percent': f"{w_rate*100:.1f}%",
                'reason': ('F ‡∏™‡∏π‡∏á' if fail_rate >= KILLER_F_RATE else '') +
                          (' ‡πÄ‡∏Å‡∏£‡∏î‡∏ï‡πà‡∏≥‡πÄ‡∏¢‡∏≠‡∏∞' if low_grade_rate >= KILLER_LOW_RATE else '') +
                          (' ‡∏Ñ‡∏ô‡∏ñ‡∏≠‡∏ô‡πÄ‡∏¢‡∏≠‡∏∞' if w_rate >= KILLER_W_RATE else '')
            })

        # Easy Course: ‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏™‡∏π‡∏á + ‡πÅ‡∏ó‡∏ö‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ô‡∏ï‡∏Å
        if avg_grade >= EASY_AVG_GRADE and fail_rate <= EASY_FAIL_RATE:
            easy_courses.append({
                'course_id': course_id,
                'avg_grade': avg_grade,
                'fail_rate': fail_rate,
                'grade_std': grade_std,
                'total_students': total_students
            })

        # Volatile Course: ‡πÄ‡∏Å‡∏£‡∏î‡∏ú‡∏±‡∏ô‡∏ú‡∏ß‡∏ô‡∏°‡∏≤‡∏Å (bimodal: ‡∏°‡∏µ‡∏ó‡∏±‡πâ‡∏á A ‡πÅ‡∏•‡∏∞ F)
        if grade_std >= VOLATILE_STD:
            volatile_courses.append({
                'course_id': course_id,
                'grade_std': grade_std,
                'avg_grade': avg_grade,
                'fail_rate': fail_rate,
                'total_students': total_students
            })

        # High Inflation Course: ‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å
        if avg_grade >= INFLATION_AVG:
            high_inflation_courses.append({
                'course_id': course_id,
                'avg_grade': avg_grade,
                'fail_rate': fail_rate,
                'total_students': total_students
            })

    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö
    killer_courses.sort(key=lambda x: x['fail_rate'] + x['w_rate'], reverse=True)
    easy_courses.sort(key=lambda x: x['avg_grade'], reverse=True)
    volatile_courses.sort(key=lambda x: x['grade_std'], reverse=True)

    return {
        "killer_courses": killer_courses[:20],
        "easy_courses": easy_courses[:20],
        "volatile_courses": volatile_courses[:20],
        "high_inflation_courses": high_inflation_courses[:20],
        "total_courses": len(course_profiles),
        "killer_count": len(killer_courses),
        "easy_count": len(easy_courses),
        "volatile_count": len(volatile_courses),
        "inflation_count": len(high_inflation_courses)
    }

def run_gemini_training_analysis(
    df: pd.DataFrame,
    analysis_goal: Optional[str],
    training_context: Dict[str, Any]
) -> Optional[Dict[str, Any]]:
    """
    ‡πÉ‡∏´‡πâ Gemini ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ï‡∏≤‡∏° prompt ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
    ‡πÅ‡∏•‡πâ‡∏ß‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•
    """
    if not is_gemini_available():
        logger.debug("Gemini not available; skip training-time analysis")
        return None
    
    try:
        logger.info("ü§ñ Starting Gemini training analysis...")
        
        # ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏ó‡∏£‡∏ô
        summary, samples = summarize_dataframe_for_gemini(df, max_columns=30, max_samples=20)
        
        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Course DNA ‡∏à‡∏≤‡∏Å training_context
        dna_info = training_context.get('course_dna_insight', {})
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Gemini
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ ‡∏ñ‡πâ‡∏≤ user ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏°‡∏≤
        goal_text = (analysis_goal or '').strip()
        if not goal_text:
            goal_text = "‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡πÅ‡∏•‡∏∞‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡πÄ‡∏Å‡∏£‡∏î‡∏Ç‡∏≠‡∏á‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏à‡∏≤‡∏Å Course DNA"
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° Course DNA ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏™‡πà‡πÉ‡∏ô prompt
        dna_section = ""
        if dna_info and dna_info.get('killer_courses'):
            killer_list = dna_info.get('killer_courses', [])[:10]  # ‡πÄ‡∏≠‡∏≤‡πÅ‡∏Ñ‡πà 10 ‡∏≠‡∏±‡∏ô‡πÅ‡∏£‡∏Å
            killer_str = ", ".join([f"{k['course_id']} (Fail Rate: {k['fail_rate_percent']})" for k in killer_list])
            easy_list = dna_info.get('easy_courses', [])[:10]
            easy_str = ", ".join([f"{e['course_id']} (Avg: {e['avg_grade']:.2f})" for e in easy_list])
            
            dna_section = f"""
**‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£ (Course DNA) - ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏î‡∏¢‡∏£‡∏∞‡∏ö‡∏ö Backend:**
- ‡∏ß‡∏¥‡∏ä‡∏≤‡∏õ‡∏£‡∏≤‡∏ö‡πÄ‡∏ã‡∏µ‡∏¢‡∏ô (Killer Courses): {killer_str if killer_str else "‡πÑ‡∏°‡πà‡∏û‡∏ö"}
- ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏õ‡∏£‡∏≤‡∏ö‡πÄ‡∏ã‡∏µ‡∏¢‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {dna_info.get('killer_count', 0)} ‡∏ß‡∏¥‡∏ä‡∏≤
- ‡∏ß‡∏¥‡∏ä‡∏≤‡∏á‡πà‡∏≤‡∏¢ (Easy Courses): {easy_str if easy_str else "‡πÑ‡∏°‡πà‡∏û‡∏ö"}
- ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏á‡πà‡∏≤‡∏¢‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {dna_info.get('easy_count', 0)} ‡∏ß‡∏¥‡∏ä‡∏≤
- ‡∏ß‡∏¥‡∏ä‡∏≤‡πÄ‡∏ü‡πâ‡∏≠‡πÄ‡∏Å‡∏£‡∏î (High Inflation): {dna_info.get('inflation_count', 0)} ‡∏ß‡∏¥‡∏ä‡∏≤
- ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {dna_info.get('total_courses', 0)} ‡∏ß‡∏¥‡∏ä‡∏≤

**‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏:** ‡∏£‡∏∞‡∏ö‡∏ö Backend ‡πÑ‡∏î‡πâ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Course DNA ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏•‡πâ‡∏ß ‡πÇ‡∏î‡∏¢:
- Killer Course = ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡∏ï‡∏Å ‚â• 30%
- Easy Course = ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ ‚â• 3.5 ‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡∏ï‡∏Å ‚â§ 10%
- High Inflation = ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ ‚â• 3.7

"""
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt ‡∏ó‡∏µ‡πà‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Ç‡∏∂‡πâ‡∏ô
        training_prompt = f"""
‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô Data Science ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤
‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ñ‡∏∑‡∏≠ Course DNA ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡πÄ‡∏Å‡∏£‡∏î‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏ö‡∏ö Dynamic Snapshots ‡πÅ‡∏•‡πâ‡∏ß

**‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå:**
{goal_text}

**‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏ó‡∏£‡∏ô:**
- ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß: {summary['row_count']} ‡πÅ‡∏ñ‡∏ß
- ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå: {summary['column_count']} ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
- ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå: {json.dumps(summary['columns'], ensure_ascii=False, indent=2)}

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (20 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å):**
{json.dumps(samples, ensure_ascii=False, indent=2)}

{dna_section}

**‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô:**
- ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {training_context.get('data_format', '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏')}
- ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô: {training_context.get('training_type', '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏')}
- ‡πÉ‡∏ä‡πâ Advanced Pipeline: {training_context.get('use_advanced_pipeline', False)}
- ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÅ‡∏•‡πâ‡∏ß: {training_context.get('prepared_samples', 0)}
- ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô features: {training_context.get('feature_count', 0)}
- ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á label: {json.dumps(training_context.get('label_distribution', {}), ensure_ascii=False)}

**‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì:**
1. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£ (‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å-‡∏á‡πà‡∏≤‡∏¢‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡πÑ‡∏´‡∏°)
2. ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ß‡πà‡∏≤ Killer Courses ‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£
3. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà ‡∏°‡∏µ missing values ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà?
4. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á label (‡∏à‡∏ö/‡πÑ‡∏°‡πà‡∏à‡∏ö) ‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà?
5. ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥: ‡∏°‡∏µ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏≠‡∏∞‡πÑ‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà?
6. ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏° ‡πÉ‡∏´‡πâ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏à‡∏≤‡∏Å‡πÄ‡∏ó‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ

‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
"""
        
        payload = {
            'analysis_goal': goal_text,
            'dataset_summary': summary,
            'sample_rows': samples,
            'training_context': training_context,
            'detailed_prompt': training_prompt
        }
        
        logger.info("üì§ Sending training data to Gemini for analysis...")
        gemini_output = call_gemini_structured('training_dataset_analysis', payload)
        
        logger.info("‚úÖ Gemini training analysis completed")
        
        return {
            'analysis_goal': goal_text,
            'dataset_summary': summary,
            'sample_rows': samples,
            'training_context': training_context,
            'gemini': gemini_output,
            'generated_at': datetime.now().isoformat(),
            'analysis_type': 'training_file_analysis'
        }
    except Exception as exc:
        logger.warning(f"Gemini training analysis failed: {exc}")
        import traceback
        logger.error(traceback.format_exc())
        return {
            'analysis_goal': analysis_goal,
            'training_context': training_context,
            'error': str(exc),
            'generated_at': datetime.now().isoformat()
        }


def repair_incomplete_json(text: str) -> str:
    """
    ‡∏ã‡πà‡∏≠‡∏°‡πÅ‡∏ã‡∏° JSON ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå (unterminated string, unclosed braces, etc.)
    """
    if not text:
        return text
    
    # ‡∏•‡∏ö markdown code blocks ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
    cleaned = text.strip()
    if cleaned.startswith('```'):
        cleaned = re.sub(r'^```(?:json)?\s*', '', cleaned)
        cleaned = re.sub(r'\s*```$', '', cleaned)
    
    # ‡∏•‡∏ö whitespace ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
    cleaned = cleaned.strip()
    
    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ { ‡πÉ‡∏´‡πâ return text ‡πÄ‡∏î‡∏¥‡∏°
    if '{' not in cleaned:
        return cleaned
    
    # ‡∏´‡∏≤ JSON object ‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
    brace_count = 0
    bracket_count = 0
    in_string = False
    escape_next = False
    last_valid_pos = -1
    start_pos = cleaned.find('{')
    
    if start_pos == -1:
        return cleaned
    
    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå JSON structure
    for i in range(start_pos, len(cleaned)):
        char = cleaned[i]
        
        if escape_next:
            escape_next = False
            continue
        
        if char == '\\':
            escape_next = True
            continue
        
        if char == '"' and not escape_next:
            in_string = not in_string
            continue
        
        if in_string:
            continue
        
        if char == '{':
            brace_count += 1
        elif char == '}':
            brace_count -= 1
            if brace_count == 0 and bracket_count == 0:
                last_valid_pos = i
                break
        elif char == '[':
            bracket_count += 1
        elif char == ']':
            bracket_count -= 1
    
    # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠ JSON object ‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏±‡πâ‡∏ô
    if last_valid_pos > start_pos:
        return cleaned[start_pos:last_valid_pos + 1]
    
    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡∏ã‡πà‡∏≠‡∏°‡πÅ‡∏ã‡∏°
    if brace_count > 0 or bracket_count > 0 or in_string:
        # ‡∏´‡∏≤ string ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏õ‡∏¥‡∏î quote
        in_string = False
        escape_next = False
        string_start = -1
        last_string_char = -1
        
        for i in range(start_pos, len(cleaned)):
            char = cleaned[i]
            
            if escape_next:
                escape_next = False
                continue
            
            if char == '\\':
                escape_next = True
                continue
            
            if char == '"' and not escape_next:
                if not in_string:
                    in_string = True
                    string_start = i
                else:
                    in_string = False
                    string_start = -1
                    last_string_char = i
        
        # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ string ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏õ‡∏¥‡∏î ‡πÉ‡∏´‡πâ‡∏õ‡∏¥‡∏î‡∏°‡∏±‡∏ô
        if in_string and string_start >= 0:
            # ‡∏´‡∏≤‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏¥‡∏î string
            # ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡∏ó‡πâ‡∏≤‡∏¢‡∏™‡∏∏‡∏î‡∏¢‡πâ‡∏≠‡∏ô‡∏Å‡∏•‡∏±‡∏ö
            insert_pos = len(cleaned)
            for i in range(len(cleaned) - 1, string_start, -1):
                if cleaned[i] in [',', '}', ']', '\n', '\r']:
                    insert_pos = i
                    break
            
            # ‡∏õ‡∏¥‡∏î string
            cleaned = cleaned[:insert_pos] + '"' + cleaned[insert_pos:]
        
        # ‡∏õ‡∏¥‡∏î brackets ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠ (‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏¥‡∏î‡∏Å‡πà‡∏≠‡∏ô braces)
        if bracket_count > 0:
            cleaned = cleaned + ']' * bracket_count
        
        # ‡∏õ‡∏¥‡∏î braces ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠
        if brace_count > 0:
            cleaned = cleaned + '}' * brace_count
    
    return cleaned

@retry_on_quota_error(max_retries=3, initial_delay=20)
def call_gemini_with_retry(prompt_or_payload, task_type='prediction_analysis'):
    """‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Gemini ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏∞‡∏ö‡∏ö retry (‡∏•‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î 3 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á)"""
    # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô string ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á payload ‡∏î‡πâ‡∏ß‡∏¢ detailed_prompt
    if isinstance(prompt_or_payload, str):
        payload = {'detailed_prompt': prompt_or_payload}
    else:
        payload = prompt_or_payload
    
    return call_gemini_structured(task_type, payload)


def call_gemini_structured(task_name: str, payload: Dict[str, Any], schema_key: str = 'insights'):
    """
    ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Gemini API ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á prompt ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤‡πÅ‡∏•‡∏∞ detailed prompt
    """
    if not is_gemini_available():
        raise RuntimeError("Gemini API is not configured")

    # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ detailed_prompt ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡πÅ‡∏ó‡∏ô
    if 'detailed_prompt' in payload:
        user_prompt = payload['detailed_prompt']
        logger.info("üìù Using detailed prompt for Gemini analysis")
    else:
        prompt_payload = {
            'task': task_name,
            'payload': payload
        }
        user_prompt = json.dumps(prompt_payload, ensure_ascii=False)

    base_generation_config = {
        "temperature": 0.3,
        "top_p": 0.95,
        "top_k": 40,
        "max_output_tokens": 4096,
        "response_mime_type": "application/json"
    }

    schema = GEMINI_RESPONSE_SCHEMAS.get(schema_key)
    if schema:
        base_generation_config["response_schema"] = schema
        logger.info(f"üìã Using schema: {schema_key}")
    else:
        logger.warning(f"‚ö†Ô∏è Schema '{schema_key}' not found, using default")

    def _execute_with_model(model_name: str):
        generation_config = copy.deepcopy(base_generation_config)
        
        # Increase token limit for 1.5 models which support higher output
        if '1.5' in model_name:
            generation_config['max_output_tokens'] = 8192
            
        model = genai.GenerativeModel(
            model_name,
            system_instruction=GEMINI_SYSTEM_PROMPT,
            generation_config=generation_config
        )

        logger.info(f"üì§ Sending request to Gemini (task: {task_name}, model: {model_name})...")
        logger.debug(f"Prompt length: {len(user_prompt)} characters")

        response = model.generate_content(user_prompt)
        response_text = None

        if hasattr(response, 'text') and response.text:
            response_text = response.text
            logger.debug("Got response from response.text")

        if not response_text and hasattr(response, 'candidates') and response.candidates:
            try:
                candidate = response.candidates[0]
                if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                    parts = candidate.content.parts
                    if parts:
                        first_part = parts[0]
                        if hasattr(first_part, 'text'):
                            response_text = first_part.text
                        elif isinstance(first_part, dict) and 'text' in first_part:
                            response_text = first_part['text']
                        logger.debug("Got response from candidates[0].content.parts[0]")
            except Exception as e:
                logger.warning(f"Error extracting from candidates: {e}")

        if not response_text:
            try:
                response_str = str(response)
                if response_str and response_str != 'None':
                    json_match = re.search(r'\{.*\}', response_str, re.DOTALL)
                    if json_match:
                        response_text = json_match.group(0)
                        logger.debug("Got response from string extraction")
            except Exception as e:
                logger.warning(f"Error extracting from string: {e}")

        if not response_text:
            error_msg = "Empty response from Gemini"
            logger.error(f"‚ùå {error_msg}")
            logger.debug(f"Response object: {response}")
            logger.debug(f"Response type: {type(response)}")
            logger.debug(f"Response dir: {dir(response)}")
            raise ValueError(error_msg)

        logger.debug(f"Response text length: {len(response_text)} characters")
        logger.debug(f"Response preview: {response_text[:200]}...")

        try:
            result = json.loads(response_text)
            logger.info("‚úÖ Gemini analysis completed successfully")
            return result
        except json.JSONDecodeError as json_err:
            logger.warning(f"‚ö†Ô∏è Initial JSON parse failed: {json_err}")
            logger.debug(f"Response text (first 1000 chars): {response_text[:1000]}...")
            
            try:
                # Attempt 1: Clean markdown code blocks
                cleaned_text = response_text.strip()
                if cleaned_text.startswith('```'):
                    cleaned_text = re.sub(r'^```(?:json)?\s*', '', cleaned_text)
                    cleaned_text = re.sub(r'\s*```$', '', cleaned_text)
                
                try:
                    result = json.loads(cleaned_text)
                    logger.info("‚úÖ Successfully parsed JSON after cleaning markdown")
                    return result
                except json.JSONDecodeError as e1:
                    logger.debug(f"Markdown cleaning failed: {e1}")
                    
                    # Attempt 2: Repair incomplete JSON (unterminated strings, unclosed braces)
                    try:
                        repaired_text = repair_incomplete_json(cleaned_text)
                        result = json.loads(repaired_text)
                        logger.info("‚úÖ Successfully parsed JSON after repair")
                        return result
                    except json.JSONDecodeError as e2:
                        logger.debug(f"JSON repair failed: {e2}")
                        
                        # Attempt 3: Extract JSON object using regex (fallback)
                        json_match = re.search(r'\{.*\}', cleaned_text, re.DOTALL)
                        if json_match:
                            extracted_json = json_match.group(0)
                            try:
                                # Try to repair extracted JSON too
                                repaired_extracted = repair_incomplete_json(extracted_json)
                                result = json.loads(repaired_extracted)
                                logger.info("‚úÖ Successfully extracted and parsed JSON using regex + repair")
                                return result
                            except json.JSONDecodeError:
                                # Last attempt: try original extracted
                                result = json.loads(extracted_json)
                                logger.info("‚úÖ Successfully extracted and parsed JSON using regex")
                                return result
                        raise ValueError(f"Invalid JSON response from Gemini: {str(json_err)}")
            except Exception as repair_exc:
                logger.error(f"‚ùå All JSON repair attempts failed: {repair_exc}")
                logger.error(f"Original error: {json_err}")
                logger.error(f"Response text (first 2000 chars): {response_text[:2000]}...")
                raise ValueError(f"Invalid JSON response from Gemini: {str(json_err)}")

    last_error = None
    tried_models: List[str] = []

    for model_name in GEMINI_MODEL_CANDIDATES:
        tried_models.append(model_name)
        try:
            return _execute_with_model(model_name)
        except ValueError as ve:
            logger.warning(f"‚ö†Ô∏è Gemini model '{model_name}' returned invalid response: {ve}")
            last_error = ve
            continue
        except Exception as exc:
            last_error = exc
            if _should_retry_with_fallback(exc) and model_name != GEMINI_MODEL_CANDIDATES[-1]:
                logger.warning(f"‚ö†Ô∏è Gemini model '{model_name}' unavailable ({exc}); trying fallback...")
                continue
            logger.error(f"‚ùå Gemini request failed on model '{model_name}': {exc}")
            import traceback
            logger.error(traceback.format_exc())
            raise RuntimeError(f"Gemini API error ({model_name}): {str(exc)}") from exc

    raise RuntimeError(
        f"Gemini API error after trying models {', '.join(tried_models)}: {last_error}"
    )

# Flask Routes (Keep all other routes unchanged)

def refresh_r2_storage_from_settings():
    """Reinitialize storage object with current environment settings."""
    global storage
    try:
        # Ensure latest settings are in os.environ before creating new S3Storage
        if admin_manager.enabled:
            admin_manager.apply_runtime_env()
        storage = S3Storage()
        return True
    except Exception as e:
        logger.error(f"Failed to refresh R2 storage runtime: {e}")
        return False


@app.route('/admin/login', methods=['GET', 'POST'])
def admin_login():
    # Always allow login ‚Äî if MongoDB is down, use fallback credentials
    if request.method == 'GET':
        if session.get('user_id'):
            return redirect(url_for('admin_settings'))
        return render_template('admin_login.html')

    payload = request.get_json(silent=True) or request.form
    username = (payload.get('username') or '').strip()
    password = payload.get('password') or ''

    user = None
    if admin_manager.enabled:
        user = admin_manager.authenticate(username, password)

    # Fallback: if MongoDB auth failed or disabled, try env-based auth
    if not user and not admin_manager.enabled:
        fallback_username = os.environ.get('ADMIN_USERNAME', 'admin')
        fallback_password = os.environ.get('ADMIN_PASSWORD', 'admin123')
        if username == fallback_username and password == fallback_password:
            user = {
                '_id': 'local-admin',
                'username': fallback_username,
                'role': 'super_admin',
                'must_change_password': False,
            }

    if not user:
        if request.path.startswith('/api/') or request.is_json:
            return _json_error('Invalid credentials', 401)
        return render_template('admin_login.html', error='‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏´‡∏±‡∏™‡∏ú‡πà‡∏≤‡∏ô‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á')

    session['user_id'] = str(user.get('_id'))
    session['username'] = user.get('username')
    session['user_role'] = user.get('role', 'user')
    session['must_change_password'] = bool(user.get('must_change_password', False))
    session.permanent = True

    if admin_manager.enabled:
        admin_manager._log_audit(session['user_id'], 'LOGIN', 'admin_login')

    if session.get('must_change_password'):
        return redirect(url_for('admin_change_password'))

    # Redirect to the page user was trying to visit before login
    next_url = request.args.get('next') or request.form.get('next') or url_for('admin_settings')
    # Safety: only allow relative URLs
    if not next_url.startswith('/'):
        next_url = url_for('admin_settings')
    return redirect(next_url)


@app.route('/admin')
def admin_home():
    if not is_logged_in():
        return redirect(url_for('admin_login'))
    if session.get('must_change_password'):
        return redirect(url_for('admin_change_password'))
    if has_any_role('admin', 'super_admin'):
        return redirect(url_for('admin_settings'))
    return redirect(url_for('index'))


@app.route('/admin/logout')
def admin_logout():
    if admin_manager.enabled and session.get('user_id'):
        admin_manager._log_audit(session.get('user_id'), 'LOGOUT', 'admin_logout')
    session.clear()
    return redirect(url_for('admin_login'))


@app.route('/admin/change-password', methods=['GET', 'POST'])
@admin_login_required
def admin_change_password():
    if not admin_manager.enabled:
        return render_template('admin_change_password.html', error='‡πÇ‡∏´‡∏°‡∏î fallback ‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏£‡∏´‡∏±‡∏™‡∏ú‡πà‡∏≤‡∏ô ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ MongoDB ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ')

    if request.method == 'GET':
        return render_template('admin_change_password.html')

    payload = request.get_json(silent=True) or request.form
    current_password = payload.get('current_password', '')
    new_password = payload.get('new_password', '')
    confirm_password = payload.get('confirm_password', '')

    if len(new_password) < 8:
        return render_template('admin_change_password.html', error='‡∏£‡∏´‡∏±‡∏™‡∏ú‡πà‡∏≤‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏¢‡∏≤‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 8 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£')
    if new_password != confirm_password:
        return render_template('admin_change_password.html', error='‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏£‡∏´‡∏±‡∏™‡∏ú‡πà‡∏≤‡∏ô‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô')

    user_doc = admin_manager.users.find_one({'username': session.get('username')})
    if not user_doc:
        return render_template('admin_change_password.html', error='‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ö‡∏±‡∏ç‡∏ä‡∏µ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ')

    if not user_doc.get('must_change_password'):
        if not check_password_hash(user_doc.get('password_hash', ''), current_password):
            return render_template('admin_change_password.html', error='‡∏£‡∏´‡∏±‡∏™‡∏ú‡πà‡∏≤‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á')

    admin_manager.change_password(user_doc['_id'], new_password)
    session['must_change_password'] = False
    admin_manager._log_audit(session.get('user_id'), 'CHANGE_PASSWORD', session.get('username'))
    return render_template('admin_change_password.html', success='‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏£‡∏´‡∏±‡∏™‡∏ú‡πà‡∏≤‡∏ô‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à')


@app.route('/admin/settings', methods=['GET', 'POST'])
@admin_login_required
@roles_required('admin', 'super_admin')
def admin_settings():
    setting_map = {
        'gemini_api_key': 'GEMINI_API_KEY',
        'gemini_model_name': 'GEMINI_MODEL_NAME',
        'gemini_model_fallbacks': 'GEMINI_MODEL_FALLBACKS',
        'r2_access_key': 'CLOUDFLARE_R2_ACCESS_KEY_ID',
        'r2_secret_key': 'CLOUDFLARE_R2_SECRET_ACCESS_KEY',
        'r2_endpoint': 'CLOUDFLARE_R2_ENDPOINT',
        'r2_bucket_name': 'CLOUDFLARE_R2_BUCKET_NAME',
    }

    success_msg = None

    if request.method == 'POST':
        form = request.get_json(silent=True) or request.form

        for form_key, setting_key in setting_map.items():
            if form_key in form:
                raw = form.get(form_key)
                value = str(raw).strip() if raw is not None else ''
                # Save value (including empty to allow clearing keys)
                if admin_manager.enabled:
                    admin_manager.set_setting(setting_key, value, actor_user_id=session.get('user_id'))
                # Always update os.environ so runtime picks it up
                if value:
                    os.environ[setting_key] = value
                elif setting_key in os.environ:
                    del os.environ[setting_key]

        if admin_manager.enabled:
            admin_manager.apply_runtime_env()
        refresh_gemini_runtime_from_settings()
        refresh_r2_storage_from_settings()

        if admin_manager.enabled:
            success_msg = '‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß ‚úÖ'
        else:
            success_msg = '‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ñ‡πà‡∏≤‡πÅ‡∏ö‡∏ö runtime ‡πÅ‡∏•‡πâ‡∏ß (‡πÇ‡∏´‡∏°‡∏î fallback: ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á MongoDB)'

    # Build current values (for both GET and after POST)
    def _get_val(key, default=''):
        return admin_manager.get_setting(key, os.environ.get(key, default))

    current_values = {
        'gemini_api_key': _get_val('GEMINI_API_KEY'),
        'gemini_model_name': _get_val('GEMINI_MODEL_NAME', 'gemini-3-flash'),
        'gemini_model_fallbacks': _get_val('GEMINI_MODEL_FALLBACKS'),
        'r2_access_key': _get_val('CLOUDFLARE_R2_ACCESS_KEY_ID'),
        'r2_secret_key': _get_val('CLOUDFLARE_R2_SECRET_ACCESS_KEY'),
        'r2_endpoint': _get_val('CLOUDFLARE_R2_ENDPOINT'),
        'r2_bucket_name': _get_val('CLOUDFLARE_R2_BUCKET_NAME'),
    }

    # Masked versions for sensitive fields
    current_values['gemini_api_key_masked'] = MongoAdminManager.mask_value(current_values['gemini_api_key'])
    current_values['r2_access_key_masked'] = MongoAdminManager.mask_value(current_values['r2_access_key'])
    current_values['r2_secret_key_masked'] = MongoAdminManager.mask_value(current_values['r2_secret_key'])

    # System status for template
    current_values['mongo_connected'] = admin_manager.enabled
    current_values['mongo_error'] = admin_manager.init_error
    current_values['gemini_ready'] = is_gemini_available()
    current_values['gemini_model'] = GEMINI_MODEL_NAME
    current_values['r2_connected'] = not getattr(storage, 'use_local', True)
    current_values['r2_bucket'] = getattr(storage, 'bucket_name', '')
    current_values['is_fallback'] = not admin_manager.enabled
    current_values['gemini_known_models'] = GEMINI_KNOWN_MODELS

    if success_msg:
        current_values['success'] = success_msg

    return render_template('admin_settings.html', **current_values)


@app.route('/api/admin/me')
@admin_login_required
def admin_me():
    return jsonify({
        'success': True,
        'username': session.get('username'),
        'role': session.get('user_role'),
        'must_change_password': bool(session.get('must_change_password'))
    })


@app.route('/api/admin/users', methods=['GET', 'POST'])
@admin_login_required
@roles_required('super_admin')
def admin_users_api():
    if not admin_manager.enabled:
        return _json_error('MongoDB admin backend is not configured', 503)

    if request.method == 'GET':
        users = []
        for u in admin_manager.users.find({}, {'password_hash': 0}):
            users.append({
                'id': str(u.get('_id')),
                'username': u.get('username'),
                'role': u.get('role', 'user'),
                'is_active': bool(u.get('is_active', True)),
                'must_change_password': bool(u.get('must_change_password', False)),
                'created_at': u.get('created_at')
            })
        return jsonify({'success': True, 'users': users})

    data = request.get_json(silent=True) or {}
    username = (data.get('username') or '').strip()
    role = (data.get('role') or 'user').strip()

    if not username:
        return _json_error('username is required', 400)
    if role not in ['user', 'admin', 'super_admin']:
        return _json_error('invalid role', 400)

    temp_password = secrets.token_urlsafe(10)
    try:
        admin_manager.users.insert_one({
            'username': username,
            'email': data.get('email'),
            'password_hash': generate_password_hash(temp_password),
            'role': role,
            'must_change_password': True,
            'is_active': True,
            'created_at': datetime.utcnow().isoformat(),
            'updated_at': datetime.utcnow().isoformat(),
            'last_login_at': None
        })
        admin_manager._log_audit(session.get('user_id'), 'CREATE_USER', username)
        return jsonify({
            'success': True,
            'username': username,
            'role': role,
            'temporary_password': temp_password
        })
    except Exception as e:
        return _json_error(f'cannot create user: {e}', 400)


@app.route('/api/admin/test-connections', methods=['POST'])
@admin_login_required
@roles_required('admin', 'super_admin')
def admin_test_connections():
    """Test runtime connections for MongoDB, Cloudflare R2, and Gemini."""
    results = {
        'success': True,
        'mongo': {'connected': False, 'message': ''},
        'r2': {'connected': False, 'message': ''},
        'gemini': {'connected': False, 'message': ''},
    }

    # MongoDB
    try:
        if admin_manager.enabled and admin_manager.client:
            admin_manager.client.admin.command('ping')
            results['mongo'] = {'connected': True, 'message': 'MongoDB ping success'}
        else:
            results['mongo'] = {'connected': False, 'message': 'MongoDB admin backend not enabled'}
    except Exception as e:
        results['mongo'] = {'connected': False, 'message': str(e)}

    # R2
    try:
        refresh_r2_storage_from_settings()
        if storage.use_local or not getattr(storage, 's3_client', None):
            results['r2'] = {'connected': False, 'message': 'R2 client not initialized (using local storage)'}
        else:
            storage.s3_client.list_objects_v2(Bucket=storage.bucket_name, MaxKeys=1)
            results['r2'] = {'connected': True, 'message': f'Connected to bucket {storage.bucket_name}'}
    except Exception as e:
        results['r2'] = {'connected': False, 'message': str(e)}

    # Gemini ‚Äî test the PRIMARY model the user selected (not fallbacks)
    try:
        refresh_gemini_runtime_from_settings()
        if not is_gemini_available():
            results['gemini'] = {'connected': False, 'message': '‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Gemini API Key ‡∏´‡∏£‡∏∑‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•'}
        else:
            if genai is None:
                raise RuntimeError('google.generativeai not installed')
            primary_model = GEMINI_MODEL_NAME
            try:
                model = genai.GenerativeModel(primary_model)
                ping_response = model.generate_content('ping')
                if getattr(ping_response, 'text', None):
                    results['gemini'] = {
                        'connected': True,
                        'message': f'‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏• {primary_model} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à',
                        'model': primary_model
                    }
                else:
                    results['gemini'] = {
                        'connected': False,
                        'message': f'‡πÇ‡∏°‡πÄ‡∏î‡∏• {primary_model} ‡πÑ‡∏°‡πà‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö ‚Äî ‡∏•‡∏≠‡∏á‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏ô‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤',
                        'model': primary_model
                    }
            except Exception as model_err:
                # Primary model failed ‚Äî suggest a working fallback
                suggested = None
                for fallback in GEMINI_DEFAULT_FALLBACKS:
                    if fallback == primary_model:
                        continue
                    try:
                        fb_model = genai.GenerativeModel(fallback)
                        fb_resp = fb_model.generate_content('ping')
                        if getattr(fb_resp, 'text', None):
                            suggested = fallback
                            break
                    except Exception:
                        continue
                msg = f'‡πÇ‡∏°‡πÄ‡∏î‡∏• {primary_model} ‡πÉ‡∏ä‡πâ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ({model_err})'
                if suggested:
                    msg += f' ‚Äî ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô {suggested}'
                results['gemini'] = {
                    'connected': False,
                    'message': msg,
                    'model': primary_model,
                    'suggested_model': suggested
                }
    except Exception as e:
        results['gemini'] = {'connected': False, 'message': str(e)}

    overall_ok = all([
        results['mongo']['connected'],
        results['r2']['connected'],
        results['gemini']['connected'],
    ])

    results['success'] = overall_ok
    admin_manager._log_audit(session.get('user_id'), 'TEST_CONNECTIONS', 'mongo_r2_gemini', 'success' if overall_ok else 'failed', results)
    return jsonify(results)


@app.route('/api/admin/gemini-models', methods=['GET'])
@admin_login_required
@roles_required('admin', 'super_admin')
def admin_list_gemini_models():
    """List available Gemini models: known models + live models from API."""
    result = {
        'success': True,
        'known_models': GEMINI_KNOWN_MODELS,
        'live_models': [],
        'current_model': GEMINI_MODEL_NAME,
    }

    # Try to fetch live models from the Gemini API
    if genai and GEMINI_API_KEY:
        try:
            genai.configure(api_key=GEMINI_API_KEY)
            live = []
            for m in genai.list_models():
                name = m.name  # e.g. "models/gemini-2.0-flash"
                model_id = name.replace('models/', '') if name.startswith('models/') else name
                # Only include models that support generateContent
                supported = [method for method in m.supported_generation_methods]
                if 'generateContent' in supported:
                    live.append({
                        'id': model_id,
                        'display_name': getattr(m, 'display_name', model_id),
                        'desc': getattr(m, 'description', '')[:80],
                    })
            result['live_models'] = live
        except Exception as e:
            logger.warning(f"Could not list Gemini models: {e}")
            result['live_error'] = str(e)

    return jsonify(result)


@app.route('/')
def index():
    """Landing page: redirect users to home page."""
    return redirect(url_for('main_page'))

@app.route('/test')
def curriculum_prediction_form():
    """Page for predicting graduation based on curriculum and prerequisites."""
    try:
        # ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô JSON string ‡∏ó‡∏µ‡πà‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
        courses_json = json.dumps(app.config.get('COURSES_DATA', []))
        terms_json = json.dumps(app.config.get('ALL_TERMS_DATA', []))
        grades_json = json.dumps(app.config.get('DATA_CONFIG', {}).get('grade_mapping', {}))
        
        return render_template(
            'curriculum_prediction_form.html',
            coursesData=courses_json,
            allTermsData=terms_json,
            gradeMapping=grades_json
        )
    except Exception as e:
        logger.error(f"Error rendering curriculum form: {str(e)}")
        # Fallback ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤
        return render_template(
            'curriculum_prediction_form.html',
            coursesData='[]',
            allTermsData='[]',
            gradeMapping='{}'
        )


@app.route('/status')
def status_page():
    """‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏£‡∏∞‡∏ö‡∏ö"""
    return render_template('status.html')

@app.route('/api/system/status')
def system_status():
    """API ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏£‡∏∞‡∏ö‡∏ö"""
    import sys
    import flask
    
    try:
        logger.info("üîß Checking system status...")
        
        # ‡πÉ‡∏ä‡πâ instance ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß
        storage_status = storage.get_connection_status()
        
        # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•
        try:
            models_list = storage.list_models()
            total_size = sum(m.get('size', 0) for m in models_list if 'size' in m)
        except Exception as e:
            logger.warning(f"Could not get models info: {e}")
            models_list = []
            total_size = 0
        
        # Get recent logs (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ log file)
        recent_logs = []
        try:
            log_file = 'app.log'
            if os.path.exists(log_file):
                with open(log_file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    recent_logs = lines[-20:]
        except Exception as e:
            logger.warning(f"Could not read logs: {e}")

        # ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å storage_status ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
        status_data = {
            'success': True,
            'r2_connected': storage_status['connected'],
            'storage_provider': storage_status['storage_type'],
            'bucket_name': storage_status['bucket_name'],
            'models_available': len(models_list),
            'total_size': total_size,
            'python_version': sys.version.split()[0],
            'flask_version': flask.__version__,
            'environment': os.environ.get('FLASK_ENV', 'production'),
            'debug_mode': app.debug,
            'server_time': datetime.now().isoformat(),
            'app_folders': {
                'upload_folder': app.config['UPLOAD_FOLDER'],
                'model_folder': app.config['MODEL_FOLDER'],
                'upload_exists': os.path.exists(app.config['UPLOAD_FOLDER']),
                'model_exists': os.path.exists(app.config['MODEL_FOLDER'])
            },
            'recent_logs': recent_logs,
            'error_message': storage_status.get('errors'),
            'env_vars_status': {
                'R2_ACCESS_KEY': bool(os.environ.get('CLOUDFLARE_R2_ACCESS_KEY_ID')),
                'R2_SECRET_KEY': bool(os.environ.get('CLOUDFLARE_R2_SECRET_ACCESS_KEY')),
                'R2_ENDPOINT': bool(os.environ.get('CLOUDFLARE_R2_ENDPOINT')),
                'R2_BUCKET': bool(os.environ.get('CLOUDFLARE_R2_BUCKET_NAME'))
            }
        }
        
        logger.info("‚úÖ System status check completed")
        return jsonify(status_data)
        
    except Exception as e:
        logger.error(f"‚ùå Error getting system status: {str(e)}")
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc(),
            'server_time': datetime.now().isoformat()
        }), 500

@app.route('/api/test-r2-connection')
def test_r2_connection():
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ R2"""
    try:
        logger.info("üß™ Testing R2 connection...")
        
        if storage.use_local or not hasattr(storage, 's3_client') or not storage.s3_client:
            return jsonify({
                'success': False,
                'error': 'R2 client not initialized - using local storage',
                'storage_provider': 'local'
            })
        
        # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö list objects
        response = storage.s3_client.list_objects_v2(
            Bucket=storage.bucket_name,
            MaxKeys=1
        )
        
        logger.info("‚úÖ R2 connection test successful")
        return jsonify({
            'success': True,
            'message': 'R2 connection successful',
            'bucket': storage.bucket_name,
            'endpoint': storage.endpoint_url,
            'objects_found': len(response.get('Contents', []))
        })
        
    except Exception as e:
        logger.error(f"‚ùå R2 connection test failed: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e),
            'storage_provider': 'local_fallback'
        })
        
@app.route('/api/test-r2')
def test_r2():
    try:
        # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏ü‡∏•‡πå
        test_data = {'test': True, 'timestamp': datetime.now().isoformat()}
        success = storage.save_model(test_data, 'test_model.joblib')
        
        if success:
            # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå
            loaded = storage.load_model('test_model.joblib')
            if loaded:
                # ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏î‡∏™‡∏≠‡∏ö
                storage.delete_model('test_model.joblib')
                return jsonify({
                    'success': True,
                    'message': 'R2 storage working perfectly!',
                    'can_write': True,
                    'can_read': True,
                    'can_delete': True
                })
        
        return jsonify({'success': False, 'message': 'R2 test failed'})
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})
        
@app.route('/api/config', methods=['GET'])
def get_config_for_frontend():
    """Provides frontend with necessary configuration data."""
    try:
        config_data = {
            'success': True,
            'COURSES_DATA': app.config.get('COURSES_DATA', []),
            'ALL_TERMS_DATA': app.config.get('ALL_TERMS_DATA', []),
            'GRADE_MAPPING': app.config.get('DATA_CONFIG', {}).get('grade_mapping', {}),
            'MESSAGES': app.config.get('MESSAGES', {}),
            'DATA_CONFIG_RISK_LEVELS': app.config.get('DATA_CONFIG', {}).get('risk_levels', {}),
            'GEMINI_ENABLED': is_gemini_available()
        }
        return jsonify(config_data)
    except Exception as e:
        logger.error(f"Error loading config data: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/gemini/analyze_file', methods=['POST'])
def gemini_analyze_file_route():
    """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÑ‡∏ü‡∏•‡πå CSV/XLSX ‡∏î‡πâ‡∏ß‡∏¢ Gemini"""
    if not is_gemini_available():
        return jsonify({'success': False, 'error': '‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Gemini API Key'}), 503
    
    uploaded_file = request.files.get('file')
    analysis_goal = request.form.get('analysis_goal', '').strip()
    
    if uploaded_file is None or uploaded_file.filename == '':
        return jsonify({'success': False, 'error': '‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV ‡∏´‡∏£‡∏∑‡∏≠ Excel'}), 400
    
    try:
        file_bytes = uploaded_file.read()
        if not file_bytes:
            return jsonify({'success': False, 'error': '‡πÑ‡∏ü‡∏•‡πå‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤'}), 400
        
        max_bytes = int(GEMINI_MAX_FILE_SIZE_MB * 1024 * 1024)
        if len(file_bytes) > max_bytes:
            return jsonify({
                'success': False,
                'error': f'‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô {GEMINI_MAX_FILE_SIZE_MB} MB'
            }), 400
        
        buffer = BytesIO(file_bytes)
        filename = uploaded_file.filename.lower()
        
        if filename.endswith(('.xlsx', '.xls')):
            df = pd.read_excel(buffer)
        else:
            buffer.seek(0)
            df = pd.read_csv(buffer)
        
        summary, samples = summarize_dataframe_for_gemini(df)
        payload = {
            'analysis_goal': analysis_goal or '‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏£‡∏î‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå',
            'dataset_summary': summary,
            'sample_rows': samples
        }
        gemini_output = call_gemini_structured('csv_dataset_analysis', payload)
        
        return jsonify({
            'success': True,
            'source': 'file',
            'dataset_summary': summary,
            'sample_rows': samples,
            'gemini': gemini_output
        })
    except Exception as exc:
        logger.error(f"Gemini file analysis error: {exc}")
        return jsonify({'success': False, 'error': str(exc)}), 400


@app.route('/api/gemini/predict', methods=['POST'])
def gemini_predict_route():
    """‡πÉ‡∏´‡πâ Gemini ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏£‡∏î‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏Å‡∏£‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏™‡∏î"""
    if not is_gemini_available():
        logger.warning("Gemini API not available - missing API key")
        return jsonify({
            'success': False, 
            'error': '‚ö†Ô∏è ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Gemini API Key',
            'suggestion': '‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ GEMINI_API_KEY ‡πÉ‡∏ô environment variables'
        }), 503
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö rate limit
    can_proceed, wait_time = gemini_rate_limiter.can_proceed()
    if not can_proceed:
        return jsonify({
            'success': False,
            'error': f'‚è±Ô∏è ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ö‡πà‡∏≠‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏≠ {wait_time} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ',
            'suggestion': '‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏≠‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏π‡πà‡∏Å‡πà‡∏≠‡∏ô‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà',
            'retry_after': wait_time
        }), 429
    
    try:
        payload = request.get_json(silent=True) or {}
        logger.info(f"Received Gemini prediction request: {list(payload.keys())}")
        
        course_grades = payload.get('course_grades') or payload.get('grades') or {}
        student_name = payload.get('student_name', '‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤')
        analysis_goal = payload.get('analysis_goal', '').strip()
        loaded_terms_count = int(payload.get('loaded_terms_count') or 0)
        model_filename = payload.get('model_filename')
        model_prediction = payload.get('model_prediction')  # from combined mode
        graduation_analysis_input = payload.get('graduation_analysis')  # from combined mode
        combined_mode = payload.get('combined_mode', False)
        avg_gpa_input = payload.get('avg_gpa')
        training_analysis = None
        model_metadata = None
        
        logger.info(f"Processing prediction for {student_name} with {len(course_grades)} course grades")
        
        if not isinstance(course_grades, dict) or len(course_grades) == 0:
            logger.warning("No course grades provided")
            return jsonify({'success': False, 'error': '‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏£‡∏î‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡∏ß‡∏¥‡∏ä‡∏≤'}), 400
    except Exception as parse_error:
        logger.error(f"Error parsing request: {parse_error}")
        return jsonify({'success': False, 'error': f'‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {str(parse_error)}'}), 400
    
    # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏Å‡∏£‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏á
    cleaned_grades = {
        str(course_id): grade
        for course_id, grade in course_grades.items()
        if grade
    }
    
    if len(cleaned_grades) == 0:
        return jsonify({'success': False, 'error': '‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏Å‡∏£‡∏î‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ô‡∏≥‡∏°‡∏≤‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÑ‡∏î‡πâ'}), 400
    
    # ‡πÇ‡∏´‡∏•‡∏î Course DNA ‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
    course_profiles = None
    course_context_str = ""
    training_analysis = None
    model_metadata = None
    
    try:
        if model_filename:
            try:
                stored_model = storage.load_model(model_filename)
                if stored_model:
                    training_analysis = stored_model.get('gemini_training_analysis')
                    course_profiles = stored_model.get('course_profiles', {})
                    model_metadata = {
                        'model_filename': model_filename,
                        'data_format': stored_model.get('data_format'),
                        'training_type': stored_model.get('training_type'),
                        'performance_metrics': stored_model.get('performance_metrics')
                    }
                    logger.info(f"Loaded model context: {model_filename}")
            except Exception as model_exc:
                logger.warning(f"Unable to load model for Gemini context: {model_exc}")
        else:
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏ö‡∏∏ model_filename ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
            try:
                models = storage.list_models()
                if models:
                    latest_model = models[0]  # ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
                    loaded_model = storage.load_model(latest_model['filename'])
                    if loaded_model:
                        course_profiles = loaded_model.get('course_profiles', {})
                        logger.info(f"Loaded course profiles from latest model: {latest_model['filename']}")
            except Exception as load_exc:
                logger.warning(f"Unable to load latest model for course profiles: {load_exc}")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö Course DNA
        if course_profiles:
            student_risk_factors = []
            student_easy_courses_failed = []
            
            for course_id, grade in cleaned_grades.items():
                if course_id in course_profiles:
                    prof = course_profiles[course_id]
                    fail_rate = prof.get('fail_rate', 0)
                    avg_grade = prof.get('avg_grade', 0)
                    difficulty_score = prof.get('difficulty_score', 0)
                    
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Killer Course ‡πÅ‡∏•‡∏∞‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ï‡∏Å
                    is_killer = fail_rate >= 0.3  # 30% fail rate threshold
                    is_failed = grade in ['F', 'W', 'WF', 'WU', 'D'] or (isinstance(grade, str) and grade.upper() in ['F', 'W', 'WF', 'WU', 'D'])
                    
                    if is_killer and is_failed:
                        student_risk_factors.append(
                            f"- ‡∏ï‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤‡∏õ‡∏£‡∏≤‡∏ö‡πÄ‡∏ã‡∏µ‡∏¢‡∏ô {course_id} (‡∏õ‡∏Å‡∏ï‡∏¥‡∏Ñ‡∏ô‡∏ï‡∏Å {fail_rate*100:.0f}%, ‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢: {avg_grade:.2f})"
                        )
                    elif is_killer and not is_failed:
                        # ‡∏ú‡πà‡∏≤‡∏ô Killer Course ‡πÅ‡∏•‡πâ‡∏ß (‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì‡∏î‡∏µ)
                        student_risk_factors.append(
                            f"- ‡∏ú‡πà‡∏≤‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏õ‡∏£‡∏≤‡∏ö‡πÄ‡∏ã‡∏µ‡∏¢‡∏ô {course_id} ‡πÅ‡∏•‡πâ‡∏ß (‡∏õ‡∏Å‡∏ï‡∏¥‡∏Ñ‡∏ô‡∏ï‡∏Å {fail_rate*100:.0f}%) ‚úì"
                        )
                    
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Easy Course ‡πÅ‡∏ï‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏Å‡∏£‡∏î‡πÅ‡∏¢‡πà
                    is_easy = avg_grade >= 3.5 and fail_rate <= 0.1
                    is_poor_grade = grade in ['C', 'D', 'F', 'W', 'WF', 'WU'] or (isinstance(grade, str) and grade.upper() in ['C', 'D', 'F', 'W', 'WF', 'WU'])
                    
                    if is_easy and is_poor_grade:
                        student_easy_courses_failed.append(
                            f"- ‡πÑ‡∏î‡πâ‡πÄ‡∏Å‡∏£‡∏î‡∏ô‡πâ‡∏≠‡∏¢‡πÉ‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ä‡πà‡∏ß‡∏¢ {course_id} (‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏õ‡∏Å‡∏ï‡∏¥: {avg_grade:.2f}, ‡πÑ‡∏î‡πâ: {grade})"
                        )
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° Course Context
            if student_risk_factors or student_easy_courses_failed:
                course_context_str = "\n**‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ï‡∏≤‡∏° Course DNA (Backend System):**\n"
                if student_risk_factors:
                    course_context_str += "\n".join(student_risk_factors)
                if student_easy_courses_failed:
                    course_context_str += "\n" + "\n".join(student_easy_courses_failed)
                course_context_str += "\n"
    except Exception as context_exc:
        logger.warning(f"Error building course context: {context_exc}")
        course_context_str = ""
    
    try:
        logger.info(f"üîÆ Starting Gemini prediction for {student_name} with {len(cleaned_grades)} courses")
        
        grade_summary = summarize_grades_for_gemini(cleaned_grades, loaded_terms_count)
        grade_summary['student_name'] = student_name
        
        # =========================================================
        # üìù AUTO-PROMPT GENERATION: ‡∏™‡∏£‡πâ‡∏≤‡∏á Prompt ‡πÉ‡∏´‡πâ User ‡πÄ‡∏≠‡∏á
        # =========================================================
        
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Goal ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ ‡∏ñ‡πâ‡∏≤ User ‡πÑ‡∏°‡πà‡∏û‡∏¥‡∏°‡∏û‡πå‡∏°‡∏≤
        if not analysis_goal:
            analysis_goal = "‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÅ‡∏•‡∏∞‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î"
        
        analysis_goal_text = analysis_goal
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Detailed Prompt ‡∏ó‡∏µ‡πà‡∏â‡∏•‡∏≤‡∏î‡∏Ç‡∏∂‡πâ‡∏ô - ‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏à‡∏ö/‡πÑ‡∏°‡πà‡∏à‡∏ö
        detailed_prompt = f"""
‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏ó‡∏≤‡∏á‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£ ‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà**‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢**‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏Ç‡∏≠‡∏á‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤

**‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤:**
- ‡∏ä‡∏∑‡πà‡∏≠: {student_name}
- ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÅ‡∏•‡πâ‡∏ß: {grade_summary.get('total_courses', 0)} ‡∏ß‡∏¥‡∏ä‡∏≤
- ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ó‡∏≠‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô: {loaded_terms_count} ‡πÄ‡∏ó‡∏≠‡∏°
- ‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ (‡πÇ‡∏î‡∏¢‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì): {grade_summary.get('estimated_gpa', 0):.2f}
- ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÅ‡∏•‡πâ‡∏ß: {grade_summary.get('total_credits_recorded', 0)} ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï
- ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏ö‡∏ï‡∏Å: {grade_summary.get('failed_count', 0)} ‡∏ß‡∏¥‡∏ä‡∏≤

**‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÄ‡∏Å‡∏£‡∏î:**
{json.dumps(grade_summary.get('grade_distribution', {}), ensure_ascii=False, indent=2)}

**‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ß‡∏¥‡∏ä‡∏≤:**
{json.dumps(grade_summary.get('course_details', [])[:20], ensure_ascii=False, indent=2)}

**‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏ö‡∏ï‡∏Å (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ):**
{json.dumps(grade_summary.get('failed_courses', []), ensure_ascii=False, indent=2)}

{course_context_str}
(‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≤‡∏á‡∏ö‡∏ô‡∏Ñ‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏£‡∏∏‡πà‡∏ô‡∏û‡∏µ‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß)
"""

        # Add trained model prediction context for combined mode
        model_context_str = ""
        if combined_mode and model_prediction:
            mp = model_prediction
            model_context_str = f"""
**‡∏ú‡∏•‡∏à‡∏≤‡∏Å AI Model ‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß (‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö):**
- ‡∏ú‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢: {mp.get('prediction', 'N/A')}
- ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏à‡∏ö: {mp.get('prob_pass', 0) * 100:.1f}%
- ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡πÑ‡∏°‡πà‡∏à‡∏ö: {mp.get('prob_fail', 0) * 100:.1f}%
- ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à: {mp.get('confidence', 0) * 100:.1f}%
- ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á: {mp.get('risk_level', 'N/A')}
- ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ: {mp.get('method', 'N/A')}
- ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ: {', '.join(mp.get('models_used', []))}
"""
        if combined_mode and graduation_analysis_input:
            ga = graduation_analysis_input
            model_context_str += f"""
**‡∏ú‡∏•‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Å‡∏≤‡∏£‡∏à‡∏ö‡∏à‡∏≤‡∏Å Backend:**
- ‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏à‡∏ö: {'‡πÉ‡∏ä‡πà' if ga.get('will_graduate') else '‡πÑ‡∏°‡πà'}
- GPA: {avg_gpa_input or 'N/A'}
- ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢: {ga.get('prediction_method', 'N/A')}
"""

        detailed_prompt += f"""
{model_context_str}

**‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì (‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î):**

**1. ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤ (‡∏ï‡πâ‡∏≠‡∏á‡∏ü‡∏±‡∏ô‡∏ò‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô):**
   - ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ß‡πà‡∏≤‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏Ñ‡∏ô‡∏ô‡∏µ‡πâ "‡∏à‡∏∞‡∏à‡∏ö‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå" ‡∏´‡∏£‡∏∑‡∏≠ "‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏à‡∏ö/‡∏à‡∏ö‡∏ä‡πâ‡∏≤"
   - ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏à‡∏ö: GPA >= 2.00 ‡πÅ‡∏•‡∏∞‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï >= 136 ‡πÅ‡∏•‡∏∞‡∏ú‡πà‡∏≤‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö/‡∏ß‡∏¥‡∏ä‡∏≤‡∏õ‡∏£‡∏≤‡∏ö‡πÄ‡∏ã‡∏µ‡∏¢‡∏ô
   - ‡∏£‡∏∞‡∏ö‡∏∏‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå (0-100%)

**2. ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÑ‡∏°‡∏ñ‡∏∂‡∏á‡∏à‡∏ö (‡∏ñ‡πâ‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ß‡πà‡∏≤‡∏à‡∏ö):**
   - ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÑ‡∏°‡∏ñ‡∏∂‡∏á‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ß‡πà‡∏≤‡∏à‡∏ö
   - ‡πÄ‡∏ä‡πà‡∏ô: "GPA ‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå (2.15 >= 2.00), ‡∏ú‡πà‡∏≤‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏õ‡∏£‡∏≤‡∏ö‡πÄ‡∏ã‡∏µ‡∏¢‡∏ô‡πÅ‡∏•‡πâ‡∏ß, ‡∏°‡∏µ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏î‡∏µ"
   - ‡∏£‡∏∞‡∏ö‡∏∏‡∏õ‡∏±‡∏à‡∏à‡∏±‡∏¢‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏à‡∏ö‡πÑ‡∏î‡πâ

**3. ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÑ‡∏°‡∏ñ‡∏∂‡∏á‡πÑ‡∏°‡πà‡∏à‡∏ö (‡∏ñ‡πâ‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏à‡∏ö):**
   - ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÑ‡∏°‡∏ñ‡∏∂‡∏á‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏à‡∏ö
   - ‡πÄ‡∏ä‡πà‡∏ô: "GPA ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏ì‡∏ë‡πå (1.85 < 2.00), ‡∏ï‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤‡∏õ‡∏£‡∏≤‡∏ö‡πÄ‡∏ã‡∏µ‡∏¢‡∏ô X, Y, Z"
   - ‡∏£‡∏∞‡∏ö‡∏∏‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÑ‡∏°‡πà‡∏à‡∏ö (GPA ‡∏ï‡πà‡∏≥, ‡∏ï‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤ Killer, ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö)

**4. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å:**
   - ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏õ‡∏±‡∏à‡∏à‡∏±‡∏¢‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏ú‡∏•‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
   - ‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö "‡∏ß‡∏¥‡∏ä‡∏≤‡∏õ‡∏£‡∏≤‡∏ö‡πÄ‡∏ã‡∏µ‡∏¢‡∏ô" (Killer Courses) ‡∏ó‡∏µ‡πà‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ï‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å
   - ‡∏ñ‡πâ‡∏≤‡πÑ‡∏î‡πâ‡πÄ‡∏Å‡∏£‡∏î‡πÅ‡∏¢‡πà‡πÉ‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏á‡πà‡∏≤‡∏¢ (Easy Courses) ‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô

**5. ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:**
   - ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡πÉ‡∏ô‡πÄ‡∏ó‡∏≠‡∏°‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏≠‡∏î
   - ‡∏ñ‡πâ‡∏≤‡∏ï‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤ Killer ‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏°‡∏Ç‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡∏ò‡∏£‡∏£‡∏°
   - ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏à‡∏ö‡πÇ‡∏î‡∏¢‡∏î‡∏π‡∏ß‡πà‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡∏î‡πà‡∏≤‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏¢‡∏≤‡∏Å‡πÜ ‡πÑ‡∏õ‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á

**‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:**
- **‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏à‡∏ö/‡πÑ‡∏°‡πà‡∏à‡∏ö‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô** (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÅ‡∏Ñ‡πà‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå)
- **‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢** ‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÑ‡∏°‡∏ñ‡∏∂‡∏á‡∏à‡∏ö/‡πÑ‡∏°‡πà‡∏à‡∏ö
- ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°
- ‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö Course DNA ‡πÅ‡∏•‡∏∞ Killer Courses
- ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏ú‡∏•‡∏à‡∏≤‡∏Å AI Model ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö ‚Äî ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Å‡∏±‡∏ö AI Model ‡πÅ‡∏•‡πâ‡∏ß‡∏™‡∏£‡∏∏‡∏õ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô
- ‡∏ï‡∏≠‡∏ö‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö JSON ‡∏ï‡∏≤‡∏° Schema ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î

‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î ‡πÇ‡∏î‡∏¢‡πÉ‡∏´‡πâ:
- graduation_prediction: 
  * will_graduate: true/false (‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ß‡πà‡∏≤‡∏à‡∏ö‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏à‡∏ö)
  * prediction_text: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ (‡πÄ‡∏ä‡πà‡∏ô "‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏à‡∏ö‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå" ‡∏´‡∏£‡∏∑‡∏≠ "‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤")
  * reason_why_graduate: ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÑ‡∏°‡∏ñ‡∏∂‡∏á‡∏à‡∏ö (‡∏ñ‡πâ‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ß‡πà‡∏≤‡∏à‡∏ö) ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
  * reason_why_not_graduate: ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÑ‡∏°‡∏ñ‡∏∂‡∏á‡πÑ‡∏°‡πà‡∏à‡∏ö (‡∏ñ‡πâ‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏à‡∏ö) ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
  * confidence_percent: ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (0-100)
- analysis_markdown: ‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ (‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 200 ‡∏Ñ‡∏≥) ‡πÇ‡∏î‡∏¢‡πÄ‡∏ô‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•
- risk_level: ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á (very_low, low, moderate, high, very_high)
- outcome_summary: ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (status: "graduated" ‡∏´‡∏£‡∏∑‡∏≠ "not_graduated", confidence: 0-1, description: ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢)
- key_metrics: ‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 3 ‡∏ï‡∏±‡∏ß) ‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Killer Courses ‡∏ó‡∏µ‡πà‡∏ï‡∏Å
- recommendations: ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ (‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 3 ‡∏Ç‡πâ‡∏≠) ‡πÇ‡∏î‡∏¢‡πÄ‡∏ô‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Killer Courses
"""
        
        prompt_payload = {
            'student_name': student_name,
            'analysis_goal': analysis_goal_text,
            'grade_summary': grade_summary,
            'detailed_prompt': detailed_prompt  # ‡πÄ‡∏û‡∏¥‡πà‡∏° detailed_prompt
        }
        
        logger.info("üì§ Calling Gemini API for prediction...")
        try:
            # ‡πÉ‡∏ä‡πâ function ‡∏ó‡∏µ‡πà‡∏°‡∏µ retry ‡πÅ‡∏ó‡∏ô (‡∏•‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î 3 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á)
            gemini_output = call_gemini_with_retry(prompt_payload, 'live_grade_prediction')
            logger.info("‚úÖ Gemini prediction completed successfully")
        except (ValueError, RuntimeError) as gemini_error:
            # ‡∏ñ‡πâ‡∏≤ Gemini API ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß ‡πÉ‡∏´‡πâ‡∏™‡πà‡∏á error message ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
            logger.error(f"‚ùå Gemini API error: {gemini_error}")
            return jsonify({
                'success': False,
                'error': f'‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Gemini API ‡πÑ‡∏î‡πâ: {str(gemini_error)}',
                'details': str(gemini_error),
                'suggestion': '‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ GEMINI_API_KEY ‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á'
            }), 400
        
        return jsonify({
            'success': True,
            'source': 'live_grades',
            'analysis': grade_summary,
            'gemini': gemini_output,
            'training_analysis': training_analysis,
            'model_metadata': model_metadata
        })
    except Exception as exc:
        error_msg = str(exc)
        
        # Check for quota/rate limit errors
        if '429' in error_msg or 'quota' in error_msg.lower() or 'resource exhausted' in error_msg.lower():
            logger.error(f"‚ùå Gemini quota exceeded: {error_msg}")
            return jsonify({
                'success': False,
                'error': '‚ö†Ô∏è ‡πÇ‡∏Ñ‡∏ß‡∏ï‡πâ‡∏≤ Gemini API ‡∏´‡∏°‡∏î‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß',
                'details': 'Free tier: 15 requests/minute, 1,500 requests/day',
                'suggestion': '‡∏£‡∏≠ 1-2 ‡∏ô‡∏≤‡∏ó‡∏µ ‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏±‡∏û‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏õ‡πá‡∏ô Paid Tier',
                'retry_after': 60
            }), 429
        
        logger.error(f"‚ùå Gemini prediction error: {exc}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({
            'success': False, 
            'error': f'‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {error_msg}',
            'suggestion': '‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á'
        }), 500

@app.route('/upload', methods=['POST'])
def upload_file():
    """Handles file upload and basic data format detection."""
    try:
        logger.info("Starting file upload process")
        
        if 'file' not in request.files:
            logger.warning("No file in request")
            return jsonify({'success': False, 'error': '‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡∏Ñ‡∏≥‡∏Ç‡∏≠'})

        file = request.files['file']
        if file.filename == '':
            logger.warning("No file selected")
            return jsonify({'success': False, 'error': '‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå'})

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•‡πÑ‡∏ü‡∏•‡πå
        if not file.filename.lower().endswith(('.csv', '.xlsx', '.xls')):
            logger.warning(f"Invalid file extension")
            return jsonify({'success': False, 'error': '‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÑ‡∏ü‡∏•‡πå .csv, .xlsx, .xls ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô'})

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á filename ‡∏ó‡∏µ‡πà‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
        from werkzeug.utils import secure_filename
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ‡πÄ‡∏Å‡πá‡∏ö extension ‡∏Å‡πà‡∏≠‡∏ô ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ secure_filename ‡∏≠‡∏≤‡∏à‡∏•‡∏ö‡∏à‡∏∏‡∏î‡∏≠‡∏≠‡∏Å
        original_filename = file.filename
        file_ext = ''
        if '.' in original_filename:
            file_ext = '.' + original_filename.rsplit('.', 1)[1].lower()
        
        safe_filename = secure_filename(file.filename)
        # ‡∏ñ‡πâ‡∏≤ secure_filename ‡∏•‡∏ö extension ‡∏≠‡∏≠‡∏Å ‡πÉ‡∏´‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏•‡∏±‡∏ö
        if file_ext and not safe_filename.lower().endswith(file_ext):
            filename_on_disk = f"{timestamp}_{safe_filename}{file_ext}"
        else:
            filename_on_disk = f"{timestamp}_{safe_filename}"
        
        # ‡πÉ‡∏ä‡πâ absolute path
        upload_folder = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'uploads')
        if not os.path.exists(upload_folder):
            os.makedirs(upload_folder, exist_ok=True)
            
        filepath = os.path.join(upload_folder, filename_on_disk)
        
        logger.info(f"Saving file to: {filepath}")
        file.save(filepath)
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏à‡∏£‡∏¥‡∏á
        if not os.path.exists(filepath):
            raise ValueError("‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å")
            
        logger.info(f"File saved successfully: {filename_on_disk}")

        # ‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå
        try:
            df = None
            if safe_filename.lower().endswith('.csv'):
                # ‡∏•‡∏≠‡∏á‡∏´‡∏•‡∏≤‡∏¢ encoding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå CSV
                encodings = ['utf-8-sig', 'utf-8', 'cp874', 'tis-620', 'iso-8859-1']
                for encoding in encodings:
                    try:
                        df = pd.read_csv(filepath, encoding=encoding)
                        logger.info(f"Successfully read CSV with encoding: {encoding}")
                        break
                    except Exception as e:
                        logger.debug(f"Failed with {encoding}: {e}")
                        continue
                        
                if df is None:
                    # ‡∏ñ‡πâ‡∏≤‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ engine python
                    try:
                        df = pd.read_csv(filepath, encoding='utf-8', engine='python')
                    except:
                        df = pd.read_csv(filepath, encoding='cp874', engine='python')
                        
            else:  # Excel files
                df = pd.read_excel(filepath, engine='openpyxl')
                logger.info(f"Successfully read Excel file")

            if df is None or df.empty:
                os.remove(filepath)
                raise ValueError("‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏î‡πâ ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤")

            data_format = detect_data_format(df)
            logger.info(f"Detected data format: {data_format}")

            response_data = {
                'success': True,
                'filename': filename_on_disk,
                'rows': len(df),
                'columns': len(df.columns),
                'data_format': data_format,
                'sample_columns': df.columns.tolist()[:10]
            }
            
            logger.info(f"Upload successful, returning: {response_data}")
            return jsonify(response_data)

        except Exception as e:
            if os.path.exists(filepath):
                os.remove(filepath)
                logger.info(f"Removed invalid file: {filepath}")
            logger.error(f"Error processing file: {str(e)}")
            return jsonify({'success': False, 'error': f'‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏î‡πâ: {str(e)}'})

    except Exception as e:
        logger.error(f"Upload error: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': f'‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î: {str(e)}'})

# Keep all other routes unchanged...
@app.route('/analyze', methods=['POST'])
def analyze_subjects():
    """Analyzes subjects from a CSV/Excel file (for Subject-based data)."""
    try:
        data = request.get_json()
        filename = data.get('filename')

        if not filename:
            return jsonify({'success': False, 'error': 'No filename provided.'})

        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        if not os.path.exists(filepath):
            return jsonify({'success': False, 'error': 'Specified file not found.'})

        file_extension = filename.rsplit('.', 1)[1].lower()
        df = None
        if file_extension == 'csv':
            encodings = app.config['DATA_CONFIG']['fallback_encodings']
            for encoding in encodings:
                try:
                    df = pd.read_csv(filepath, encoding=encoding)
                    break
                except Exception as e:
                    logger.debug(f"Failed to read CSV with {encoding}: {e}")
                    continue
            if df is None:
                raise ValueError("Could not read CSV file with any supported encoding.")
        elif file_extension in ['xlsx', 'xls']:
            df = pd.read_excel(filepath)
        else:
            raise ValueError("Unsupported file type for analysis.")

        if df is None:
            return jsonify({'success': False, 'error': 'Could not read file.'})

        data_format = detect_data_format(df)

        if data_format != 'subject_based':
            return jsonify({'success': False, 'error': 'Only subject-based data is supported for this analysis.'})

        name_col = None
        possible_name_cols = ['‡∏ä‡∏∑‡πà‡∏≠-‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•', '‡∏ä‡∏∑‡πà‡∏≠', '‡∏ä‡∏∑‡πà‡∏≠‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤', 'name', 'student_name', '‡∏£‡∏´‡∏±‡∏™', 'student_id']
        for col in possible_name_cols:
            if col in df.columns:
                name_col = col
                break
        if not name_col:
            name_candidates = [col for col in df.columns if '‡∏ä‡∏∑‡πà‡∏≠' in col.lower() or '‡∏£‡∏´‡∏±‡∏™' in col.lower() or 'id' in col.lower()]
            if name_candidates:
                name_col = name_candidates[0]
            else:
                return jsonify({'success': False, 'error': 'Could not find student name or ID column.'})

        exclude_cols = [name_col, '‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤', '‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏à‡∏ö', 'year_in', 'year_out']
        exclude_keywords = ['gpa', '‡πÄ‡∏Å‡∏£‡∏î', '‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à', '‡∏à‡∏ö', 'success', 'graduated', 'status']
        subject_cols = [
            col for col in df.columns
            if col not in exclude_cols and not any(kw in col.lower() for kw in exclude_keywords)
        ]

        logger.info(f"Analyzing {len(subject_cols)} subjects.")

        subject_analysis = {}
        all_gpas = []
        all_fail_rates = []
        subject_stats = []

        for subject in subject_cols:
            grades = []
            for _, row in df.iterrows():
                grade = row[subject]
                if pd.notna(grade) and str(grade).strip():
                    numeric_grade = grade_to_numeric(grade)
                    grades.append(numeric_grade)

            if grades:
                category = categorize_subject(subject)
                if category not in subject_analysis:
                    subject_analysis[category] = {}

                avg_grade = np.mean(grades)
                min_grade = np.min(grades)
                max_grade = np.max(grades)
                std_grade = np.std(grades) if len(grades) > 1 else 0
                fail_count = sum(1 for g in grades if g <= app.config['DATA_CONFIG']['grade_mapping'].get('D', 1.0) - 0.01)
                fail_rate = fail_count / len(grades)
                pass_rate = 1 - fail_rate

                grade_distribution = {}
                fail_grade_chars = [k for k, v in app.config['DATA_CONFIG']['grade_mapping'].items() if v == 0.0]
                fail_char_for_display = 'F/W/I/NP' if 'F' in fail_grade_chars else '0.0'

                for grade_point_val in sorted(list(set(app.config['DATA_CONFIG']['grade_mapping'].values())), reverse=True):
                    count = sum(1 for g in grades if g == grade_point_val)
                    if count > 0:
                        if grade_point_val == 0.0:
                            grade_distribution[fail_char_for_display] = grade_distribution.get(fail_char_for_display, 0) + count
                        else:
                            char_grade = next((k for k, v in app.config['DATA_CONFIG']['grade_mapping'].items() if v == grade_point_val and k not in fail_grade_chars), str(grade_point_val))
                            grade_distribution[char_grade] = count

                subject_info = {
                    'average': avg_grade,
                    'minimum': min_grade,
                    'maximum': max_grade,
                    'std_dev': std_grade,
                    'fail_rate': fail_rate,
                    'pass_rate': pass_rate,
                    'num_students': len(grades),
                    'num_failed': fail_count,
                    'grade_distribution': grade_distribution
                }

                subject_analysis[category][subject] = subject_info
                subject_stats.append({
                    'subject': subject,
                    'category': category,
                    'average': avg_grade,
                    'fail_rate': fail_rate,
                    'num_students': len(grades)
                })

                all_gpas.append(avg_grade)
                all_fail_rates.append(fail_rate)

        overall_stats = {
            'total_students': len(df),
            'total_subjects': len(subject_cols),
            'avg_gpa': np.mean(all_gpas) if all_gpas else 0,
            'overall_fail_rate': np.mean(all_fail_rates) if all_fail_rates else 0,
            'max_gpa_subject': np.max(all_gpas) if all_gpas else 0,
            'min_gpa_subject': np.min(all_gpas) if all_gpas else 0
        }

        high_fail_subjects = []
        low_gpa_subjects = []
        excellent_subjects = []

        high_fail_rate_threshold = app.config['DATA_CONFIG']['risk_levels']['high_fail_rate_threshold']
        low_gpa_threshold = app.config['DATA_CONFIG']['risk_levels']['low_gpa_threshold']

        for stat in subject_stats:
            if stat['fail_rate'] > high_fail_rate_threshold:
                high_fail_subjects.append(stat)
            if stat['average'] < low_gpa_threshold:
                low_gpa_subjects.append(stat)
            if stat['average'] >= 3.5 and stat['fail_rate'] < 0.1:
                excellent_subjects.append(stat)

        high_fail_subjects.sort(key=lambda x: x['fail_rate'], reverse=True)
        low_gpa_subjects.sort(key=lambda x: x['average'])
        excellent_subjects.sort(key=lambda x: x['average'], reverse=True)

        recommendations = []
        overall_fail_rate_warning = app.config['DATA_CONFIG']['risk_levels']['medium_fail_rate_threshold']
        overall_fail_rate_high = app.config['DATA_CONFIG']['risk_levels']['high_fail_rate_threshold']

        if overall_stats['overall_fail_rate'] > overall_fail_rate_high:
            recommendations.append(f"‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡∏ï‡∏Å‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å (> {(overall_fail_rate_high * 100):.0f}%) ‡∏Ñ‡∏ß‡∏£‡∏ó‡∏ö‡∏ó‡∏ß‡∏ô‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î")
        elif overall_stats['overall_fail_rate'] > overall_fail_rate_warning:
            recommendations.append(f"‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡∏ï‡∏Å‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°‡∏Ñ‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡∏™‡∏π‡∏á (> {(overall_fail_rate_warning * 100):.0f}%) ‡∏Ñ‡∏ß‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ô")

        if high_fail_subjects:
            top_problem_subjects = [s['subject'] for s in high_fail_subjects[:3]]
            recommendations.append(f"‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏ï‡∏Å‡∏™‡∏π‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î: {', '.join(top_problem_subjects)}")
            recommendations.append("‡∏Ñ‡∏ß‡∏£‡∏ó‡∏ö‡∏ó‡∏ß‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ô ‡πÅ‡∏•‡∏∞‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÉ‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ")

        if low_gpa_subjects:
            recommendations.append(f"‡∏û‡∏ö {len(low_gpa_subjects)} ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ {low_gpa_threshold}")
            recommendations.append("‡∏Ñ‡∏ß‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏£‡∏±‡∏ö‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å‡∏Ç‡∏≠‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤")

        if excellent_subjects:
            recommendations.append(f"‡∏û‡∏ö {len(excellent_subjects)} ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏î‡∏µ‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏° ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ô‡∏≥‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ö‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á")

        category_summary = {}
        for category, subjects in subject_analysis.items():
            if subjects:
                avg_gpas = [s['average'] for s in subjects.values()]
                avg_fail_rates = [s['fail_rate'] for s in subjects.values()]
                total_students_in_category = sum(s['num_students'] for s in subjects.values())

                category_summary[category] = {
                    'num_subjects': len(subjects),
                    'avg_gpa': np.mean(avg_gpas),
                    'avg_fail_rate': np.mean(avg_fail_rates),
                    'total_students_in_category': total_students_in_category,
                    'hardest_subject': min(subjects.items(), key=lambda x: x[1]['average'])[0] if subjects else None,
                    'easiest_subject': max(subjects.items(), key=lambda x: x[1]['average'])[0] if subjects else None
                }

        logger.info("Subject analysis successful.")

        return jsonify({
            'success': True,
            'subject_analysis': subject_analysis,
            'overall_stats': overall_stats,
            'category_summary': category_summary,
            'problem_subjects': {
                'high_fail_rate': high_fail_subjects[:10],
                'low_gpa': low_gpa_subjects[:10]
            },
            'excellent_subjects': excellent_subjects[:10],
            'recommendations': recommendations
        })

    except Exception as e:
        logger.error(f"Error during subject analysis: {str(e)}")
        return jsonify({'success': False, 'error': f'An error occurred during analysis: {str(e)}'})



@app.route('/api/analyze_curriculum', methods=['POST'])
def analyze_curriculum():
    """
    ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤
    ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
    """
    try:
        data = request.get_json()
        current_grades = data.get('current_grades', {})
        loaded_terms_count = data.get('loaded_terms_count', 0)
        repeated_courses_in_this_term_ids = data.get('repeated_courses_in_this_term_ids', [])
        model_filename = data.get('model_filename')
        student_name = data.get('student_name', '‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤')

        logger.info(f"üîÆ Analyzing curriculum for {student_name} with {len(current_grades)} grades")
        logger.info(f"üìä Grade values sent for analysis: {list(current_grades.keys())[:10]}...")  # Log first 10 courses
        
        # Get configuration
        courses_data = app.config['COURSES_DATA']
        all_terms_data = app.config['ALL_TERMS_DATA']
        grade_mapping = app.config['DATA_CONFIG']['grade_mapping']
        
        # Get loaded courses
        loaded_courses_ids = []
        for i in range(loaded_terms_count):
            if i < len(all_terms_data):
                loaded_courses_ids.extend(all_terms_data[i]['ids'])
        loaded_courses_ids.extend(repeated_courses_in_this_term_ids)
        loaded_courses_ids = list(set(loaded_courses_ids))  # Remove duplicates
        
        # Calculate GPA and failed courses
        total_points = 0
        total_credits = 0
        completed_credits = 0
        failed_courses_ids = []
        incomplete_courses_ids = []
        
        for course_id in loaded_courses_ids:
            course = next((c for c in courses_data if c['id'] == course_id), None)
            if not course:
                continue
            
            grade = current_grades.get(course_id, '')
            
            if not grade:
                incomplete_courses_ids.append(course_id)
            elif grade:
                grade_point = grade_mapping.get(grade, 0)
                
                if grade_point > 0:
                    total_points += grade_point * course['credit']
                    total_credits += course['credit']
                    completed_credits += course['credit']
                elif grade == 'F':
                    failed_courses_ids.append(course_id)
                    total_credits += course['credit']  # F still counts for GPA calculation
        
        avg_gpa = total_points / total_credits if total_credits > 0 else 0
        
        # Check for blocked courses
        blocked_courses = find_all_blocked_courses(
            current_grades, loaded_courses_ids, courses_data, grade_mapping
        )
        
        # Build dependency graph and find impact chains
        dependency_graph = build_course_dependency_graph(loaded_courses_ids, courses_data)
        
        blocked_chain_texts = []
        for failed_id in failed_courses_ids:
            affected = find_courses_affected_by_failure(failed_id, dependency_graph)
            if affected:
                failed_course = next((c for c in courses_data if c['id'] == failed_id), None)
                affected_names = []
                for aid in affected:
                    course = next((c for c in courses_data if c['id'] == aid), None)
                    if course:
                        affected_names.append(f"{course['thaiName']} ({aid})")
                
                if affected_names:
                    chain_text = f"‡∏ß‡∏¥‡∏ä‡∏≤ {failed_course['thaiName']} ({failed_id}) ‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡πà‡∏≠‡∏ß‡∏¥‡∏ä‡∏≤‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:\n"
                    chain_text += "\n".join([f"  - {name}" for name in affected_names])
                    blocked_chain_texts.append(chain_text)
        
        # Calculate completion rate
        total_required_credits = sum(c['credit'] for c in courses_data)
        completion_rate = (completed_credits / total_required_credits * 100) if total_required_credits > 0 else 0
        
        # Determine graduation status
        graduation_status = determine_graduation_status_backend(
            completion_rate, avg_gpa, blocked_courses, failed_courses_ids,
            loaded_courses_ids, current_grades, all_terms_data, courses_data, loaded_terms_count
        )
        
        # Generate recommendations
        recommendations = []
        if failed_courses_ids:
            recommendations.append("‡∏Ñ‡∏ß‡∏£‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ã‡πâ‡∏≥‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏Å (F) ‡πÇ‡∏î‡∏¢‡πÄ‡∏£‡πá‡∏ß")
            recommendations.append("‡∏Ñ‡∏ß‡∏£‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPA ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (‡∏ï‡πâ‡∏≠‡∏á >= 2.00 ‡∏ñ‡∏∂‡∏á‡∏à‡∏∞‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå)
        gpa_threshold = 2.00
        if avg_gpa > 0 and avg_gpa < gpa_threshold:
            recommendations.append(f"GPA ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏ì‡∏ë‡πå ({avg_gpa:.2f} < {gpa_threshold:.2f}) ‡∏Ñ‡∏ß‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏£‡πà‡∏á‡∏î‡πà‡∏ß‡∏ô")
        
        if blocked_courses:
            recommendations.append("‡∏Ñ‡∏ß‡∏£‡∏ú‡πà‡∏≤‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡∏•‡∏á‡∏ß‡∏¥‡∏ä‡∏≤‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á")
        
        if not recommendations:
            recommendations.append("‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏î‡∏µ ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡πÉ‡∏´‡πâ‡∏à‡∏ö‡∏ï‡∏≤‡∏°‡∏Å‡∏≥‡∏´‡∏ô‡∏î")
        
        # Prepare response
        response_data = {
            'success': True,
            'student_name': student_name,
            'completion_rate': completion_rate,
            'avg_gpa': avg_gpa,
            'completed_credits': completed_credits,
            'total_required_credits': total_required_credits,
            'graduation_status': graduation_status,
            'incomplete_courses': [
                next((c['thaiName'] for c in courses_data if c['id'] == cid), cid)
                for cid in incomplete_courses_ids
            ],
            'failed_courses': [
                next((c['thaiName'] for c in courses_data if c['id'] == cid), cid)
                for cid in failed_courses_ids
            ],
            'blocked_courses_details': blocked_courses,
            'blocked_chain_texts': blocked_chain_texts,
            'recommendations': recommendations,
            'debug': {
                'current_grades_count': len(current_grades),
                'loaded_terms_count': loaded_terms_count,
                'loaded_courses_count': len(loaded_courses_ids),
                'blocked_courses_count': len(blocked_courses)
            }
        }
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
        def check_curriculum_completion(current_grades, courses_data, min_credits=136, min_gpa=2.0):
            """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏à‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á"""
            total_credits = 0
            total_grade_points = 0
            total_credits_for_gpa = 0
            passed_courses = 0
            
            for course_id, grade in current_grades.items():
                if grade:  # ‡∏°‡∏µ‡πÄ‡∏Å‡∏£‡∏î
                    course = next((c for c in courses_data if c['id'] == course_id), None)
                    if course:
                        credit = course.get('credit', 3)
                        grade_point = grade_mapping.get(grade, 0)
                        
                        # ‡∏ô‡∏±‡∏ö‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà F, W, I)
                        if grade not in ['F', 'W', 'I']:
                            total_credits += credit
                            passed_courses += 1
                        
                        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì GPA (‡∏£‡∏ß‡∏° F ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏° W, I)
                        if grade not in ['W', 'I']:
                            total_grade_points += grade_point * credit
                            total_credits_for_gpa += credit
            
            gpa = total_grade_points / total_credits_for_gpa if total_credits_for_gpa > 0 else 0
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏Å‡∏≤‡∏£‡∏à‡∏ö
            is_completed = total_credits >= min_credits and gpa >= min_gpa
            
            return {
                'is_completed': is_completed,
                'total_credits': total_credits,
                'gpa': gpa,
                'passed_courses': passed_courses,
                'min_credits_met': total_credits >= min_credits,
                'min_gpa_met': gpa >= min_gpa,
                'min_credits': min_credits,
                'min_gpa': min_gpa
            }
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£
        completion_status = check_curriculum_completion(current_grades, courses_data)
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÉ‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        response_data['completion_status'] = {
            'total_credits': completion_status['total_credits'],
            'min_credits': completion_status['min_credits'],
            'credits_remaining': max(0, completion_status['min_credits'] - completion_status['total_credits']),
            'gpa': completion_status['gpa'],
            'min_gpa': completion_status['min_gpa'],
            'is_completed': completion_status['is_completed'],
            'min_credits_met': completion_status['min_credits_met'],
            'min_gpa_met': completion_status['min_gpa_met']
        }
        
        # Try to predict if model is provided
        if model_filename:
            try:
                logger.info(f"Making prediction with model: {model_filename}")
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏à‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á
                if completion_status['is_completed']:
                    # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏£‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡πÅ‡∏•‡∏∞ GPA ‚â• 2.0 ‡πÅ‡∏•‡πâ‡∏ß ‚Üí ‡∏à‡∏ö‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô
                    response_data['prediction_result'] = {
                        'prediction': '‡∏à‡∏ö',
                        'prob_pass': 1.0,
                        'prob_fail': 0.0,
                        'confidence': 1.0,
                        'risk_level': '‡πÑ‡∏°‡πà‡∏°‡∏µ',
                        'gpa_input': float(completion_status['gpa']),
                        'reason': '‡πÉ‡∏™‡πà‡πÄ‡∏Å‡∏£‡∏î‡∏Ñ‡∏£‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡πÅ‡∏•‡∏∞ GPA ‚â• 2.00 ‡πÅ‡∏•‡πâ‡∏ß',
                        'method': 'Curriculum Completion Check',
                        'status': 'graduated'
                    }
                    logger.info(f"Curriculum completed: GPA {completion_status['gpa']:.2f}, Credits {completion_status['total_credits']}")
                    
                elif completion_status['min_credits_met'] and not completion_status['min_gpa_met']:
                    # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏£‡∏ö‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡πÅ‡∏ï‡πà GPA < 2.0 ‚Üí ‡πÑ‡∏°‡πà‡∏à‡∏ö
                    response_data['prediction_result'] = {
                        'prediction': '‡πÑ‡∏°‡πà‡∏à‡∏ö',
                        'prob_pass': 0.0,
                        'prob_fail': 1.0,
                        'confidence': 1.0,
                        'risk_level': '‡∏™‡∏π‡∏á',
                        'gpa_input': float(completion_status['gpa']),
                        'reason': f'GPA {completion_status["gpa"]:.2f} ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥ {completion_status["min_gpa"]:.2f}',
                        'method': 'Curriculum Completion Check',
                        'status': 'failed_gpa'
                    }
                    logger.info(f"Failed due to low GPA: {completion_status['gpa']:.2f} < {completion_status['min_gpa']}")
                    
                else:
                    # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£ ‚Üí ‡πÉ‡∏ä‡πâ AI ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (create_dynamic_features ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö training)
                    loaded_model_data = storage.load_model(model_filename)
                    if loaded_model_data:
                        models = loaded_model_data.get('models', {})
                        scaler = loaded_model_data.get('scaler', None)
                        feature_names = loaded_model_data.get('feature_columns', loaded_model_data.get('feature_names', []))
                        course_profiles = loaded_model_data.get('course_profiles', {})
                        course_credit_map = loaded_model_data.get('course_credit_map', {})

                        logger.info(f"üì¶ Model loaded: models={list(models.keys())}, features={len(feature_names)}, scaler={'‚úÖ' if scaler else '‚ùå'}")

                        # ‡∏™‡∏£‡πâ‡∏≤‡∏á grades_dict ‡∏à‡∏≤‡∏Å current_grades (‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏Å‡∏£‡∏î)
                        grades_dict = {cid: g for cid, g in current_grades.items() if g}

                        if grades_dict:
                            # ‡∏™‡∏£‡πâ‡∏≤‡∏á trainer instance ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö create_dynamic_features
                            trainer = AdvancedModelTrainer()
                            trainer.course_credit_map = course_credit_map

                            # ‡πÉ‡∏ä‡πâ loaded_terms_count ‡πÄ‡∏õ‡πá‡∏ô semester_number (fix #4)
                            semester_number = max(1, loaded_terms_count) if loaded_terms_count else None

                            # total_courses_taken = ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ô‡∏®.‡∏Å‡∏£‡∏≠‡∏Å‡∏°‡∏≤ (‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ï‡∏≠‡∏ô‡πÄ‡∏ó‡∏£‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ len(cumulative_grades))
                            total_courses_taken = len(grades_dict)

                            # ‡∏™‡∏£‡πâ‡∏≤‡∏á features ‡∏î‡πâ‡∏ß‡∏¢ create_dynamic_features ‚Äî ‡∏ï‡∏£‡∏á 100% ‡∏Å‡∏±‡∏ö‡∏ï‡∏≠‡∏ô‡πÄ‡∏ó‡∏£‡∏ô (fix #2)
                            features_array = trainer.create_dynamic_features(
                                grades_dict=grades_dict,
                                course_profiles=course_profiles,
                                total_courses_taken=total_courses_taken,
                                semester_number=semester_number
                            )

                            # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô DataFrame ‡∏î‡πâ‡∏ß‡∏¢ feature_names 28 ‡∏ä‡∏∑‡πà‡∏≠
                            X = pd.DataFrame([features_array], columns=feature_names)

                            # Scale features ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• (fix #3)
                            X_scaled = scaler.transform(X) if scaler else X

                            # Ensemble prediction ‚Äî ‡∏ß‡∏ô loop ‡∏ó‡∏∏‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
                            predictions = []
                            model_confidences = {}
                            model_icons = {'rf': 'üå≤', 'gb': 'üöÄ', 'lr': 'üìä', 'svm': 'üéØ'}

                            for model_name, model_obj in models.items():
                                try:
                                    pred = model_obj.predict_proba(X_scaled)[0][1]
                                    predictions.append(pred)
                                    model_confidences[model_name] = float(pred)
                                    icon = model_icons.get(model_name, 'ü§ñ')
                                    logger.info(f"{icon} {model_name} prediction: {pred:.3f}")
                                except Exception as e:
                                    logger.warning(f"‚ö†Ô∏è {model_name} prediction failed: {e}")

                            if predictions:
                                prob_pass = float(np.mean(predictions))
                                prob_fail = 1 - prob_pass

                                # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à: ‡∏¢‡∏¥‡πà‡∏á std ‡∏ô‡πâ‡∏≠‡∏¢ = ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏´‡πá‡∏ô‡∏û‡πâ‡∏≠‡∏á = ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏°‡∏≤‡∏Å
                                if len(predictions) > 1:
                                    prediction_std = float(np.std(predictions))
                                    confidence = max(0.5, min(0.95, 1.0 - prediction_std))
                                else:
                                    distance = abs(prob_pass - 0.5)
                                    confidence = max(0.5, min(0.95, 0.5 + distance))

                                prediction = '‡∏à‡∏ö' if prob_pass >= 0.5 else '‡πÑ‡∏°‡πà‡∏à‡∏ö'

                                risk_level = '‡∏™‡∏π‡∏á'
                                if confidence > 0.8:
                                    risk_level = '‡∏ï‡πà‡∏≥' if prediction == '‡∏à‡∏ö' else '‡∏™‡∏π‡∏á'
                                elif confidence > 0.6:
                                    risk_level = '‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á'

                                # Feature importance ‡∏à‡∏≤‡∏Å Random Forest
                                feature_importance = {}
                                if 'rf' in models:
                                    try:
                                        importances = models['rf'].feature_importances_
                                        importance_dict = dict(zip(feature_names, importances))
                                        feature_importance = dict(sorted(
                                            importance_dict.items(),
                                            key=lambda x: x[1],
                                            reverse=True
                                        )[:10])
                                    except Exception:
                                        pass

                                models_used = list(model_confidences.keys())

                                response_data['prediction_result'] = {
                                    'prediction': prediction,
                                    'prob_pass': prob_pass,
                                    'prob_fail': prob_fail,
                                    'confidence': float(confidence),
                                    'risk_level': risk_level,
                                    'gpa_input': float(completion_status['gpa']),
                                    'features_used': len(feature_names),
                                    'courses_analyzed': len(grades_dict),
                                    'method': 'AI_MODEL',
                                    'models_used': models_used,
                                    'model_confidence': model_confidences,
                                    'feature_importance': feature_importance,
                                    'status': 'predicted'
                                }
                                logger.info(f"ü§ñ AI Model Prediction: {prediction} (prob={prob_pass:.3f}, confidence={confidence:.3f}, models={models_used})")
                            else:
                                logger.warning("No model could make a prediction")
                        else:
                            logger.warning("No valid grades for prediction")
                    else:
                        logger.warning(f"Failed to load model: {model_filename}")

            except Exception as e:
                logger.error(f"Error during prediction: {str(e)}")
                import traceback
                logger.error(traceback.format_exc())

        # Fallback: rule-based prediction when no AI model prediction was made
        if 'prediction_result' not in response_data:
            rule_prob_pass = min(1.0, max(0.0, (avg_gpa - 1.0) / 2.0)) if avg_gpa > 0 else 0.5
            failed_penalty = len(failed_courses_ids) * 0.08
            rule_prob_pass = max(0.0, rule_prob_pass - failed_penalty)

            if completion_status['is_completed']:
                rule_prob_pass = 1.0
            elif completion_status['min_credits_met'] and not completion_status['min_gpa_met']:
                rule_prob_pass = 0.0

            rule_prediction = '‡∏à‡∏ö' if rule_prob_pass >= 0.5 else '‡πÑ‡∏°‡πà‡∏à‡∏ö'
            response_data['prediction_result'] = {
                'prediction': rule_prediction,
                'prob_pass': float(rule_prob_pass),
                'prob_fail': float(1 - rule_prob_pass),
                'confidence': 0.6,
                'risk_level': '‡∏ï‡πà‡∏≥' if rule_prob_pass >= 0.7 else ('‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á' if rule_prob_pass >= 0.4 else '‡∏™‡∏π‡∏á'),
                'gpa_input': float(avg_gpa),
                'method': 'Rule-based (‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏°‡πÄ‡∏î‡∏• AI)',
                'models_used': [],
                'feature_importance': {},
                'status': 'rule_based'
            }
            logger.info(f"üìä Rule-based fallback: {rule_prediction} (prob_pass={rule_prob_pass:.3f}, gpa={avg_gpa:.2f})")
        
        # ‚ú® ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏£‡∏≤‡∏ü 3 ‡πÄ‡∏™‡πâ‡∏ô (‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤)
        try:
            chart_data = generate_three_line_chart_data(current_grades, loaded_terms_count)
            response_data['chart_data'] = chart_data
            logger.info(f"Added chart data with {len(chart_data.get('terms', []))} terms")
        except Exception as e:
            logger.error(f"Error generating chart data: {str(e)}")
            response_data['chart_data'] = None
        
        # ‚ú® ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡∏à‡∏ö - ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• AI ‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢
        try:
            # ‡∏î‡∏∂‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• AI (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
            ai_prediction_result = response_data.get('prediction_result', None)
            
            graduation_analysis = analyze_graduation_failure_reasons(
                current_grades, 
                loaded_terms_count, 
                prediction_result=ai_prediction_result  # ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• AI
            )
            response_data['graduation_analysis'] = graduation_analysis
            
            # Log ‡∏ß‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡πÑ‡∏´‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
            prediction_method = graduation_analysis.get('prediction_method', 'Unknown')
            will_graduate = graduation_analysis.get('will_graduate', False)
            logger.info(f"‚úÖ Graduation analysis: will_graduate={will_graduate}, method={prediction_method}, risk={graduation_analysis.get('risk_level', 'N/A')}")
        except Exception as e:
            logger.error(f"Error in graduation analysis: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            response_data['graduation_analysis'] = None
        
        return jsonify(response_data)
        
    except Exception as e:
        logger.error(f"Error during curriculum analysis: {str(e)}")
        return jsonify({'success': False, 'error': str(e)})


@app.route('/api/explain_prediction', methods=['POST'])
def explain_prediction():
    """
    API ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
    ‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏ä‡∏¥‡∏á‡∏£‡∏∏‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á
    """
    try:
        data = request.get_json()
        current_grades = data.get('current_grades', {})
        loaded_terms_count = data.get('loaded_terms_count', 0)
        student_name = data.get('student_name', '‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤')
        
        logger.info(f"Explaining prediction for {student_name}")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ advanced_trainer ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if not hasattr(app, 'advanced_trainer') or not app.advanced_trainer:
            return jsonify({
                'success': False,
                'error': '‡πÇ‡∏°‡πÄ‡∏î‡∏• AI ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡πÇ‡∏´‡∏•‡∏î ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏≠‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á'
            })
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á student_data DataFrame
        courses_data = app.config.get('COURSES_DATA', [])
        grade_mapping = app.config.get('DATA_CONFIG', {}).get('grade_mapping', {})
        
        student_data_rows = []
        for course_id, grade in current_grades.items():
            if grade and grade in grade_mapping:
                course = next((c for c in courses_data if c['id'] == course_id), None)
                if course:
                    student_data_rows.append({
                        'course_code': course_id,
                        'grade': grade,
                        'grade_point': grade_mapping[grade],
                        'credit': course.get('credit', 3),
                        'semester': 1,
                        'academic_year': 2024
                    })
        
        if not student_data_rows:
            return jsonify({
                'success': False,
                'error': '‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏£‡∏î‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á'
            })
        
        student_data = pd.DataFrame(student_data_rows)
        
        # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ Context-Aware Predictor ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢
        try:
            if hasattr(app.advanced_trainer, 'predictor'):
                prediction_result = app.advanced_trainer.predictor.predict_graduation_probability(
                    student_data, explain=True
                )
            else:
                # Fallback: ‡∏™‡∏£‡πâ‡∏≤‡∏á predictor ‡πÉ‡∏´‡∏°‡πà
                predictor = ContextAwarePredictor(
                    feature_engineer=app.advanced_trainer.feature_engineer,
                    models=app.advanced_trainer.models if hasattr(app.advanced_trainer, 'models') else {},
                    scaler=app.advanced_trainer.scaler if hasattr(app.advanced_trainer, 'scaler') else None,
                    feature_names=app.advanced_trainer.feature_names if hasattr(app.advanced_trainer, 'feature_names') else []
                )
                prediction_result = predictor.predict_graduation_probability(student_data, explain=True)
        except Exception as e:
            logger.error(f"Error in prediction: {e}")
            return jsonify({
                'success': False,
                'error': f'‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢: {str(e)}'
            })
        
        # ‡∏î‡∏∂‡∏á‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢
        explanation = prediction_result.get('explanation')
        
        if not explanation:
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö fallback
            probability = prediction_result.get('probability', 0.5)
            will_graduate = probability >= 0.5
            
            explanation = {
                'summary': {
                    'will_graduate': will_graduate,
                    'probability': round(probability * 100, 1),
                    'confidence': round(prediction_result.get('confidence', 0.5) * 100, 1),
                    'status': '‡∏°‡∏µ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤' if will_graduate else '‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏à‡∏ö'
                },
                'key_factors': [],
                'reasons': ['‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ'],
                'obstacles': [],
                'strengths': [],
                'recommendations': [],
                'graduation_path': {},
                'future_projections': {}
            }
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°
        total_credits = sum([row['credit'] for row in student_data_rows])
        gpa = sum([grade_mapping[row['grade']] * row['credit'] for row in student_data_rows]) / total_credits if total_credits > 0 else 0
        failed_count = sum([1 for row in student_data_rows if grade_mapping[row['grade']] == 0])
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
        response_data = {
            'success': True,
            'student_name': student_name,
            'current_status': {
                'gpa': round(gpa, 2),
                'total_credits': total_credits,
                'courses_taken': len(student_data_rows),
                'failed_courses': failed_count,
                'terms_completed': loaded_terms_count
            },
            'prediction': {
                'probability': prediction_result.get('probability', 0.5),
                'confidence': prediction_result.get('confidence', 0.5),
                'will_graduate': prediction_result.get('probability', 0.5) >= 0.5
            },
            'explanation': explanation,
            'factors': prediction_result.get('factors', {}),
            'timestamp': datetime.now().isoformat()
        }
        
        logger.info(f"‚úÖ Successfully explained prediction for {student_name}")
        return jsonify(response_data)
        
    except Exception as e:
        logger.error(f"Error in explain_prediction: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({
            'success': False,
            'error': f'‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}'
        })
    
    
@app.route('/model_status', methods=['GET'])
def model_status():
    """Checks the status of currently loaded models."""
    try:
        logger.info("Checking model status...")
        
        global models
        if models is None:
            models = {
                'subject_model': None,
                'gpa_model': None,
                'subject_model_info': None,
                'gpa_model_info': None,
                'subject_feature_cols': None,
                'gpa_feature_cols': None
            }

        subject_model_status = models.get('subject_model') is not None
        subject_info = models.get('subject_model_info')
        
        logger.info(f"Subject model status: {subject_model_status}")

        status = {
            'success': True,
            'subject_model': subject_model_status,
            'gpa_model': False,  # Always false since we're removing GPA model
            'subject_model_info': subject_info,
            'gpa_model_info': None,  # No GPA model info
            'server_time': datetime.now().isoformat(),
            'models_folder_exists': os.path.exists(app.config['MODEL_FOLDER']),
            'uploads_folder_exists': os.path.exists(app.config['UPLOAD_FOLDER']),
            's3_available': not storage.use_local
        }
        
        logger.info("Model status check completed successfully")
        return jsonify(status)

    except Exception as e:
        logger.error(f"Error in model_status: {str(e)}")
        return jsonify({
            'success': False,
            'subject_model': False,
            'gpa_model': False,
            'error': str(e),
            'server_time': datetime.now().isoformat()
        }), 500



@app.route('/api/sync-models', methods=['POST'])
def sync_local_models_to_storage():
    """Sync local models to cloud storage"""
    try:
        if storage.use_local:
            return jsonify({'success': False, 'error': 'Storage is in local mode'})
        
        synced = []
        model_folder = app.config['MODEL_FOLDER']
        
        if os.path.exists(model_folder):
            for filename in os.listdir(model_folder):
                if filename.endswith('.joblib'):
                    filepath = os.path.join(model_folder, filename)
                    try:
                        model_data = joblib.load(filepath)
                        if storage.save_model(model_data, filename):
                            synced.append(filename)
                            logger.info(f"Synced {filename} to storage")
                    except Exception as e:
                        logger.error(f"Could not sync {filename}: {e}")
        
        return jsonify({'success': True, 'synced': synced})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

# ===============================
# API Endpoints ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏≤‡∏ü 3 ‡πÄ‡∏™‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á
# ===============================

@app.route('/api/three-line-chart', methods=['POST'])
def get_three_line_chart_data():
    """API ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏£‡∏≤‡∏ü 3 ‡πÄ‡∏™‡πâ‡∏ô"""
    try:
        data = request.get_json()
        current_grades = data.get('current_grades', {})
        loaded_terms_count = data.get('loaded_terms_count', 8)
        
        chart_data = generate_three_line_chart_data(current_grades, loaded_terms_count)
        
        return jsonify({
            'success': True,
            'chart_data': chart_data
        })
        
    except Exception as e:
        logger.error(f"Error generating three line chart data: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/graduation-analysis', methods=['POST'])
def get_graduation_analysis():
    """API ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤"""
    try:
        data = request.get_json()
        current_grades = data.get('current_grades', {})
        loaded_terms_count = data.get('loaded_terms_count', 8)
        
        analysis_result = analyze_graduation_failure_reasons(current_grades, loaded_terms_count)
        
        return jsonify({
            'success': True,
            'analysis': analysis_result
        })
        
    except Exception as e:
        logger.error(f"Error analyzing graduation failure reasons: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/next-term-prediction', methods=['POST'])
def get_next_term_prediction():
    """API ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏ó‡∏≠‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ"""
    try:
        data = request.get_json()
        current_grades = data.get('current_grades', {})
        
        prediction_table = generate_next_term_grade_prediction_table(current_grades)
        
        return jsonify({
            'success': True,
            'prediction_table': prediction_table
        })
        
    except Exception as e:
        logger.error(f"Error generating next term prediction: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/comprehensive-analysis', methods=['POST'])
def get_comprehensive_analysis():
    """API ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"""
    try:
        data = request.get_json()
        current_grades = data.get('current_grades', {})
        loaded_terms_count = data.get('loaded_terms_count', 8)
        student_name = data.get('student_name', '‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤')
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏£‡∏≤‡∏ü 3 ‡πÄ‡∏™‡πâ‡∏ô
        chart_data = generate_three_line_chart_data(current_grades, loaded_terms_count)
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏ö
        graduation_analysis = analyze_graduation_failure_reasons(current_grades, loaded_terms_count)
        
        # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏ó‡∏≠‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
        next_term_prediction = generate_next_term_grade_prediction_table(current_grades)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏£‡∏ß‡∏°
        comprehensive_summary = {
            'student_name': student_name,
            'analysis_date': datetime.now().isoformat(),
            'current_status': {
                'gpa': graduation_analysis['current_gpa'],
                'total_courses': graduation_analysis['total_courses'],
                'failed_courses': graduation_analysis['failed_courses_count'],
                'risk_level': graduation_analysis['risk_level'],
                'graduation_probability': graduation_analysis['graduation_probability']
            },
            'predictions': {
                'next_term_gpa': next_term_prediction['predicted_term_gpa'],
                'cumulative_gpa': next_term_prediction['predicted_cumulative_gpa'],
                'improvement': next_term_prediction['improvement']
            },
            'key_insights': [
                f"GPA ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô: {graduation_analysis['current_gpa']:.2f}",
                f"‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á: {graduation_analysis['risk_level']}",
                f"‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏à‡∏ö: {graduation_analysis['graduation_probability']:.1f}%",
                f"‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ GPA ‡πÄ‡∏ó‡∏≠‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ: {next_term_prediction['predicted_term_gpa']:.2f}"
            ]
        }
        
        return jsonify({
            'success': True,
            'chart_data': chart_data,
            'graduation_analysis': graduation_analysis,
            'next_term_prediction': next_term_prediction,
            'comprehensive_summary': comprehensive_summary
        })
        
    except Exception as e:
        logger.error(f"Error generating comprehensive analysis: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500
    
# Keep all other routes unchanged...
@app.route('/page')
def main_page():
    return render_template('main_page.html')

# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç route /curriculum  
@app.route('/curriculum')
def curriculum_page():
    """Page for curriculum analysis."""
    try:
        courses_json = json.dumps(app.config.get('COURSES_DATA', []))
        terms_json = json.dumps(app.config.get('ALL_TERMS_DATA', []))
        grades_json = json.dumps(app.config.get('DATA_CONFIG', {}).get('grade_mapping', {}))
        
        return render_template(
            'curriculum_prediction_form.html',
            coursesData=courses_json,
            allTermsData=terms_json,
            gradeMapping=grades_json
        )
    except Exception as e:
        logger.error(f"Error rendering curriculum page: {str(e)}")
        return render_template(
            'curriculum_prediction_form.html',
            coursesData='[]',
            allTermsData='[]',
            gradeMapping='{}'
        )

@app.route('/advanced')
def advanced_test_page():
    """Page for advanced curriculum analysis."""
    return render_template('advanced_test.html')

# =========================================================
# BATCH PREDICTION API
# =========================================================

@app.route('/api/predict_batch', methods=['POST'])
def api_predict_batch():
    """API for batch prediction from uploaded CSV/Excel file."""
    try:
        logger.info("=== Starting batch prediction ===")

        if 'file' not in request.files:
            return jsonify({'success': False, 'error': 'No file uploaded.'})

        file = request.files['file']
        model_filename = request.form.get('model_filename', '')

        if not file or file.filename == '':
            return jsonify({'success': False, 'error': 'No file selected.'})

        # --- Load model ---
        if not model_filename:
            models_list = storage.list_models()
            subject_models = [m for m in models_list if _is_subject_model(m)]
            if subject_models:
                model_filename = subject_models[0]['filename']
            else:
                return jsonify({'success': False, 'error': 'No trained model found. Please train a model first.'})

        model_data = storage.load_model(model_filename)
        if not model_data:
            return jsonify({'success': False, 'error': f'Failed to load model: {model_filename}'})

        subject_model = model_data.get('models', {}).get('rf')
        scaler = model_data.get('scaler')
        feature_columns = model_data.get('feature_columns')
        course_profiles = model_data.get('course_profiles', {})
        grade_mapping = app.config['DATA_CONFIG']['grade_mapping']

        if not all([subject_model, scaler, feature_columns]):
            return jsonify({'success': False, 'error': 'Incomplete model data. Missing model, scaler, or feature columns.'})

        # --- Read uploaded file ---
        filename = file.filename
        ext = filename.rsplit('.', 1)[-1].lower() if '.' in filename else ''

        try:
            if ext == 'csv':
                raw_bytes = file.read()
                for enc in ['utf-8-sig', 'utf-8', 'cp874', 'iso-8859-1']:
                    try:
                        from io import StringIO
                        df = pd.read_csv(StringIO(raw_bytes.decode(enc)))
                        break
                    except (UnicodeDecodeError, Exception):
                        continue
                else:
                    return jsonify({'success': False, 'error': 'Cannot decode CSV file. Please use UTF-8 encoding.'})
            elif ext in ['xlsx', 'xls']:
                df = pd.read_excel(BytesIO(file.read()))
            else:
                return jsonify({'success': False, 'error': f'Unsupported file type: .{ext}. Use CSV or Excel.'})
        except Exception as e:
            return jsonify({'success': False, 'error': f'Error reading file: {str(e)}'})

        if df.empty:
            return jsonify({'success': False, 'error': 'File is empty.'})

        logger.info(f"Read file: {len(df)} rows, columns: {list(df.columns)}")

        # --- Detect format: Wide vs Long ---
        cols_lower = {c: c.lower().strip() for c in df.columns}
        detected_format = _detect_batch_format(df, cols_lower)
        logger.info(f"Detected format: {detected_format}")

        if detected_format == 'unknown':
            return jsonify({
                'success': False,
                'error': 'Cannot detect file format. Please use Wide or Long format.',
                'detected_columns': list(df.columns)[:20],
                'hint': 'Wide format: STUDENT_ID + course codes as column headers. Long format: STUDENT_ID, COURSE_CODE, GRADE columns.'
            })

        # --- Parse students based on format ---
        courses_data = app.config.get('COURSES_DATA', [])
        course_credit_map = {c['id']: c['credit'] for c in courses_data}
        course_name_map = {c['id']: c['thaiName'] for c in courses_data}
        all_terms = app.config.get('ALL_TERMS_DATA', [])

        if detected_format == 'wide':
            students = _parse_wide_format(df, cols_lower, grade_mapping, course_credit_map)
        else:
            students = _parse_long_format(df, cols_lower, grade_mapping, course_credit_map)

        if not students:
            return jsonify({'success': False, 'error': 'No valid student data found in the file.'})

        logger.info(f"Parsed {len(students)} students")

        # --- Setup trainer (same as individual prediction) ---
        trainer = AdvancedModelTrainer()
        trainer.course_credit_map = course_credit_map
        # Also try to get course_credit_map from model data (same as individual prediction)
        model_credit_map = model_data.get('course_credit_map', {})
        if model_credit_map:
            trainer.course_credit_map = model_credit_map

        # --- Predict for each student ---
        results = []
        errors = []

        for student in students:
            try:
                student_id = student['student_id']
                student_name = student.get('student_name', '')
                grades_dict = student['grades']  # {course_id: grade_str}

                if not grades_dict:
                    errors.append({'student_id': student_id, 'error': 'No grade data found'})
                    continue

                # Use create_dynamic_features ‚Äî exactly the same as training
                total_courses_taken = len(grades_dict)
                loaded_terms_count = student.get('loaded_terms_count')
                semester_number = max(1, loaded_terms_count) if loaded_terms_count else None
                retake_info = student.get('retake_info')  # from long format parser
                cumulative_retakes = retake_info.get('num_retake_courses', 0) if retake_info else 0

                features_array = trainer.create_dynamic_features(
                    grades_dict=grades_dict,
                    course_profiles=course_profiles,
                    total_courses_taken=total_courses_taken,
                    semester_number=semester_number,
                    retake_info=retake_info,
                    cumulative_retakes=cumulative_retakes
                )

                # Convert to DataFrame with correct feature column names (same as individual prediction)
                X_pred = pd.DataFrame([features_array], columns=feature_columns)

                # Scale and predict
                X_scaled = scaler.transform(X_pred)
                pred_proba = subject_model.predict_proba(X_scaled)[0]
                prob_graduate = float(pred_proba[1]) if len(pred_proba) > 1 else float(pred_proba[0])
                prediction = prob_graduate >= 0.5

                # --- Calculate stats ---
                valid_grades = [grade_mapping.get(str(g).upper().strip(), None) for g in grades_dict.values()]
                valid_grades = [g for g in valid_grades if g is not None]

                total_credits = sum(course_credit_map.get(cid, 3) for cid in grades_dict.keys())
                weighted_sum = sum(
                    grade_mapping.get(str(g).upper().strip(), 0) * course_credit_map.get(cid, 3)
                    for cid, g in grades_dict.items()
                    if grade_mapping.get(str(g).upper().strip()) is not None
                        and str(g).upper().strip() not in ['W', 'I', 'S', 'U', 'AU', 'P', 'NP']
                )
                gpa_credits = sum(
                    course_credit_map.get(cid, 3)
                    for cid, g in grades_dict.items()
                    if str(g).upper().strip() not in ['W', 'I', 'S', 'U', 'AU', 'P', 'NP']
                )
                gpa = weighted_sum / gpa_credits if gpa_credits > 0 else 0.0

                failed_grades = ['F', 'WF', 'U', 'NP']
                failed_count = sum(1 for g in grades_dict.values() if str(g).upper().strip() in failed_grades)
                total_courses = len(grades_dict)

                # --- Build recommendation & reason ---
                confidence = max(prob_graduate, 1 - prob_graduate)
                risk_level, recommendation, reasons = _build_batch_recommendation(
                    prediction, prob_graduate, gpa, failed_count, total_courses,
                    total_credits, grades_dict, grade_mapping, course_name_map,
                    course_credit_map, all_terms
                )

                results.append({
                    'student_id': student_id,
                    'student_name': student_name,
                    'gpa': round(gpa, 2),
                    'total_credits': total_credits,
                    'total_courses': total_courses,
                    'failed_count': failed_count,
                    'probability': round(prob_graduate, 4),
                    'prediction': '‡∏ú‡πà‡∏≤‡∏ô' if prediction else '‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô',
                    'confidence': round(confidence, 4),
                    'risk_level': risk_level,
                    'recommendation': recommendation,
                    'reasons': reasons,
                    'loaded_terms_count': loaded_terms_count or 0,
                    'terms_detail': student.get('terms_detail', []),
                    'retake_info': retake_info or {},
                    'grades': grades_dict  # ‡∏™‡πà‡∏á grades ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö Gemini AI
                })

            except Exception as e:
                logger.warning(f"Error predicting for student {student.get('student_id', '?')}: {e}")
                errors.append({'student_id': student.get('student_id', '?'), 'error': str(e)})

        if not results:
            return jsonify({'success': False, 'error': 'Could not predict for any student.', 'errors': errors})

        # --- Build summary ---
        pass_count = sum(1 for r in results if r['prediction'] == '‡∏ú‡πà‡∏≤‡∏ô')
        fail_count_total = len(results) - pass_count
        gpas = [r['gpa'] for r in results]
        probs = [r['probability'] for r in results]

        summary = {
            'total_students': len(results),
            'pass_count': pass_count,
            'pass_percentage': round(pass_count / len(results) * 100, 1),
            'fail_count': fail_count_total,
            'fail_percentage': round(fail_count_total / len(results) * 100, 1),
            'avg_gpa': round(sum(gpas) / len(gpas), 2),
            'min_gpa': round(min(gpas), 2),
            'max_gpa': round(max(gpas), 2),
            'avg_probability': round(sum(probs) / len(probs), 4),
            'risk_distribution': {
                'high': sum(1 for r in results if r['risk_level'] == '‡∏™‡∏π‡∏á'),
                'medium': sum(1 for r in results if r['risk_level'] == '‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á'),
                'low': sum(1 for r in results if r['risk_level'] == '‡∏ï‡πà‡∏≥')
            },
            'probability_distribution': _calc_prob_distribution([r['probability'] for r in results])
        }

        logger.info(f"=== Batch prediction complete: {len(results)} students, {pass_count} pass, {fail_count_total} fail ===")

        return jsonify({
            'success': True,
            'detected_format': 'Wide Format' if detected_format == 'wide' else 'Long Format',
            'results': results,
            'summary': summary,
            'errors': errors
        })

    except Exception as e:
        logger.error(f"Batch prediction error: {str(e)}", exc_info=True)
        return jsonify({'success': False, 'error': f'Batch prediction error: {str(e)}'})


# =========================================================
# BATCH GEMINI AI ANALYSIS (per-student, same as individual)
# =========================================================

@app.route('/api/batch_gemini_analyze', methods=['POST'])
def api_batch_gemini_analyze():
    """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏£‡∏≤‡∏¢‡∏Ñ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Gemini AI ‚Äî ‡πÉ‡∏ä‡πâ prompt ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß"""
    if not is_gemini_available():
        return jsonify({'success': False, 'error': '‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Gemini API Key'}), 503

    can_proceed, wait_time = gemini_rate_limiter.can_proceed()
    if not can_proceed:
        return jsonify({
            'success': False,
            'error': f'‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏≠ {wait_time} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ‡∏Å‡πà‡∏≠‡∏ô‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ',
            'retry_after': wait_time
        }), 429

    try:
        payload = request.get_json(silent=True) or {}
        grades_dict = payload.get('grades', {})
        student_id = payload.get('student_id', '')
        student_name = payload.get('student_name', '‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤')
        loaded_terms_count = int(payload.get('loaded_terms_count') or 0)
        terms_detail = payload.get('terms_detail', [])
        retake_info = payload.get('retake_info', {})
        model_filename = payload.get('model_filename', '')
        prob_graduate = payload.get('probability', 0)
        prediction_label = payload.get('prediction', '')
        confidence_val = payload.get('confidence', 0)
        risk_level_val = payload.get('risk_level', '')

        if not grades_dict:
            return jsonify({'success': False, 'error': '‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏£‡∏î'}), 400

        # Load course profiles from model (same as individual prediction)
        course_profiles = {}
        training_analysis = None
        if model_filename:
            try:
                stored_model = storage.load_model(model_filename)
                if stored_model:
                    course_profiles = stored_model.get('course_profiles', {})
                    training_analysis = stored_model.get('gemini_training_analysis')
            except Exception:
                pass

        # Build Course DNA context (same as individual prediction)
        course_context_str = ""
        if course_profiles:
            student_risk_factors = []
            student_easy_courses_failed = []

            for course_id, grade in grades_dict.items():
                if course_id in course_profiles:
                    prof = course_profiles[course_id]
                    fail_rate = prof.get('fail_rate', 0)
                    avg_grade = prof.get('avg_grade', 0)
                    difficulty_score = prof.get('difficulty_score', 0)

                    is_killer = fail_rate >= 0.3
                    is_failed = str(grade).upper() in ['F', 'W', 'WF', 'WU', 'D']

                    if is_killer and is_failed:
                        student_risk_factors.append(
                            f"- ‡∏ï‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤‡∏õ‡∏£‡∏≤‡∏ö‡πÄ‡∏ã‡∏µ‡∏¢‡∏ô {course_id} (‡∏õ‡∏Å‡∏ï‡∏¥‡∏Ñ‡∏ô‡∏ï‡∏Å {fail_rate*100:.0f}%, ‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢: {avg_grade:.2f})"
                        )
                    elif is_killer and not is_failed:
                        student_risk_factors.append(
                            f"- ‡∏ú‡πà‡∏≤‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏õ‡∏£‡∏≤‡∏ö‡πÄ‡∏ã‡∏µ‡∏¢‡∏ô {course_id} ‡πÅ‡∏•‡πâ‡∏ß (‡∏õ‡∏Å‡∏ï‡∏¥‡∏Ñ‡∏ô‡∏ï‡∏Å {fail_rate*100:.0f}%) ‚úì"
                        )

                    is_easy = avg_grade >= 3.5 and fail_rate <= 0.1
                    is_poor_grade = str(grade).upper() in ['C', 'D', 'F', 'W', 'WF', 'WU']

                    if is_easy and is_poor_grade:
                        student_easy_courses_failed.append(
                            f"- ‡πÑ‡∏î‡πâ‡πÄ‡∏Å‡∏£‡∏î‡∏ô‡πâ‡∏≠‡∏¢‡πÉ‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ä‡πà‡∏ß‡∏¢ {course_id} (‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏õ‡∏Å‡∏ï‡∏¥: {avg_grade:.2f}, ‡πÑ‡∏î‡πâ: {grade})"
                        )

            if student_risk_factors or student_easy_courses_failed:
                course_context_str = "\n**‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ï‡∏≤‡∏° Course DNA (Backend System):**\n"
                if student_risk_factors:
                    course_context_str += "\n".join(student_risk_factors)
                if student_easy_courses_failed:
                    course_context_str += "\n" + "\n".join(student_easy_courses_failed)
                course_context_str += "\n"

        # Summarize grades (same function as individual prediction)
        grade_summary = summarize_grades_for_gemini(grades_dict, loaded_terms_count)
        grade_summary['student_name'] = student_name

        # Build model prediction context
        model_context_str = f"""
**‡∏ú‡∏•‡∏à‡∏≤‡∏Å AI Model ‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß:**
- ‡∏ú‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢: {prediction_label}
- ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏à‡∏ö: {prob_graduate * 100:.1f}%
- ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à: {confidence_val * 100:.1f}%
- ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á: {risk_level_val}
- ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ: Random Forest
"""

        # Build detailed prompt (SAME as individual prediction)
        detailed_prompt = f"""
‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏ó‡∏≤‡∏á‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£ ‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà**‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢**‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏Ç‡∏≠‡∏á‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤

**‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤:**
- ‡∏£‡∏´‡∏±‡∏™: {student_id}
- ‡∏ä‡∏∑‡πà‡∏≠: {student_name}
- ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÅ‡∏•‡πâ‡∏ß: {grade_summary.get('total_courses', 0)} ‡∏ß‡∏¥‡∏ä‡∏≤
- ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ó‡∏≠‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô: {loaded_terms_count} ‡πÄ‡∏ó‡∏≠‡∏°
- ‡πÄ‡∏ó‡∏≠‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {', '.join(terms_detail) if terms_detail else '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏'}
- ‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ (‡πÇ‡∏î‡∏¢‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì): {grade_summary.get('estimated_gpa', 0):.2f}
- ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÅ‡∏•‡πâ‡∏ß: {grade_summary.get('total_credits_recorded', 0)} ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï
- ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏ö‡∏ï‡∏Å: {grade_summary.get('failed_count', 0)} ‡∏ß‡∏¥‡∏ä‡∏≤
- ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏•‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ã‡πâ‡∏≥: {retake_info.get('num_retake_courses', 0)} ‡∏ß‡∏¥‡∏ä‡∏≤ (‡∏ï‡∏Å F ‡πÅ‡∏•‡πâ‡∏ß‡∏ã‡πâ‡∏≥: {retake_info.get('retake_f_count', 0)}, ‡∏ñ‡∏≠‡∏ô W ‡πÅ‡∏•‡πâ‡∏ß‡∏ã‡πâ‡∏≥: {retake_info.get('retake_w_count', 0)})
- ‡πÄ‡∏Å‡∏£‡∏î‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ã‡πâ‡∏≥: {retake_info.get('avg_retake_improvement', 0):.2f} ‡∏à‡∏∏‡∏î

**‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÄ‡∏Å‡∏£‡∏î:**
{json.dumps(grade_summary.get('grade_distribution', {}), ensure_ascii=False, indent=2)}

**‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ß‡∏¥‡∏ä‡∏≤:**
{json.dumps(grade_summary.get('course_details', [])[:20], ensure_ascii=False, indent=2)}

**‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏ö‡∏ï‡∏Å (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ):**
{json.dumps(grade_summary.get('failed_courses', []), ensure_ascii=False, indent=2)}

{course_context_str}
(‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≤‡∏á‡∏ö‡∏ô‡∏Ñ‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏£‡∏∏‡πà‡∏ô‡∏û‡∏µ‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß)

{model_context_str}

**‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì (‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î):**

**1. ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤ (‡∏ï‡πâ‡∏≠‡∏á‡∏ü‡∏±‡∏ô‡∏ò‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô):**
   - ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ß‡πà‡∏≤‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏Ñ‡∏ô‡∏ô‡∏µ‡πâ "‡∏à‡∏∞‡∏à‡∏ö‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå" ‡∏´‡∏£‡∏∑‡∏≠ "‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏à‡∏ö/‡∏à‡∏ö‡∏ä‡πâ‡∏≤"
   - ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏à‡∏ö: GPA >= 2.00 ‡πÅ‡∏•‡∏∞‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï >= 136 ‡πÅ‡∏•‡∏∞‡∏ú‡πà‡∏≤‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö/‡∏ß‡∏¥‡∏ä‡∏≤‡∏õ‡∏£‡∏≤‡∏ö‡πÄ‡∏ã‡∏µ‡∏¢‡∏ô
   - ‡∏£‡∏∞‡∏ö‡∏∏‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå (0-100%)

**2. ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÑ‡∏°‡∏ñ‡∏∂‡∏á‡∏à‡∏ö/‡πÑ‡∏°‡πà‡∏à‡∏ö:**
   - ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
   - ‡∏£‡∏∞‡∏ö‡∏∏‡∏õ‡∏±‡∏à‡∏à‡∏±‡∏¢‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏à‡∏ö/‡πÑ‡∏°‡πà‡∏à‡∏ö

**3. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å:**
   - ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏õ‡∏±‡∏à‡∏à‡∏±‡∏¢‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏ú‡∏•‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
   - ‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö "‡∏ß‡∏¥‡∏ä‡∏≤‡∏õ‡∏£‡∏≤‡∏ö‡πÄ‡∏ã‡∏µ‡∏¢‡∏ô" (Killer Courses) ‡∏ó‡∏µ‡πà‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ï‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å
   - ‡∏ñ‡πâ‡∏≤‡πÑ‡∏î‡πâ‡πÄ‡∏Å‡∏£‡∏î‡πÅ‡∏¢‡πà‡πÉ‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏á‡πà‡∏≤‡∏¢ (Easy Courses) ‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô

**4. ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:**
   - ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡πÉ‡∏ô‡πÄ‡∏ó‡∏≠‡∏°‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏≠‡∏î
   - ‡∏ñ‡πâ‡∏≤‡∏ï‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤ Killer ‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏°‡∏Ç‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡∏ò‡∏£‡∏£‡∏°
   - ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏à‡∏ö‡πÇ‡∏î‡∏¢‡∏î‡∏π‡∏ß‡πà‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡∏î‡πà‡∏≤‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏¢‡∏≤‡∏Å‡πÜ ‡πÑ‡∏õ‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á

**‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:**
- **‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏à‡∏ö/‡πÑ‡∏°‡πà‡∏à‡∏ö‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô**
- **‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢**
- ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏ú‡∏•‡∏à‡∏≤‡∏Å AI Model ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö
- ‡∏ï‡∏≠‡∏ö‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö JSON ‡∏ï‡∏≤‡∏° Schema ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î

‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î ‡πÇ‡∏î‡∏¢‡πÉ‡∏´‡πâ:
- graduation_prediction:
  * will_graduate: true/false
  * prediction_text: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
  * reason_why_graduate: ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÑ‡∏°‡∏ñ‡∏∂‡∏á‡∏à‡∏ö (‡∏ñ‡πâ‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ß‡πà‡∏≤‡∏à‡∏ö)
  * reason_why_not_graduate: ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÑ‡∏°‡∏ñ‡∏∂‡∏á‡πÑ‡∏°‡πà‡∏à‡∏ö (‡∏ñ‡πâ‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏à‡∏ö)
  * confidence_percent: ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à (0-100)
- analysis_markdown: ‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡πÇ‡∏î‡∏¢‡πÄ‡∏ô‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•
- risk_level: ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á (very_low, low, moderate, high, very_high)
- outcome_summary: ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (status, confidence, description)
- key_metrics: ‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 3 ‡∏ï‡∏±‡∏ß)
- recommendations: ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ (‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 3 ‡∏Ç‡πâ‡∏≠)
"""

        prompt_payload = {
            'student_name': student_name,
            'analysis_goal': '‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÅ‡∏•‡∏∞‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î',
            'grade_summary': grade_summary,
            'detailed_prompt': detailed_prompt
        }

        logger.info(f"üîÆ Batch Gemini analysis for student {student_id}")
        gemini_output = call_gemini_with_retry(prompt_payload, 'live_grade_prediction')

        return jsonify({
            'success': True,
            'student_id': student_id,
            'gemini': gemini_output
        })

    except Exception as exc:
        error_msg = str(exc)
        if '429' in error_msg or 'quota' in error_msg.lower():
            return jsonify({
                'success': False,
                'error': '‡πÇ‡∏Ñ‡∏ß‡∏ï‡πâ‡∏≤ Gemini API ‡∏´‡∏°‡∏î‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏≠ 1-2 ‡∏ô‡∏≤‡∏ó‡∏µ',
                'retry_after': 60
            }), 429
        logger.error(f"Batch Gemini analysis error: {exc}", exc_info=True)
        return jsonify({'success': False, 'error': f'‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {error_msg}'}), 500


def _detect_batch_format(df, cols_lower):
    """Detect if uploaded file is Wide or Long format."""
    col_names_lower = set(cols_lower.values())

    # Long format: has STUDENT_ID + COURSE_CODE + GRADE columns
    has_student_id = any(c in col_names_lower for c in ['student_id', 'studentid', 'id', '‡∏£‡∏´‡∏±‡∏™‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤', '‡∏£‡∏´‡∏±‡∏™', 'student_no'])
    has_course_code = any(c in col_names_lower for c in ['course_code', 'coursecode', 'course_id', '‡∏£‡∏´‡∏±‡∏™‡∏ß‡∏¥‡∏ä‡∏≤', 'subject_code'])
    has_grade = any(c in col_names_lower for c in ['grade', '‡πÄ‡∏Å‡∏£‡∏î', 'letter_grade'])

    if has_student_id and has_course_code and has_grade:
        return 'long'

    # Wide format: has STUDENT_ID and columns that look like course codes (XX-XXX-XXX-XXX)
    course_code_pattern = re.compile(r'^\d{2}-\d{3}-\d{3}-\d{3}$')
    course_cols = [c for c in df.columns if course_code_pattern.match(str(c).strip())]

    if has_student_id and len(course_cols) >= 3:
        return 'wide'

    # Fallback: check if any column headers match known course IDs
    courses_data = app.config.get('COURSES_DATA', [])
    known_ids = {c['id'] for c in courses_data}
    matching_cols = [c for c in df.columns if str(c).strip() in known_ids]
    if has_student_id and len(matching_cols) >= 3:
        return 'wide'

    return 'unknown'


def _find_col(cols_lower, candidates):
    """Find the original column name that matches any of the candidate lower-case names."""
    for orig, lower in cols_lower.items():
        if lower in candidates:
            return orig
    return None


def _parse_wide_format(df, cols_lower, grade_mapping, course_credit_map):
    """Parse Wide format: STUDENT_ID, STUDENT_NAME, GPA, [course_code_1], [course_code_2], ..."""
    students = []

    id_col = _find_col(cols_lower, ['student_id', 'studentid', 'id', '‡∏£‡∏´‡∏±‡∏™‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤', '‡∏£‡∏´‡∏±‡∏™', 'student_no'])
    name_col = _find_col(cols_lower, ['student_name', 'studentname', 'name', '‡∏ä‡∏∑‡πà‡∏≠', '‡∏ä‡∏∑‡πà‡∏≠‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤', '‡∏ä‡∏∑‡πà‡∏≠-‡∏™‡∏Å‡∏∏‡∏•'])

    if not id_col:
        return []

    # Course columns = any column that looks like a course code or is a known course
    course_code_pattern = re.compile(r'^\d{2}-\d{3}-\d{3}-\d{3}$')
    courses_data = app.config.get('COURSES_DATA', [])
    known_ids = {c['id'] for c in courses_data}

    course_cols = []
    for c in df.columns:
        c_stripped = str(c).strip()
        if course_code_pattern.match(c_stripped) or c_stripped in known_ids:
            course_cols.append(c)

    valid_grades = set(grade_mapping.keys())

    for _, row in df.iterrows():
        student_id = str(row.get(id_col, '')).strip()
        if not student_id or student_id == 'nan':
            continue

        student_name = str(row.get(name_col, '')).strip() if name_col else ''
        if student_name == 'nan':
            student_name = ''

        grades = {}
        for col in course_cols:
            val = str(row.get(col, '')).strip().upper()
            if val and val != 'NAN' and val != '' and val != '-':
                if val in valid_grades:
                    grades[str(col).strip()] = val

        if grades:
            # Determine loaded_terms_count and term details from which courses have grades
            all_terms = app.config.get('ALL_TERMS_DATA', [])
            terms_with_grades = set()
            terms_detail = []
            for term_idx, term_info in enumerate(all_terms):
                term_courses_with_grade = [cid for cid in term_info['ids'] if cid in grades]
                if term_courses_with_grade:
                    terms_with_grades.add(term_idx)
                    terms_detail.append(f"‡∏õ‡∏µ{term_info['year']}‡πÄ‡∏ó‡∏≠‡∏°{term_info['term']}({len(term_courses_with_grade)}‡∏ß‡∏¥‡∏ä‡∏≤)")
            loaded_terms_count = len(terms_with_grades) if terms_with_grades else None

            students.append({
                'student_id': student_id,
                'student_name': student_name,
                'grades': grades,
                'loaded_terms_count': loaded_terms_count,
                'terms_detail': terms_detail
            })

    return students


def _parse_long_format(df, cols_lower, grade_mapping, course_credit_map):
    """Parse Long format: STUDENT_ID, COURSE_CODE, GRADE, [CREDIT], [STUDENT_NAME], [YEAR], [TERM]"""
    id_col = _find_col(cols_lower, ['student_id', 'studentid', 'id', '‡∏£‡∏´‡∏±‡∏™‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤', '‡∏£‡∏´‡∏±‡∏™', 'student_no'])
    course_col = _find_col(cols_lower, ['course_code', 'coursecode', 'course_id', '‡∏£‡∏´‡∏±‡∏™‡∏ß‡∏¥‡∏ä‡∏≤', 'subject_code'])
    grade_col = _find_col(cols_lower, ['grade', '‡πÄ‡∏Å‡∏£‡∏î', 'letter_grade'])
    name_col = _find_col(cols_lower, ['student_name', 'studentname', 'name', '‡∏ä‡∏∑‡πà‡∏≠', '‡∏ä‡∏∑‡πà‡∏≠‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤'])
    year_col = _find_col(cols_lower, ['year', '‡∏õ‡∏µ', '‡∏õ‡∏µ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤', 'academic_year'])
    term_col = _find_col(cols_lower, ['term', '‡πÄ‡∏ó‡∏≠‡∏°', 'semester', '‡∏†‡∏≤‡∏Ñ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô'])

    if not id_col or not course_col or not grade_col:
        return []

    valid_grades = set(grade_mapping.keys())
    students_dict = {}

    for _, row in df.iterrows():
        sid = str(row.get(id_col, '')).strip()
        cid = str(row.get(course_col, '')).strip()
        grade = str(row.get(grade_col, '')).strip().upper()

        if not sid or sid == 'nan' or not cid or not grade:
            continue
        if grade not in valid_grades:
            continue

        if sid not in students_dict:
            sname = str(row.get(name_col, '')).strip() if name_col else ''
            if sname == 'nan':
                sname = ''
            students_dict[sid] = {
                'student_id': sid, 'student_name': sname,
                'grades': {}, 'terms_set': set(),
                'all_attempts': {}  # {course_id: [grade1, grade2, ...]} ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö detect retake
            }

        # Track ALL attempts per course (for retake detection)
        if cid not in students_dict[sid]['all_attempts']:
            students_dict[sid]['all_attempts'][cid] = []
        students_dict[sid]['all_attempts'][cid].append(grade)

        # Keep latest grade as the final grade
        students_dict[sid]['grades'][cid] = grade

        # Track unique (year, term) pairs to calculate loaded_terms_count
        if year_col and term_col:
            yr = str(row.get(year_col, '')).strip()
            tm = str(row.get(term_col, '')).strip()
            if yr and yr != 'nan' and tm and tm != 'nan':
                students_dict[sid]['terms_set'].add((yr, tm))

    # Convert to result list with retake_info
    result = []
    for sid, data in students_dict.items():
        terms_set = data.get('terms_set', set())
        loaded_terms_count = len(terms_set)
        terms_detail = []
        if terms_set:
            sorted_terms = sorted(terms_set, key=lambda x: (x[0], x[1]))
            for yr, tm in sorted_terms:
                terms_detail.append(f"‡∏û.‡∏®.{yr}‡πÄ‡∏ó‡∏≠‡∏°{tm}")

        # Build retake_info from all_attempts (same format as training)
        retake_info = _build_retake_info(data['all_attempts'], grade_mapping)

        result.append({
            'student_id': data['student_id'],
            'student_name': data['student_name'],
            'grades': data['grades'],
            'loaded_terms_count': loaded_terms_count if loaded_terms_count > 0 else None,
            'terms_detail': terms_detail,
            'retake_info': retake_info
        })

    return result


def _build_retake_info(all_attempts, grade_mapping):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á retake_info ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ã‡πâ‡∏≥ (same format as training)

    Parameters:
        all_attempts: dict {course_id: [grade1, grade2, ...]}
        grade_mapping: dict {grade_letter: grade_point}

    Returns:
        dict with keys: num_retake_courses, retake_f_count, retake_w_count, avg_retake_improvement
    """
    num_retake_courses = 0
    retake_f_count = 0
    retake_w_count = 0
    retake_improvements = []

    for cid, grades_list in all_attempts.items():
        if len(grades_list) <= 1:
            continue  # ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà retake

        num_retake_courses += 1
        first_grade = grades_list[0]
        last_grade = grades_list[-1]

        if first_grade == 'F':
            retake_f_count += 1
        if first_grade == 'W':
            retake_w_count += 1

        # Calculate improvement (last - first grade point)
        first_gp = grade_mapping.get(first_grade)
        last_gp = grade_mapping.get(last_grade)
        if first_gp is not None and last_gp is not None:
            retake_improvements.append(last_gp - first_gp)

    return {
        'num_retake_courses': num_retake_courses,
        'retake_f_count': retake_f_count,
        'retake_w_count': retake_w_count,
        'avg_retake_improvement': float(np.mean(retake_improvements)) if retake_improvements else 0.0,
    }


def _build_batch_recommendation(prediction, prob_graduate, gpa, failed_count, total_courses,
                                 total_credits, grades_dict, grade_mapping, course_name_map,
                                 course_credit_map, all_terms):
    """Build risk level, recommendation text, and reasons list for a student."""
    reasons = []
    recommendations = []

    # --- Risk level ---
    if prob_graduate >= 0.8:
        risk_level = '‡∏ï‡πà‡∏≥'
    elif prob_graduate >= 0.5:
        risk_level = '‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á'
    else:
        risk_level = '‡∏™‡∏π‡∏á'

    # --- GPA analysis ---
    if gpa < 1.5:
        reasons.append(f'GPA ‡∏ï‡πà‡∏≥‡∏°‡∏≤‡∏Å ({gpa:.2f}) ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏à‡∏ö 2.00 ‡∏°‡∏≤‡∏Å')
        recommendations.append('‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏£‡πà‡∏á‡∏î‡πà‡∏ß‡∏ô')
    elif gpa < 2.0:
        reasons.append(f'GPA ({gpa:.2f}) ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤ (2.00)')
        recommendations.append('‡∏Ñ‡∏ß‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏° GPA ‡πÉ‡∏´‡πâ‡∏ñ‡∏∂‡∏á 2.00 ‡πÇ‡∏î‡∏¢‡πÄ‡∏£‡πá‡∏ß')
    elif gpa < 2.5:
        reasons.append(f'GPA ({gpa:.2f}) ‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡πÄ‡∏ù‡πâ‡∏≤‡∏£‡∏∞‡∏ß‡∏±‡∏á')
        recommendations.append('‡∏Ñ‡∏ß‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÉ‡∏´‡πâ‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡∏´‡∏£‡∏∑‡∏≠‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô')
    elif gpa >= 3.0:
        reasons.append(f'GPA ({gpa:.2f}) ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏î‡∏µ')
    else:
        reasons.append(f'GPA ({gpa:.2f}) ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á')

    # --- Failed courses ---
    if failed_count > 0:
        failed_courses = []
        for cid, g in grades_dict.items():
            if str(g).upper().strip() in ['F', 'WF', 'U', 'NP']:
                cname = course_name_map.get(cid, cid)
                failed_courses.append(cname)

        reasons.append(f'‡∏™‡∏≠‡∏ö‡∏ï‡∏Å {failed_count} ‡∏ß‡∏¥‡∏ä‡∏≤: {", ".join(failed_courses[:5])}')
        if failed_count >= 3:
            recommendations.append(f'‡∏ï‡πâ‡∏≠‡∏á‡∏•‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ã‡πâ‡∏≥ {failed_count} ‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏Å')
        else:
            recommendations.append(f'‡∏Ñ‡∏ß‡∏£‡∏•‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ã‡πâ‡∏≥‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏Å')

    # --- Credit progress ---
    required_credits = 136
    progress_pct = (total_credits / required_credits) * 100
    if total_credits < required_credits:
        reasons.append(f'‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏™‡∏∞‡∏™‡∏° {total_credits}/{required_credits} ({progress_pct:.0f}%)')
        remaining = required_credits - total_credits
        if remaining > 60:
            recommendations.append(f'‡∏¢‡∏±‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏≠‡∏µ‡∏Å {remaining} ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï ‡∏ï‡πâ‡∏≠‡∏á‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡πÉ‡∏´‡πâ‡∏î‡∏µ')
    else:
        reasons.append(f'‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï‡∏™‡∏∞‡∏™‡∏°‡∏Ñ‡∏£‡∏ö {total_credits}/{required_credits}')

    # --- W (Withdraw) courses ---
    w_count = sum(1 for g in grades_dict.values() if str(g).upper().strip() in ['W', 'I'])
    if w_count >= 3:
        reasons.append(f'‡∏ñ‡∏≠‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤ (W/I) {w_count} ‡∏ß‡∏¥‡∏ä‡∏≤ ‡∏ã‡∏∂‡πà‡∏á‡∏≠‡∏≤‡∏à‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÑ‡∏°‡πà‡∏ó‡∏±‡∏ô‡∏à‡∏ö')
        recommendations.append('‡∏Ñ‡∏ß‡∏£‡∏•‡∏î‡∏Å‡∏≤‡∏£‡∏ñ‡∏≠‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡πÅ‡∏•‡∏∞‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤')

    # --- Low grade in core courses ---
    core_ids = app.config.get('CORE_SUBJECTS_IDS', [])
    weak_core = []
    for cid in core_ids:
        if cid in grades_dict:
            g = str(grades_dict[cid]).upper().strip()
            gv = grade_mapping.get(g, None)
            if gv is not None and gv < 2.0:
                cname = course_name_map.get(cid, cid)
                weak_core.append(f'{cname}({g})')
    if weak_core:
        reasons.append(f'‡πÄ‡∏Å‡∏£‡∏î‡∏ï‡πà‡∏≥‡πÉ‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡πÅ‡∏Å‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: {", ".join(weak_core[:3])}')
        recommendations.append('‡∏Ñ‡∏ß‡∏£‡πÄ‡∏ô‡πâ‡∏ô‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏ß‡∏¥‡∏ä‡∏≤‡πÅ‡∏Å‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏£‡∏î‡∏ï‡πà‡∏≥')

    # --- Final recommendation text ---
    if prediction:
        main_rec = '‡∏°‡∏µ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÑ‡∏î‡πâ'
        if recommendations:
            main_rec += ' ‡πÅ‡∏ï‡πà' + recommendations[0]
    else:
        if recommendations:
            main_rec = recommendations[0]
        else:
            main_rec = '‡∏Ñ‡∏ß‡∏£‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô'

    return risk_level, main_rec, reasons


def _calc_prob_distribution(probs):
    """Calculate probability distribution for batch summary."""
    bins = [
        {'range': '0-20%', 'min': 0, 'max': 0.2, 'count': 0},
        {'range': '21-40%', 'min': 0.2, 'max': 0.4, 'count': 0},
        {'range': '41-60%', 'min': 0.4, 'max': 0.6, 'count': 0},
        {'range': '61-80%', 'min': 0.6, 'max': 0.8, 'count': 0},
        {'range': '81-100%', 'min': 0.8, 'max': 1.01, 'count': 0},
    ]
    for p in probs:
        for b in bins:
            if b['min'] <= p < b['max']:
                b['count'] += 1
                break
    return [{'range': b['range'], 'count': b['count']} for b in bins]


# =========================================================
# BATCH TEMPLATE DOWNLOAD API
# =========================================================

@app.route('/api/batch_template/<fmt>')
def api_batch_template(fmt):
    """Download CSV template for batch prediction (wide or long format)."""
    try:
        courses_data = app.config.get('COURSES_DATA', [])
        all_terms = app.config.get('ALL_TERMS_DATA', [])

        if fmt == 'wide':
            return _generate_wide_template(courses_data, all_terms)
        elif fmt == 'long':
            return _generate_long_template(courses_data, all_terms)
        else:
            return jsonify({'success': False, 'error': 'Invalid format. Use "wide" or "long".'}), 400
    except Exception as e:
        logger.error(f"Error generating template: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500


def _generate_wide_template(courses_data, all_terms):
    """Generate Wide format CSV template with course codes as column headers."""
    from io import StringIO

    output = StringIO()

    # Collect all course IDs in curriculum order
    ordered_ids = []
    for term_info in all_terms:
        for cid in term_info['ids']:
            if cid not in ordered_ids:
                ordered_ids.append(cid)

    # Build course lookup
    course_lookup = {c['id']: c for c in courses_data}

    # --- Build course-to-term mapping ---
    course_term_map = {}
    for t in all_terms:
        for cid in t['ids']:
            course_term_map[cid] = f"‡∏õ‡∏µ{t['year']}‡πÄ‡∏ó‡∏≠‡∏°{t['term']}"

    # --- Header row: STUDENT_ID, STUDENT_NAME, then course IDs ---
    headers = ['STUDENT_ID', 'STUDENT_NAME'] + ordered_ids
    output.write(','.join(headers) + '\n')

    # --- Row 2: TERM label row (clearly shows year/term for each column) ---
    term_row = ['# ‡∏õ‡∏µ/‡πÄ‡∏ó‡∏≠‡∏°', '#']
    for cid in ordered_ids:
        term_row.append(course_term_map.get(cid, ''))
    output.write(','.join(term_row) + '\n')

    # --- Row 3: Course names (Thai) with credits ---
    comment_row = ['# ‡∏£‡∏´‡∏±‡∏™‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤', '# ‡∏ä‡∏∑‡πà‡∏≠‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤']
    for cid in ordered_ids:
        c = course_lookup.get(cid)
        if c:
            comment_row.append(f'# {c["thaiName"]} ({c["credit"]}‡∏´‡∏ô‡πà‡∏ß‡∏¢)')
        else:
            comment_row.append(f'# {cid}')
    output.write(','.join(comment_row) + '\n')

    # --- Row 4: Guide row ---
    guide_row = ['# ‡πÄ‡∏Å‡∏£‡∏î‡∏ó‡∏µ‡πà‡πÉ‡∏™‡πà‡πÑ‡∏î‡πâ: A / B+ / B / C+ / C / D+ / D / F / W / I', '# ‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡πà‡∏≤‡∏á‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏•‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô']
    guide_row += [''] * len(ordered_ids)
    output.write(','.join(guide_row) + '\n')

    # --- Row 5: Retake + term guide ---
    retake_row = ['# ‡∏ñ‡πâ‡∏≤‡∏•‡∏á‡∏ã‡πâ‡∏≥‡πÉ‡∏™‡πà‡πÄ‡∏Å‡∏£‡∏î‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î | ‡∏£‡∏∞‡∏ö‡∏ö‡∏£‡∏π‡πâ‡πÄ‡∏ó‡∏≠‡∏°‡∏à‡∏≤‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏Å‡∏£‡∏≠‡∏Å', '# ‡∏Å‡∏£‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏ó‡∏≠‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÅ‡∏•‡πâ‡∏ß']
    retake_row += [''] * len(ordered_ids)
    output.write(','.join(retake_row) + '\n')

    # --- Build cumulative course index ranges per term ---
    term_ranges = []  # [(start_idx, end_idx, year, term), ...]
    idx_offset = 0
    for t in all_terms:
        n = len(t['ids'])
        term_ranges.append((idx_offset, idx_offset + n, t['year'], t['term']))
        idx_offset += n

    # ============================================================
    # 5 ‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏à‡∏≥‡∏•‡∏≠‡∏á ‚Äî ‡∏Ñ‡∏•‡∏∞‡πÄ‡∏Å‡∏£‡∏î ‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏à‡∏£‡∏¥‡∏á
    # Wide format: ‡πÉ‡∏™‡πà‡πÄ‡∏Å‡∏£‡∏î‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î (‡∏ñ‡πâ‡∏≤‡∏•‡∏á‡∏ã‡πâ‡∏≥ ‡πÉ‡∏™‡πà‡πÄ‡∏Å‡∏£‡∏î‡∏´‡∏•‡∏±‡∏á‡∏ã‡πâ‡∏≥)
    # ============================================================

    # === ‡∏ô‡∏®.1: ‡∏Å‡∏¥‡∏ï‡∏ï‡∏¥‡∏û‡∏á‡∏®‡πå ‚Äî ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ñ‡∏∂‡∏á ‡∏õ‡∏µ3‡πÄ‡∏ó‡∏≠‡∏°2 (6 ‡πÄ‡∏ó‡∏≠‡∏°, 40 ‡∏ß‡∏¥‡∏ä‡∏≤) ===
    # ‡πÄ‡∏Å‡πà‡∏á‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°/‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì ‡πÅ‡∏ï‡πà‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á, GPA ~3.1, ‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡∏•‡∏á‡∏ã‡πâ‡∏≥
    grades_s1 = [
        'B+','B','C+','A','A','A','A','B+',           # ‡∏õ‡∏µ1‡πÄ‡∏ó‡∏≠‡∏°1: GE ‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á, ‡πÅ‡∏Ñ‡∏•‡∏Ø/‡∏ü‡∏¥‡∏™‡∏¥‡∏Å‡∏™‡πå/‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏î‡∏µ
        'B','B+','B','B+','B+','A','A','A',            # ‡∏õ‡∏µ1‡πÄ‡∏ó‡∏≠‡∏°2: ‡∏î‡∏µ‡∏™‡∏°‡πà‡∏≥‡πÄ‡∏™‡∏°‡∏≠
        'A','B+','A','B','B+',                          # ‡∏õ‡∏µ2‡πÄ‡∏ó‡∏≠‡∏°1: ‡πÄ‡∏≠‡∏Å‡πÄ‡∏î‡πà‡∏ô
        'C+','B+','B','A','A','A','A',                 # ‡∏õ‡∏µ2‡πÄ‡∏ó‡∏≠‡∏°2: GE ‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á ‡∏ß‡∏¥‡∏ä‡∏≤‡πÄ‡∏≠‡∏Å‡∏î‡∏µ
        'C+','B+','B+','A','A','B+',                   # ‡∏õ‡∏µ3‡πÄ‡∏ó‡∏≠‡∏°1: ‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á ‡πÄ‡∏≠‡∏Å‡∏î‡∏µ
        'B','B+','B+','A','B+','A',                    # ‡∏õ‡∏µ3‡πÄ‡∏ó‡∏≠‡∏°2: ‡∏î‡∏µ
    ]
    ex1 = ['66010001', '‡∏ô‡∏≤‡∏¢ ‡∏Å‡∏¥‡∏ï‡∏ï‡∏¥‡∏û‡∏á‡∏®‡πå ‡∏ß‡∏á‡∏®‡πå‡∏™‡∏∏‡∏ß‡∏£‡∏£‡∏ì']
    for i, cid in enumerate(ordered_ids):
        ex1.append(grades_s1[i] if i < len(grades_s1) else '')
    output.write(','.join(ex1) + '\n')

    # === ‡∏ô‡∏®.2: ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏ä‡∏ô‡∏Å ‚Äî ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ñ‡∏∂‡∏á ‡∏õ‡∏µ2‡πÄ‡∏ó‡∏≠‡∏°1 (3 ‡πÄ‡∏ó‡∏≠‡∏°, 21 ‡∏ß‡∏¥‡∏ä‡∏≤) ===
    # GE ‡∏î‡∏µ ‡πÅ‡∏ï‡πà‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì/‡∏ü‡∏¥‡∏™‡∏¥‡∏Å‡∏™‡πå‡πÅ‡∏¢‡πà ‡∏ï‡∏Å‡πÅ‡∏Ñ‡∏•‡∏Ø2 ‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏á‡∏ã‡πâ‡∏≥, GPA ~2.2
    # Wide format: ‡πÅ‡∏Ñ‡∏•‡∏Ø2 ‡πÉ‡∏™‡πà D+ (‡πÄ‡∏Å‡∏£‡∏î‡∏´‡∏•‡∏±‡∏á‡∏•‡∏á‡∏ã‡πâ‡∏≥)
    grades_s2 = [
        'B+','B','B+','D+','C','D','C+','C',          # ‡∏õ‡∏µ1‡πÄ‡∏ó‡∏≠‡∏°1: GE ‡∏î‡∏µ, ‡πÅ‡∏Ñ‡∏•‡∏Ø D+, ‡∏ü‡∏¥‡∏™‡∏¥‡∏Å‡∏™‡πå‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á
        'B','B+','A','D+','D+','D','C','C+',           # ‡∏õ‡∏µ1‡πÄ‡∏ó‡∏≠‡∏°2: ‡πÅ‡∏Ñ‡∏•‡∏Ø2=D+(‡∏´‡∏•‡∏±‡∏á‡∏ã‡πâ‡∏≥), ‡∏ü‡∏¥‡∏™‡∏¥‡∏Å‡∏™‡πå2‡πÅ‡∏¢‡πà
        'C','C+','C','D+','C',                          # ‡∏õ‡∏µ2‡πÄ‡∏ó‡∏≠‡∏°1: ‡∏ß‡∏¥‡∏ä‡∏≤‡πÄ‡∏≠‡∏Å‡∏¢‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
    ]
    ex2 = ['65010045', '‡∏ô‡∏≤‡∏á‡∏™‡∏≤‡∏ß ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏ä‡∏ô‡∏Å ‡∏®‡∏£‡∏µ‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå']
    for i, cid in enumerate(ordered_ids):
        ex2.append(grades_s2[i] if i < len(grades_s2) else '')
    output.write(','.join(ex2) + '\n')

    # === ‡∏ô‡∏®.3: ‡∏ò‡∏ô‡∏≤‡∏Å‡∏£ ‚Äî ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ñ‡∏∂‡∏á ‡∏õ‡∏µ3‡πÄ‡∏ó‡∏≠‡∏°1 (5 ‡πÄ‡∏ó‡∏≠‡∏°, 34 ‡∏ß‡∏¥‡∏ä‡∏≤) ===
    # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÅ‡∏¢‡πà‡πÅ‡∏ï‡πà‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢‡πÜ, ‡∏ï‡∏Å‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°+‡∏ñ‡∏≠‡∏ô Workshop ‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏á‡∏ã‡πâ‡∏≥, GPA ~2.4
    # Wide format: ‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°=C+(‡∏´‡∏•‡∏±‡∏á‡∏ã‡πâ‡∏≥), Workshop=C(‡∏´‡∏•‡∏±‡∏á‡∏ã‡πâ‡∏≥)
    grades_s3 = [
        'C','C+','D+','D','C','D+','C+','C',          # ‡∏õ‡∏µ1‡πÄ‡∏ó‡∏≠‡∏°1: ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÑ‡∏°‡πà‡∏ó‡∏±‡∏ô (‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°=C+‡∏´‡∏•‡∏±‡∏á‡∏ã‡πâ‡∏≥, Workshop=C‡∏´‡∏•‡∏±‡∏á‡∏ã‡πâ‡∏≥)
        'C+','C','C+','D+','C+','C','C','D+',          # ‡∏õ‡∏µ1‡πÄ‡∏ó‡∏≠‡∏°2: ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
        'C+','B','C+','C','C',                          # ‡∏õ‡∏µ2‡πÄ‡∏ó‡∏≠‡∏°1: ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏µ‡∏Å
        'B','B','C+','C','B','B','B+',                 # ‡∏õ‡∏µ2‡πÄ‡∏ó‡∏≠‡∏°2: ‡πÄ‡∏Å‡∏£‡∏î‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡∏Å!
        'C+','B','B','B+','B+','C+',                   # ‡∏õ‡∏µ3‡πÄ‡∏ó‡∏≠‡∏°1: ‡∏î‡∏µ‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á
    ]
    ex3 = ['66010112', '‡∏ô‡∏≤‡∏¢ ‡∏ò‡∏ô‡∏≤‡∏Å‡∏£ ‡πÅ‡∏™‡∏á‡∏≠‡∏£‡∏∏‡∏ì']
    for i, cid in enumerate(ordered_ids):
        ex3.append(grades_s3[i] if i < len(grades_s3) else '')
    output.write(','.join(ex3) + '\n')

    # === ‡∏ô‡∏®.4: ‡∏™‡∏∏‡∏†‡∏≤‡∏û‡∏£ ‚Äî ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ñ‡∏∂‡∏á ‡∏õ‡∏µ1‡πÄ‡∏ó‡∏≠‡∏°2 (2 ‡πÄ‡∏ó‡∏≠‡∏°, 16 ‡∏ß‡∏¥‡∏ä‡∏≤) ===
    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏î‡∏µ‡∏™‡∏°‡πà‡∏≥‡πÄ‡∏™‡∏°‡∏≠ ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÑ‡∏î‡πâ‡πÄ‡∏£‡πá‡∏ß GPA ~3.4 ‡πÑ‡∏°‡πà‡∏°‡∏µ F/W
    grades_s4 = [
        'A','A','B+','B+','B','B+','A','B+',          # ‡∏õ‡∏µ1‡πÄ‡∏ó‡∏≠‡∏°1: ‡∏î‡∏µ‡∏ó‡∏∏‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤
        'B+','A','A','B','B+','B','A','B+',            # ‡∏õ‡∏µ1‡πÄ‡∏ó‡∏≠‡∏°2: ‡∏î‡∏µ‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á ‡πÅ‡∏Ñ‡∏•‡∏Ø2=B
    ]
    ex4 = ['66010078', '‡∏ô‡∏≤‡∏á‡∏™‡∏≤‡∏ß ‡∏™‡∏∏‡∏†‡∏≤‡∏û‡∏£ ‡∏à‡∏±‡∏ô‡∏ó‡∏£‡πå‡πÄ‡∏û‡πá‡∏ç']
    for i, cid in enumerate(ordered_ids):
        ex4.append(grades_s4[i] if i < len(grades_s4) else '')
    output.write(','.join(ex4) + '\n')

    # === ‡∏ô‡∏®.5: ‡∏ß‡∏£‡∏ß‡∏¥‡∏ó‡∏¢‡πå ‚Äî ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ñ‡∏∂‡∏á ‡∏õ‡∏µ4‡πÄ‡∏ó‡∏≠‡∏°1 (7 ‡πÄ‡∏ó‡∏≠‡∏°, 46 ‡∏ß‡∏¥‡∏ä‡∏≤) ===
    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏°‡∏≤‡∏ô‡∏≤‡∏ô GPA ~2.0, ‡∏°‡∏µ‡∏•‡∏á‡∏ã‡πâ‡∏≥‡∏´‡∏•‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤, borderline ‡∏à‡∏∞‡∏à‡∏ö‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏à‡∏ö
    # Wide format: ‡πÅ‡∏Ñ‡∏•‡∏Ø1=D+(‡∏´‡∏•‡∏±‡∏á‡∏ã‡πâ‡∏≥), ‡∏ß‡∏á‡∏à‡∏£‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏•=C(‡∏´‡∏•‡∏±‡∏á‡∏ã‡πâ‡∏≥), ‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•=D(‡∏´‡∏•‡∏±‡∏á‡∏ã‡πâ‡∏≥)
    grades_s5 = [
        'C+','C','D+','D+','D+','D','D+','D',         # ‡∏õ‡∏µ1‡πÄ‡∏ó‡∏≠‡∏°1: ‡πÅ‡∏Ñ‡∏•‡∏Ø1=D+(‡∏´‡∏•‡∏±‡∏á‡∏ã‡πâ‡∏≥‡∏à‡∏≤‡∏ÅF)
        'C','D+','C+','D','C','D+','C','C',            # ‡∏õ‡∏µ1‡πÄ‡∏ó‡∏≠‡∏°2: ‡πÅ‡∏¢‡πà‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
        'D+','C','D+','C','D+',                         # ‡∏õ‡∏µ2‡πÄ‡∏ó‡∏≠‡∏°1: ‡∏ß‡∏á‡∏à‡∏£‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏•=C(‡∏´‡∏•‡∏±‡∏á‡∏ã‡πâ‡∏≥‡∏à‡∏≤‡∏ÅW)
        'C','D+','C','D+','C+','C','C',                # ‡∏õ‡∏µ2‡πÄ‡∏ó‡∏≠‡∏°2: ‡∏û‡∏≠‡∏ú‡πà‡∏≤‡∏ô
        'D+','C','D','C','C+','D+',                    # ‡∏õ‡∏µ3‡πÄ‡∏ó‡∏≠‡∏°1: ‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•=D(‡∏´‡∏•‡∏±‡∏á‡∏ã‡πâ‡∏≥‡∏à‡∏≤‡∏ÅF)
        'C','C+','C','D+','C','C+',                    # ‡∏õ‡∏µ3‡πÄ‡∏ó‡∏≠‡∏°2: ‡∏û‡∏≠‡πÑ‡∏õ‡πÑ‡∏î‡πâ
        'C','C+','C','D+','C','C+',                    # ‡∏õ‡∏µ4‡πÄ‡∏ó‡∏≠‡∏°1: ‡πÉ‡∏Å‡∏•‡πâ‡∏à‡∏ö
    ]
    ex5 = ['64010023', '‡∏ô‡∏≤‡∏¢ ‡∏ß‡∏£‡∏ß‡∏¥‡∏ó‡∏¢‡πå ‡∏û‡∏±‡∏ô‡∏ò‡∏∏‡πå‡∏î‡∏µ']
    for i, cid in enumerate(ordered_ids):
        ex5.append(grades_s5[i] if i < len(grades_s5) else '')
    output.write(','.join(ex5) + '\n')

    # Build response
    csv_content = output.getvalue()
    output.close()

    response = app.response_class(
        response='\ufeff' + csv_content,
        status=200,
        mimetype='text/csv; charset=utf-8'
    )
    response.headers['Content-Disposition'] = 'attachment; filename=batch_template_wide.csv'
    response.headers['Content-Type'] = 'text/csv; charset=utf-8-sig'
    return response


def _generate_long_template(courses_data, all_terms):
    """Generate Long format CSV template."""
    from io import StringIO

    output = StringIO()
    course_lookup = {c['id']: c for c in courses_data}

    # Header
    output.write('STUDENT_ID,STUDENT_NAME,COURSE_CODE,COURSE_NAME,CREDIT,YEAR,TERM,GRADE\n')

    # Guide comments
    output.write('# ‡∏£‡∏´‡∏±‡∏™‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤,‡∏ä‡∏∑‡πà‡∏≠‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤,‡∏£‡∏´‡∏±‡∏™‡∏ß‡∏¥‡∏ä‡∏≤,‡∏ä‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡∏ä‡∏≤(‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏£‡∏≠‡∏Å‡∏Å‡πá‡πÑ‡∏î‡πâ),‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Å‡∏¥‡∏ï,‡∏õ‡∏µ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤(‡∏û.‡∏®.),‡πÄ‡∏ó‡∏≠‡∏°(1/2/3),‡πÄ‡∏Å‡∏£‡∏î\n')
    output.write('# ‡πÄ‡∏Å‡∏£‡∏î‡∏ó‡∏µ‡πà‡πÉ‡∏™‡πà‡πÑ‡∏î‡πâ: A / B+ / B / C+ / C / D+ / D / F / W / I\n')
    output.write('# ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÅ‡∏ñ‡∏ß = 1 ‡∏ß‡∏¥‡∏ä‡∏≤‡∏Ç‡∏≠‡∏á 1 ‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤ | ‡∏ñ‡πâ‡∏≤‡∏•‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ã‡πâ‡∏≥ ‡πÉ‡∏´‡πâ‡πÉ‡∏™‡πà‡πÅ‡∏ñ‡∏ß‡πÉ‡∏´‡∏°‡πà (‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÄ‡∏Å‡∏£‡∏î‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î)\n')
    output.write('# ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏∏ YEAR(‡∏õ‡∏µ‡∏û.‡∏®.) ‡πÅ‡∏•‡∏∞ TERM(‡πÄ‡∏ó‡∏≠‡∏°) ‡∏ó‡∏∏‡∏Å‡πÅ‡∏ñ‡∏ß ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ AI ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏õ‡∏µ/‡πÄ‡∏ó‡∏≠‡∏°‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á\n')
    output.write('#\n')

    # Show term mapping guide
    output.write('# === ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏õ‡∏µ/‡πÄ‡∏ó‡∏≠‡∏°‡∏Å‡∏±‡∏ö‡∏ß‡∏¥‡∏ä‡∏≤‡πÉ‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£ ===\n')
    for term_info in all_terms:
        course_count = len(term_info['ids'])
        course_names = []
        for cid in term_info['ids'][:3]:
            c = course_lookup.get(cid, {'thaiName': cid})
            course_names.append(c['thaiName'][:20])
        etc = '...' if course_count > 3 else ''
        output.write(f'# ‡∏õ‡∏µ{term_info["year"]}‡πÄ‡∏ó‡∏≠‡∏°{term_info["term"]}: {course_count} ‡∏ß‡∏¥‡∏ä‡∏≤ ‡πÄ‡∏ä‡πà‡∏ô {" / ".join(course_names)}{etc}\n')
    output.write('# ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: YEAR = ‡∏õ‡∏µ ‡∏û.‡∏®. ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô (‡πÄ‡∏ä‡πà‡∏ô 2566, 2567) ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏õ‡∏µ‡∏ó‡∏µ‡πà (1,2,3,4)\n')
    output.write('#\n')

    # ============================================================
    # 5 ‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏à‡∏≥‡∏•‡∏≠‡∏á ‚Äî ‡∏Ñ‡∏•‡∏∞‡πÄ‡∏Å‡∏£‡∏î ‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏à‡∏£‡∏¥‡∏á
    # Long format: ‡πÉ‡∏™‡πà‡∏ó‡∏∏‡∏Å‡πÅ‡∏ñ‡∏ß ‡∏ñ‡πâ‡∏≤‡∏•‡∏á‡∏ã‡πâ‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏ñ‡∏ß‡πÉ‡∏´‡∏°‡πà (‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞ detect retake)
    # ============================================================

    students_examples = [
        {
            'id': '66010001', 'name': '‡∏ô‡∏≤‡∏¢ ‡∏Å‡∏¥‡∏ï‡∏ï‡∏¥‡∏û‡∏á‡∏®‡πå ‡∏ß‡∏á‡∏®‡πå‡∏™‡∏∏‡∏ß‡∏£‡∏£‡∏ì',
            'start_year': 2566,
            'max_terms': 6,  # ‡∏õ‡∏µ1‡πÄ‡∏ó‡∏≠‡∏°1 ‡∏ñ‡∏∂‡∏á ‡∏õ‡∏µ3‡πÄ‡∏ó‡∏≠‡∏°2
            'label': '‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ñ‡∏∂‡∏á ‡∏õ‡∏µ3‡πÄ‡∏ó‡∏≠‡∏°2 (6 ‡πÄ‡∏ó‡∏≠‡∏°, ‡πÄ‡∏Å‡πà‡∏á‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°/‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì ‡πÅ‡∏ï‡πà‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á, GPA~3.1)',
            'grades': [
                'B+','B','C+','A','A','A','A','B+',           # ‡∏õ‡∏µ1‡πÄ‡∏ó‡∏≠‡∏°1
                'B','B+','B','B+','B+','A','A','A',            # ‡∏õ‡∏µ1‡πÄ‡∏ó‡∏≠‡∏°2
                'A','B+','A','B','B+',                          # ‡∏õ‡∏µ2‡πÄ‡∏ó‡∏≠‡∏°1
                'C+','B+','B','A','A','A','A',                 # ‡∏õ‡∏µ2‡πÄ‡∏ó‡∏≠‡∏°2
                'C+','B+','B+','A','A','B+',                   # ‡∏õ‡∏µ3‡πÄ‡∏ó‡∏≠‡∏°1
                'B','B+','B+','A','B+','A',                    # ‡∏õ‡∏µ3‡πÄ‡∏ó‡∏≠‡∏°2
            ],
            'retakes': []  # ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏•‡∏á‡∏ã‡πâ‡∏≥
        },
        {
            'id': '65010045', 'name': '‡∏ô‡∏≤‡∏á‡∏™‡∏≤‡∏ß ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏ä‡∏ô‡∏Å ‡∏®‡∏£‡∏µ‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå',
            'start_year': 2565,
            'max_terms': 3,  # ‡∏õ‡∏µ1‡πÄ‡∏ó‡∏≠‡∏°1 ‡∏ñ‡∏∂‡∏á ‡∏õ‡∏µ2‡πÄ‡∏ó‡∏≠‡∏°1
            'label': '‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ñ‡∏∂‡∏á ‡∏õ‡∏µ2‡πÄ‡∏ó‡∏≠‡∏°1 (3 ‡πÄ‡∏ó‡∏≠‡∏°, GE ‡∏î‡∏µ‡πÅ‡∏ï‡πà‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÅ‡∏¢‡πà ‡∏ï‡∏Å‡πÅ‡∏Ñ‡∏•‡∏Ø2 ‡∏•‡∏á‡∏ã‡πâ‡∏≥, GPA~2.2)',
            'grades': [
                'B+','B','B+','D+','C','D','C+','C',          # ‡∏õ‡∏µ1‡πÄ‡∏ó‡∏≠‡∏°1: GE ‡∏î‡∏µ, ‡πÅ‡∏Ñ‡∏•‡∏Ø1=D+, ‡∏ü‡∏¥‡∏™‡∏¥‡∏Å‡∏™‡πå‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á
                'B','B+','A','F','D+','D','C','C+',            # ‡∏õ‡∏µ1‡πÄ‡∏ó‡∏≠‡∏°2: ‡πÅ‡∏Ñ‡∏•‡∏Ø2=F!
                'C','C+','C','D+','C',                          # ‡∏õ‡∏µ2‡πÄ‡∏ó‡∏≠‡∏°1
            ],
            'retakes': [
                # ‡∏ï‡∏Å‡πÅ‡∏Ñ‡∏•‡∏Ø2 (F) ‚Üí ‡∏•‡∏á‡∏ã‡πâ‡∏≥‡∏õ‡∏µ2‡πÄ‡∏ó‡∏≠‡∏°2 ‡πÑ‡∏î‡πâ D+
                {'course_idx_in_terms': (1, 3), 'retake_year': 2567, 'retake_term': 2, 'new_grade': 'D+'},
            ]
        },
        {
            'id': '66010112', 'name': '‡∏ô‡∏≤‡∏¢ ‡∏ò‡∏ô‡∏≤‡∏Å‡∏£ ‡πÅ‡∏™‡∏á‡∏≠‡∏£‡∏∏‡∏ì',
            'start_year': 2566,
            'max_terms': 5,  # ‡∏õ‡∏µ1‡πÄ‡∏ó‡∏≠‡∏°1 ‡∏ñ‡∏∂‡∏á ‡∏õ‡∏µ3‡πÄ‡∏ó‡∏≠‡∏°1
            'label': '‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ñ‡∏∂‡∏á ‡∏õ‡∏µ3‡πÄ‡∏ó‡∏≠‡∏°1 (5 ‡πÄ‡∏ó‡∏≠‡∏°, ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÅ‡∏¢‡πà‡πÅ‡∏ï‡πà‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô ‡∏•‡∏á‡∏ã‡πâ‡∏≥2‡∏ß‡∏¥‡∏ä‡∏≤, GPA~2.4)',
            'grades': [
                'C','C+','D+','D','C','D+','F','W',           # ‡∏õ‡∏µ1‡πÄ‡∏ó‡∏≠‡∏°1: ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÑ‡∏°‡πà‡∏ó‡∏±‡∏ô ‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°=F Workshop=W
                'C+','C','C+','D+','C+','C','C','D+',          # ‡∏õ‡∏µ1‡πÄ‡∏ó‡∏≠‡∏°2: ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
                'C+','B','C+','C','C',                          # ‡∏õ‡∏µ2‡πÄ‡∏ó‡∏≠‡∏°1
                'B','B','C+','C','B','B','B+',                 # ‡∏õ‡∏µ2‡πÄ‡∏ó‡∏≠‡∏°2: ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡∏Å!
                'C+','B','B','B+','B+','C+',                   # ‡∏õ‡∏µ3‡πÄ‡∏ó‡∏≠‡∏°1: ‡∏î‡∏µ‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á
            ],
            'retakes': [
                # ‡∏ï‡∏Å‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏° (F) ‚Üí ‡∏•‡∏á‡∏ã‡πâ‡∏≥‡∏õ‡∏µ2‡πÄ‡∏ó‡∏≠‡∏°1 ‡πÑ‡∏î‡πâ C+
                {'course_idx_in_terms': (0, 6), 'retake_year': 2567, 'retake_term': 1, 'new_grade': 'C+'},
                # ‡∏ñ‡∏≠‡∏ô Workshop (W) ‚Üí ‡∏•‡∏á‡∏ã‡πâ‡∏≥‡∏õ‡∏µ2‡πÄ‡∏ó‡∏≠‡∏°2 ‡πÑ‡∏î‡πâ C
                {'course_idx_in_terms': (0, 7), 'retake_year': 2567, 'retake_term': 2, 'new_grade': 'C'},
            ]
        },
        {
            'id': '66010078', 'name': '‡∏ô‡∏≤‡∏á‡∏™‡∏≤‡∏ß ‡∏™‡∏∏‡∏†‡∏≤‡∏û‡∏£ ‡∏à‡∏±‡∏ô‡∏ó‡∏£‡πå‡πÄ‡∏û‡πá‡∏ç',
            'start_year': 2566,
            'max_terms': 2,  # ‡∏õ‡∏µ1‡πÄ‡∏ó‡∏≠‡∏°1 ‡∏ñ‡∏∂‡∏á ‡∏õ‡∏µ1‡πÄ‡∏ó‡∏≠‡∏°2
            'label': '‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ñ‡∏∂‡∏á ‡∏õ‡∏µ1‡πÄ‡∏ó‡∏≠‡∏°2 (2 ‡πÄ‡∏ó‡∏≠‡∏°, ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏î‡∏µ‡∏™‡∏°‡πà‡∏≥‡πÄ‡∏™‡∏°‡∏≠ ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏£‡πá‡∏ß, GPA~3.4)',
            'grades': [
                'A','A','B+','B+','B','B+','A','B+',          # ‡∏õ‡∏µ1‡πÄ‡∏ó‡∏≠‡∏°1: ‡∏î‡∏µ‡∏ó‡∏∏‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤
                'B+','A','A','B','B+','B','A','B+',            # ‡∏õ‡∏µ1‡πÄ‡∏ó‡∏≠‡∏°2: ‡∏î‡∏µ‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á
            ],
            'retakes': []  # ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏•‡∏á‡∏ã‡πâ‡∏≥
        },
        {
            'id': '64010023', 'name': '‡∏ô‡∏≤‡∏¢ ‡∏ß‡∏£‡∏ß‡∏¥‡∏ó‡∏¢‡πå ‡∏û‡∏±‡∏ô‡∏ò‡∏∏‡πå‡∏î‡∏µ',
            'start_year': 2564,
            'max_terms': 7,  # ‡∏õ‡∏µ1‡πÄ‡∏ó‡∏≠‡∏°1 ‡∏ñ‡∏∂‡∏á ‡∏õ‡∏µ4‡πÄ‡∏ó‡∏≠‡∏°1
            'label': '‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ñ‡∏∂‡∏á ‡∏õ‡∏µ4‡πÄ‡∏ó‡∏≠‡∏°1 (7 ‡πÄ‡∏ó‡∏≠‡∏°, GPA~2.0 ‡∏•‡∏á‡∏ã‡πâ‡∏≥3‡∏ß‡∏¥‡∏ä‡∏≤, borderline ‡∏à‡∏∞‡∏à‡∏ö‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏à‡∏ö)',
            'grades': [
                'C+','C','D+','F','D+','D','D+','D',          # ‡∏õ‡∏µ1‡πÄ‡∏ó‡∏≠‡∏°1: ‡πÅ‡∏Ñ‡∏•‡∏Ø1=F!
                'C','D+','C+','D','C','D+','C','C',            # ‡∏õ‡∏µ1‡πÄ‡∏ó‡∏≠‡∏°2: ‡πÅ‡∏¢‡πà‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
                'D+','C','D+','W','D+',                         # ‡∏õ‡∏µ2‡πÄ‡∏ó‡∏≠‡∏°1: ‡∏ß‡∏á‡∏à‡∏£‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏•=W
                'C','D+','C','D+','C+','C','C',                # ‡∏õ‡∏µ2‡πÄ‡∏ó‡∏≠‡∏°2
                'D+','C','F','C','C+','D+',                    # ‡∏õ‡∏µ3‡πÄ‡∏ó‡∏≠‡∏°1: ‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•=F!
                'C','C+','C','D+','C','C+',                    # ‡∏õ‡∏µ3‡πÄ‡∏ó‡∏≠‡∏°2
                'C','C+','C','D+','C','C+',                    # ‡∏õ‡∏µ4‡πÄ‡∏ó‡∏≠‡∏°1: ‡πÉ‡∏Å‡∏•‡πâ‡∏à‡∏ö
            ],
            'retakes': [
                # ‡∏ï‡∏Å‡πÅ‡∏Ñ‡∏•‡∏Ø1 (F) ‚Üí ‡∏•‡∏á‡∏ã‡πâ‡∏≥‡∏õ‡∏µ2‡πÄ‡∏ó‡∏≠‡∏°1 ‡πÑ‡∏î‡πâ D+
                {'course_idx_in_terms': (0, 3), 'retake_year': 2565, 'retake_term': 1, 'new_grade': 'D+'},
                # ‡∏ñ‡∏≠‡∏ô‡∏ß‡∏á‡∏à‡∏£‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏• (W) ‚Üí ‡∏•‡∏á‡∏ã‡πâ‡∏≥‡∏õ‡∏µ2‡πÄ‡∏ó‡∏≠‡∏°2 ‡πÑ‡∏î‡πâ C
                {'course_idx_in_terms': (2, 3), 'retake_year': 2565, 'retake_term': 2, 'new_grade': 'C'},
                # ‡∏ï‡∏Å‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (F) ‚Üí ‡∏•‡∏á‡∏ã‡πâ‡∏≥‡∏õ‡∏µ3‡πÄ‡∏ó‡∏≠‡∏°2 ‡πÑ‡∏î‡πâ D
                {'course_idx_in_terms': (4, 2), 'retake_year': 2566, 'retake_term': 2, 'new_grade': 'D'},
            ]
        },
    ]

    for student in students_examples:
        output.write(f'# --- {student["name"]}: {student["label"]} ---\n')
        grade_idx = 0
        terms_written = 0
        for term_info in all_terms:
            if terms_written >= student['max_terms']:
                break
            year_label = student['start_year'] + (term_info['year'] - 1)
            for cid in term_info['ids']:
                c = course_lookup.get(cid, {'thaiName': cid, 'credit': 3})
                grade = student['grades'][grade_idx] if grade_idx < len(student['grades']) else ''
                if grade:
                    output.write(f'{student["id"]},{student["name"]},{cid},{c["thaiName"]},{c["credit"]},{year_label},{term_info["term"]},{grade}\n')
                grade_idx += 1
            terms_written += 1

        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏ñ‡∏ß‡∏•‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ã‡πâ‡∏≥ (retake rows) ‚Äî ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞ detect ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
        if student['retakes']:
            for retake in student['retakes']:
                term_idx, course_idx = retake['course_idx_in_terms']
                cid = all_terms[term_idx]['ids'][course_idx]
                c = course_lookup.get(cid, {'thaiName': cid, 'credit': 3})
                output.write(f'{student["id"]},{student["name"]},{cid},{c["thaiName"]},{c["credit"]},{retake["retake_year"]},{retake["retake_term"]},{retake["new_grade"]}\n')
            output.write(f'# ^ ‡∏•‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ã‡πâ‡∏≥ {len(student["retakes"])} ‡∏ß‡∏¥‡∏ä‡∏≤ (‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö retake ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥)\n')

    output.write('#\n')
    output.write('# === ‡∏™‡∏£‡∏∏‡∏õ: ‡∏£‡∏∞‡∏ö‡∏ö‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏•‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ã‡πâ‡∏≥ ===\n')
    output.write('# ‡∏ñ‡πâ‡∏≤‡∏ß‡∏¥‡∏ä‡∏≤‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏ñ‡∏ß ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞:\n')
    output.write('# 1. ‡πÉ‡∏ä‡πâ‡πÄ‡∏Å‡∏£‡∏î‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢\n')
    output.write('# 2. ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏•‡∏á‡∏ã‡πâ‡∏≥ + ‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏ (F/W)\n')
    output.write('# 3. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏Å‡∏£‡∏î‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ ‚Üí ‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ AI ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå\n')

    csv_content = output.getvalue()
    output.close()

    response = app.response_class(
        response='\ufeff' + csv_content,
        status=200,
        mimetype='text/csv; charset=utf-8'
    )
    response.headers['Content-Disposition'] = 'attachment; filename=batch_template_long.csv'
    response.headers['Content-Type'] = 'text/csv; charset=utf-8-sig'
    return response


@app.route('/predict-batch')
def predict_batch_page():
    """Page for training models (upload data to train)."""
    return render_template('index.html')

@app.route('/batch-prediction')
def batch_prediction_page():
    """Page for batch prediction from uploaded file (predict multiple students)."""
    try:
        courses_json = json.dumps(app.config.get('COURSES_DATA', []))
        terms_json = json.dumps(app.config.get('ALL_TERMS_DATA', []))
        return render_template(
            'predict_batch.html',
            coursesData=courses_json,
            allTermsData=terms_json
        )
    except Exception as e:
        logger.error(f"Error rendering batch prediction page: {str(e)}")
        return render_template(
            'predict_batch.html',
            coursesData='[]',
            allTermsData='[]'
        )

@app.route('/models')
def models_page():
    if admin_manager.enabled and not has_any_role('admin', 'super_admin'):
        return redirect(url_for('index'))
    return render_template('model_management.html')
@app.route('/predict_manual_input', methods=['POST'])
def predict_manual_input():
    """Predicts outcome from manually entered subject data."""
    try:
        data = request.json
        student_name = data.pop('student_name', '‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô')
        model_filename = data.get('model_filename')

        if not model_filename:
            models_list = storage.list_models()
            subject_models = [m for m in models_list if _is_subject_model(m)]
            if subject_models:
                model_filename = subject_models[0]['filename']
            else:
                return jsonify({'success': False, 'error': 'No model filename provided and no trained model found.'})

        # ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏≤‡∏Å storage
        loaded_model_data = storage.load_model(model_filename)
        if not loaded_model_data:
            return jsonify({'success': False, 'error': f'Specified model file {model_filename} not found.'})

        model_info = {
            'models': loaded_model_data.get('models', {}),
            'scaler': loaded_model_data.get('scaler')
        }
        feature_cols = loaded_model_data.get('feature_columns', [])
        data_format_used = loaded_model_data.get('data_format', 'unknown')
        logger.info(f"Loaded model '{model_filename}' (format: {data_format_used}) for manual prediction.")

        input_grades_raw = {cid: grade for cid, grade in data.items() if cid != 'model_filename'}

        student_data_for_prediction = {}
        grade_mapping = app.config['DATA_CONFIG']['grade_mapping']
        subject_categories = app.config['SUBJECT_CATEGORIES']
        all_known_courses_from_config = app.config['COURSES_DATA']

        all_grades_entered = []
        subject_categories_grades = {cat: [] for cat in subject_categories.keys()}
        subject_categories_grades['‡∏≠‡∏∑‡πà‡∏ô‡πÜ'] = []

        for course_id, grade_str in input_grades_raw.items():
            if grade_str and grade_str.strip():
                numeric_grade = None
                try:
                    numeric_grade = float(grade_str)
                    if not (0.0 <= numeric_grade <= 4.0):
                        numeric_grade = 0.0
                except ValueError:
                    numeric_grade = grade_mapping.get(str(grade_str).upper(), 0.0)
                
                all_grades_entered.append(numeric_grade)

                course_name = ""
                for c_data in all_known_courses_from_config:
                    if c_data['id'] == course_id:
                        course_name = c_data['thaiName']
                        break

                category = '‡∏≠‡∏∑‡πà‡∏ô‡πÜ'
                if course_name:
                    for cat, info in subject_categories.items():
                        if any(keyword in course_name.lower() for keyword in info['keywords']):
                            category = cat
                            break
                        
                subject_categories_grades.get(category, subject_categories_grades['‡∏≠‡∏∑‡πà‡∏ô‡πÜ']).append(numeric_grade)

        gpa = np.mean(all_grades_entered) if all_grades_entered else 0.0
        min_grade = np.min(all_grades_entered) if all_grades_entered else 0.0
        max_grade = np.max(all_grades_entered) if all_grades_entered else 0.0
        std_grade = np.std(all_grades_entered) if len(all_grades_entered) > 1 else 0.0
        fail_count = sum(1 for g in all_grades_entered if g <= app.config['DATA_CONFIG']['grade_mapping'].get('D', 1.0) - 0.01)
        fail_rate = fail_count / len(all_grades_entered) if all_grades_entered else 0.0
        total_subjects = len(all_grades_entered)

        student_data_for_prediction = {
            'gpa': gpa,
            'min_grade': min_grade,
            'max_grade': max_grade,
            'std_grade': std_grade,
            'fail_count': fail_count,
            'fail_rate': fail_rate,
            'total_subjects': total_subjects,
            'year_in': 0,
            'year_out': 0,
            'total_terms': 0
        }

        for cat, cat_grades in subject_categories_grades.items():
            if cat_grades:
                student_data_for_prediction[f'gpa_{cat}'] = np.mean(cat_grades)
                student_data_for_prediction[f'min_{cat}'] = np.min(cat_grades)
                student_data_for_prediction[f'max_{cat}'] = np.max(cat_grades)
                student_data_for_prediction[f'fail_rate_{cat}'] = sum(1 for g in cat_grades if g <= app.config['DATA_CONFIG']['grade_mapping'].get('D', 1.0) - 0.01) / len(cat_grades)
            else:
                student_data_for_prediction[f'gpa_{cat}'] = 0.0
                student_data_for_prediction[f'min_{cat}'] = 0.0
                student_data_for_prediction[f'max_{cat}'] = 0.0
                student_data_for_prediction[f'fail_rate_{cat}'] = 0.0

        processed_input_for_df = {}
        for feature in feature_cols:
            processed_input_for_df[feature] = [student_data_for_prediction.get(feature, 0.0)]

        input_df = pd.DataFrame(processed_input_for_df)

        trained_models = model_info['models']
        scaler = model_info['scaler']

        predictions_proba_list = []
        for name, model in trained_models.items():
            try:
                if name == 'lr':
                    X_scaled = scaler.transform(input_df)
                    pred_proba = model.predict_proba(X_scaled)
                else:
                    pred_proba = model.predict_proba(input_df)
                
                if pred_proba.shape[1] == 1:
                    pred_proba = np.hstack((1 - pred_proba, pred_proba))
                
                predictions_proba_list.append(pred_proba)
            except Exception as e:
                logger.warning(f"Could not predict with model {name} from manual input: {str(e)}")
                continue

        if not predictions_proba_list:
            return jsonify({'success': False, 'error': 'Could not make predictions with manual input.'})

        avg_prob_per_student = np.mean([pred_proba_array[0] for pred_proba_array in predictions_proba_list], axis=0)
        avg_prob_fail = avg_prob_per_student[0]
        avg_prob_pass = avg_prob_per_student[1]

        prediction = '‡∏à‡∏ö' if avg_prob_pass >= avg_prob_fail else '‡πÑ‡∏°‡πà‡∏à‡∏ö'

        confidence = max(avg_prob_pass, avg_prob_fail)
        high_confidence_threshold = app.config['DATA_CONFIG']['risk_levels']['high_confidence_threshold']
        medium_confidence_threshold = app.config['DATA_CONFIG']['risk_levels']['medium_confidence_threshold']

        if confidence > high_confidence_threshold:
            risk_level = '‡∏ï‡πà‡∏≥' if prediction == '‡∏à‡∏ö' else '‡∏™‡∏π‡∏á'
        elif confidence > medium_confidence_threshold:
            risk_level = '‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á'
        else:
            risk_level = '‡∏™‡∏π‡∏á' if prediction == '‡πÑ‡∏°‡πà‡∏à‡∏ö' else '‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á'

        gpa_for_analysis = student_data_for_prediction.get('gpa', 0.0)

        analysis = []
        recommendations = []

        low_gpa_threshold = app.config['DATA_CONFIG']['risk_levels']['low_gpa_threshold']
        warning_gpa_threshold = app.config['DATA_CONFIG']['risk_levels']['warning_gpa_threshold']
        high_fail_rate_threshold = app.config['DATA_CONFIG']['risk_levels']['high_fail_rate_threshold']

        if gpa_for_analysis < low_gpa_threshold:
            analysis.append(f"GPA ‡∏ï‡πà‡∏≥‡∏°‡∏≤‡∏Å ({float(gpa_for_analysis):.2f})")
            recommendations.extend(app.config['MESSAGES']['recommendations']['high_risk'])
        elif gpa_for_analysis < warning_gpa_threshold:
            analysis.append(f"GPA ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á ({float(gpa_for_analysis):.2f})")
            recommendations.extend(app.config['MESSAGES']['recommendations']['medium_risk'])
        elif gpa_for_analysis < 3.0:
            analysis.append(f"GPA ‡∏û‡∏≠‡πÉ‡∏ä‡πâ ({float(gpa_for_analysis):.2f})")
            recommendations.append("‡∏°‡∏µ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÉ‡∏´‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô")
        else:
            analysis.append(f"GPA ‡∏î‡∏µ ({float(gpa_for_analysis):.2f})")
            recommendations.extend(app.config['MESSAGES']['recommendations']['low_risk'])

        if prediction == '‡πÑ‡∏°‡πà‡∏à‡∏ö':
            recommendations.append("‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡∏ó‡∏ö‡∏ó‡∏ß‡∏ô‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÅ‡∏•‡∏∞‡∏Ç‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠")
            if student_data_for_prediction.get('fail_rate', 0) > high_fail_rate_threshold:
                recommendations.append("‡∏°‡∏µ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡∏ï‡∏Å‡πÉ‡∏ô‡∏ö‡∏≤‡∏á‡∏ß‡∏¥‡∏ä‡∏≤‡∏™‡∏π‡∏á ‡∏Ñ‡∏ß‡∏£‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ã‡πà‡∏≠‡∏°")

        if data_format_used == 'subject_based':
            weak_categories = []
            for cat_key in app.config['SUBJECT_CATEGORIES'].keys():
                cat_gpa_key = f'gpa_{cat_key}'
                if student_data_for_prediction.get(cat_gpa_key, 0) < low_gpa_threshold:
                    weak_categories.append(cat_key)
            if weak_categories:
                recommendations.append(f"‡∏Ñ‡∏ß‡∏£‡πÄ‡∏ô‡πâ‡∏ô‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏ß‡∏¥‡∏ä‡∏≤‡πÉ‡∏ô‡∏´‡∏°‡∏ß‡∏î: {', '.join(weak_categories[:2])}")

        return jsonify({
            'success': True,
            'student_name': student_name,
            'prediction': prediction,
            'prob_pass': float(avg_prob_pass),
            'prob_fail': float(avg_prob_fail),
            'gpa_input': float(gpa_for_analysis),
            'risk_level': risk_level,
            'confidence': float(confidence),
            'analysis': list(set(analysis)),
            'recommendations': list(set(recommendations)),
            'data_format_used': data_format_used
        })

    except Exception as e:
        logger.error(f"Error during manual input prediction: {str(e)}")
        return jsonify({'success': False, 'error': f'An error occurred during prediction: {str(e)}'})

def load_existing_models():
    """Loads existing trained models from storage."""
    try:
        logger.info("üîç Searching for existing models...")
        
        # ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏≤‡∏Å‡∏ó‡∏±‡πâ‡∏á storage ‡πÅ‡∏•‡∏∞ local
        models_found = []
        
        # 1. ‡∏•‡∏≠‡∏á‡∏à‡∏≤‡∏Å storage ‡∏Å‡πà‡∏≠‡∏ô
        try:
            models_list = storage.list_models()
            if models_list:
                models_found.extend(models_list)
                logger.info(f"Found {len(models_list)} models in storage")
        except Exception as e:
            logger.warning(f"Could not load from storage: {e}")
        
        # 2. ‡∏•‡∏≠‡∏á‡∏à‡∏≤‡∏Å local folder
        try:
            model_folder = app.config['MODEL_FOLDER']
            if os.path.exists(model_folder):
                for filename in os.listdir(model_folder):
                    if filename.endswith('.joblib'):
                        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
                        if not any(m.get('filename') == filename for m in models_found):
                            filepath = os.path.join(model_folder, filename)
                            try:
                                model_data = joblib.load(filepath)
                                models_found.append({
                                    'filename': filename,
                                    'created_at': model_data.get('created_at', datetime.fromtimestamp(os.path.getctime(filepath)).isoformat()),
                                    'data_format': model_data.get('data_format', 'subject_based'),
                                    'performance_metrics': model_data.get('performance_metrics', {}),
                                    'storage': 'local'
                                })
                            except Exception as e:
                                logger.warning(f"Could not load local model {filename}: {e}")
        except Exception as e:
            logger.warning(f"Could not check local folder: {e}")
        
        if not models_found:
            logger.info("No existing models found")
            return
        
        # Load subject-based model ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (‡∏ï‡∏±‡∏î GPA-based ‡∏≠‡∏≠‡∏Å)
        subject_models = [m for m in models_found if _is_subject_model(m)]
        
        if subject_models:
            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á
            subject_models.sort(key=lambda x: x.get('created_at', ''), reverse=True)
            latest_subject = subject_models[0]
            
            # ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•
            loaded_data = None
            
            # ‡∏•‡∏≠‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å storage ‡∏Å‡πà‡∏≠‡∏ô
            if latest_subject.get('storage') != 'local':
                try:
                    loaded_data = storage.load_model(latest_subject['filename'])
                except Exception as e:
                    logger.warning(f"Could not load from storage: {e}")
            
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡∏•‡∏≠‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å local
            if not loaded_data:
                try:
                    filepath = os.path.join(app.config['MODEL_FOLDER'], latest_subject['filename'])
                    if os.path.exists(filepath):
                        loaded_data = joblib.load(filepath)
                        logger.info(f"Loaded from local: {latest_subject['filename']}")
                except Exception as e:
                    logger.error(f"Could not load model: {e}")
            
            if loaded_data:
                models['subject_model'] = {
                    'models': loaded_data.get('models', {}),
                    'scaler': loaded_data.get('scaler')
                }
                models['subject_feature_cols'] = loaded_data.get('feature_columns', [])
                models['subject_model_info'] = loaded_data.get('performance_metrics', 
                    {'accuracy': 0.85, 'precision': 0.85, 'recall': 0.85, 'f1_score': 0.85})
                models['subject_model_info']['created_at'] = loaded_data.get('created_at', datetime.now().isoformat())
                models['subject_model_info']['loaded_from_file'] = True
                models['subject_model_info']['filename'] = latest_subject['filename']
                logger.info(f"‚úÖ Loaded latest subject model: {latest_subject['filename']}")

    except Exception as e:
        logger.error(f"‚ùå Error loading existing models: {str(e)}")

if __name__ == '__main__':
    logger.info("=== FLASK APP CONFIGURATION ===")
    logger.info(f"App name: {app.name}")
    logger.info(f"App debug: {app.debug}")
    logger.info(f"App testing: {app.testing}")
    logger.info(f"Config keys: {list(app.config.keys())}")
    logger.info(f"Upload folder: {app.config['UPLOAD_FOLDER']}")
    logger.info(f"Model folder: {app.config['MODEL_FOLDER']}")
    logger.info(f"S3 Storage: {'Enabled' if not storage.use_local else 'Disabled (using local)'}")
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
    for folder in [app.config['UPLOAD_FOLDER'], app.config['MODEL_FOLDER']]:
        if not os.path.exists(folder):
            os.makedirs(folder)
            logger.info(f"‚úÖ Created folder: {folder}")
    
    # ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà
    load_existing_models()
    
    logger.info("üöÄ Starting server...")
    app.run(host='0.0.0.0', port=int(os.environ.get('PORT', 5000)), debug=False)
